<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Harsha Kokel</title>
    <link>https://harshakokel.com/</link>
    <description>Recent content on Harsha Kokel</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Original-Theme is licensed with the Creative Commons Attribution 3.0 Unported License</copyright>
    <lastBuildDate>Wed, 21 Oct 2020 16:40:08 +0200</lastBuildDate><atom:link href="https://harshakokel.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Automatically Generating Abstractions for Planning</title>
      <link>https://harshakokel.com/posts/abstraction-for-planning/</link>
      <pubDate>Wed, 21 Oct 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/abstraction-for-planning/</guid>
      <description>An effective hierarchical decomposition of a problem would solve a task at lower level without violating the conditions in more abstract/higher levels of the hierarchy. &lt;a href=&#34;https://www.isi.edu/integration/papers/knoblock94-aij.pdf&#34; target=&#34;_blank&#34;&gt;Knoblock (1994)&lt;/a&gt; formalizises this intuition as &lt;strong&gt;ordered monotonicity property&lt;/strong&gt;. This post briefly explains that property and describes how to learn the hierarchy using the sufficient condition for that property.</description>
    </item>
    
    <item>
      <title>Logical Neural Network</title>
      <link>https://harshakokel.com/posts/logical-nn/</link>
      <pubDate>Fri, 25 Sep 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/logical-nn/</guid>
      <description>&lt;a href=&#34;https://arxiv.org/abs/2006.13155&#34; target=&#34;_blank&#34;&gt;Ryan Riegel, et al. (arxiv 2020)&lt;/a&gt; proposes to build Neural Network by adding one neuron for each logical gate and literal in a logic formulae and hence building a neural framework for logical inference. This article reviews their work. It was written jointly with &lt;a href=&#34;https://dtrycode.github.io&#34; target=&#34;_blank&#34;&gt;Siwen Yan&lt;/a&gt;, as part of the course on NeuroSymbolic systems by Prof. Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>Augmenting Neural Networks with First-order Logic</title>
      <link>https://harshakokel.com/posts/nn-with-fol/</link>
      <pubDate>Tue, 22 Sep 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/nn-with-fol/</guid>
      <description>Declarative knowledge, first-order rules are used in ILP (a lot) to reduce dependency on the data. Since deep neural network are data hungry, can we use some first-order rules and reduce their data requirement? This post reviews the work by Tao and Srikumar &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1028v2.pdf&#34; target=&#34;_blank&#34;&gt;(ACL 2019)&lt;/a&gt; which attempts to answer this research question.</description>
    </item>
    
    <item>
      <title> The 6 Types of Neuro-Symbolic Systems</title>
      <link>https://harshakokel.com/posts/neurosymbolic-systems/</link>
      <pubDate>Tue, 09 Jun 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/neurosymbolic-systems/</guid>
      <description>I attended the AAAI 2020 conference in NY, and one of the most influencing talk in that conference (for me, of course!) was the address by Prof. Henry Kautz on &lt;a href=&#34;https://roc-hci.com/announcements/the-third-ai-summer/&#34; target=&#34;_blank&#34;&gt;The Third AI Summer&lt;/a&gt;. In that talk, he provided some taxonomy for the future Neural and Symbolic approaches. This article is my attempt to summarize that taxonomy.</description>
    </item>
    
    <item>
      <title>Tools for Causal Inference</title>
      <link>https://harshakokel.com/posts/causal-tools/</link>
      <pubDate>Sun, 24 May 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/causal-tools/</guid>
      <description>I read the Book of Why last year and recently in the reading group at Starling lab we read the 7 Tools of causal inference by Prof. Pearl. I was a little taken by how much I have forgotten about causal inference. So I found the need to jolt down my understanding, so I can refer to it later. This article summarized my current understanding of the tools presented in the paper, based on the paper and the book.</description>
    </item>
    
    <item>
      <title>Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach</title>
      <link>https://harshakokel.com/posts/afe/</link>
      <pubDate>Fri, 08 May 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/afe/</guid>
      <description>Natarjan et al., &lt;a href=&#34;http://utdallas.edu/~sriraam.natarajan/Papers/AFE_IJCAI18.pdf&#34; target=&#34;_blank&#34;&gt;IJCAI 2018&lt;/a&gt; is one of the most ebullient papers from the Starling Lab (in my opinion, of course!). It formalizes a unique problem setting called &lt;strong&gt;Active Feature Elicitation&lt;/strong&gt;. The task here is to select the best set of examples on whom the missing features can be queried actively. This blog post summarizes my understanding of that paper.</description>
    </item>
    
    <item>
      <title>Adversarial Attacks on Graph Neural Networks via Meta Learning</title>
      <link>https://harshakokel.com/posts/meta-attack-gnn/</link>
      <pubDate>Wed, 29 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/meta-attack-gnn/</guid>
      <description>This article reviews a very exciting ICLR 2019 paper: &lt;a href=&#34;https://openreview.net/forum?id=Bylnx209YX&amp;amp;noteId=r1xNHe2tAQ&#34; target=&#34;_blank&#34;&gt;Adversarial Attacks on Graph Neural Networks via Meta Learning&lt;/a&gt;. This was originally written as part of a class assignment at UT dallas.</description>
    </item>
    
    <item>
      <title>From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning</title>
      <link>https://harshakokel.com/posts/skills-to-symbols/</link>
      <pubDate>Wed, 22 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/skills-to-symbols/</guid>
      <description>In the pursuit of learning planner from data, I ended up reading Konidaris et al. &lt;a href=&#34;https://www.jair.org/index.php/jair/article/view/11175&#34; target=&#34;_blank&#34;&gt;(JAIR 2018)&lt;/a&gt;. Getting through this paper was an onerous task. Which I would not like to do again. So, here are my notes on the key concepts from that paper, which are relevant for learning high-level, abstract planner.</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning With Relational Inductive Biases</title>
      <link>https://harshakokel.com/posts/drrl/</link>
      <pubDate>Wed, 15 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/drrl/</guid>
      <description>Relational RL has not made a lot of splashes in real-world because it is easier to write a planner than learn a relational RL agent. This might be about to change with the current achievements of the graph based relational reasoning approaches. This article summarizes my understanding of the pioneering work of Vinicius Zambaldi et al. &lt;a href=&#34;https://openreview.net/forum?id=HkxaFoC9KQ&#34; target=&#34;_blank&#34;&gt;(ICLR 2019)&lt;/a&gt; on Deep Relational RL.</description>
    </item>
    
    <item>
      <title>A simple neural network module for relational reasoning</title>
      <link>https://harshakokel.com/posts/relational-network/</link>
      <pubDate>Mon, 13 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/relational-network/</guid>
      <description>Overview of Adam Santoro et al. &lt;a href=&#34;https://arxiv.org/abs/1706.01427&#34; target=&#34;_blank&#34;&gt;(NeurIPS 2017)&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Relational Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/rrl/</link>
      <pubDate>Sun, 12 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/rrl/</guid>
      <description>My notes on Džeroski, Sašo, Luc De Raedt, and Kurt Driessens, &lt;a href=&#34;https://link.springer.com/article/10.1023/A:1007694015589&#34; target=&#34;_blank&#34;&gt;Machine Learning 2001&lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Few-Shot Learning with GNN</title>
      <link>https://harshakokel.com/posts/few-shot-learning-gnn/</link>
      <pubDate>Sat, 04 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/few-shot-learning-gnn/</guid>
      <description>My notes on Victor Garcia, Joan Bruna, &lt;a href=&#34;https://openreview.net/forum?id=BJj6qGbRW&#34; target=&#34;_blank&#34;&gt;ICLR 2018&lt;/a&gt;. Written as part of the Complex  Networks &lt;a href=&#34;https://personal.utdallas.edu/~fxc190007/courses/20S-7301/&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; by Prof. Feng Chen.</description>
    </item>
    
    <item>
      <title>Relational Inductive Bias</title>
      <link>https://harshakokel.com/posts/relational-inductive-bias/</link>
      <pubDate>Mon, 30 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/relational-inductive-bias/</guid>
      <description>There is a recent surge in papers which are using &lt;strong&gt;Relational Inductive Bias&lt;/strong&gt; with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic.</description>
    </item>
    
    <item>
      <title>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
      <link>https://harshakokel.com/posts/maml/</link>
      <pubDate>Fri, 27 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/maml/</guid>
      <description>My notes on Chelsea Finn, Pieter Abbeel, Sergey Levine, &lt;a href=&#34;http://proceedings.mlr.press/v70/finn17a&#34; target=&#34;_blank&#34;&gt;ICML 2017&lt;/a&gt;. Written as part of the Complex  Networks &lt;a href=&#34;https://personal.utdallas.edu/~fxc190007/courses/20S-7301/&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; by Prof. Feng Chen.</description>
    </item>
    
    <item>
      <title>Hindsight Experience Replay</title>
      <link>https://harshakokel.com/posts/her/</link>
      <pubDate>Fri, 20 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/her/</guid>
      <description>My notes on Marcin Andrychowicz et al. &lt;a href=&#34;https://arxiv.org/abs/1707.01495&#34; target=&#34;_blank&#34;&gt;NeurIPS 2017&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Few-Shot Bayesian Imitation Learning with Logical Program Policies</title>
      <link>https://harshakokel.com/posts/logic-program-policies/</link>
      <pubDate>Wed, 18 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/logic-program-policies/</guid>
      <description>My notes on Tom Silver, Kelsey R. Allen, Alex K. Lew, Leslie Kaelbling, and Josh Tenenbaum, &lt;a href=&#34;https://github.com/tomsilver/policies_logic_programs&#34; target=&#34;_blank&#34;&gt;AAAI 2020&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Understanding Attention and Generalization in Graph Neural Networks</title>
      <link>https://harshakokel.com/posts/understanding-node-attention/</link>
      <pubDate>Wed, 26 Feb 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/understanding-node-attention/</guid>
      <description>My notes on Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer, &lt;a href=&#34;https://arxiv.org/abs/1905.02850&#34; target=&#34;_blank&#34;&gt;NeurIPS 2019&lt;/a&gt;. Written as part of the Complex Networks &lt;a href=&#34;https://personal.utdallas.edu/~fxc190007/courses/20S-7301/&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; by Prof. Feng Chen.</description>
    </item>
    
    <item>
      <title>Graph Attention Networks</title>
      <link>https://harshakokel.com/posts/gat/</link>
      <pubDate>Mon, 17 Feb 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/gat/</guid>
      <description>My notes on Peter Veličković et al. &lt;a href=&#34;https://openreview.net/pdf?id=rJXMpikCZ&#34; target=&#34;_blank&#34;&gt;ICLR 2018&lt;/a&gt;.  Written as part of the Complex  Networks &lt;a href=&#34;https://personal.utdallas.edu/~fxc190007/courses/20S-7301/&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; by Prof. Feng Chen.</description>
    </item>
    
    <item>
      <title>Graph Convolutional Networks</title>
      <link>https://harshakokel.com/posts/gcn/</link>
      <pubDate>Wed, 05 Feb 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/gcn/</guid>
      <description>My notes on Thomas N Kipf and Max Welling &lt;a href=&#34;https://arxiv.org/abs/1609.02907&#34; target=&#34;_blank&#34;&gt;ICLR 2017&lt;/a&gt;.  Written as part of the Complex  Networks &lt;a href=&#34;https://personal.utdallas.edu/~fxc190007/courses/20S-7301/&#34; target=&#34;_blank&#34;&gt;course&lt;/a&gt; by Prof. Feng Chen.</description>
    </item>
    
    <item>
      <title>Fitted Q and Batch Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/batch-rl/</link>
      <pubDate>Fri, 06 Dec 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/batch-rl/</guid>
      <description>Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an &lt;a href=&#34;https://harshakokel.com/pdf/ECMO-RL.pdf&#34;&gt;RL for healthcare&lt;/a&gt; project, as part of Advanced RL course by Prof. Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>Hierarchical Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/hierarchical-rl/</link>
      <pubDate>Mon, 28 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/hierarchical-rl/</guid>
      <description>An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>Active Advice Seeking for Inverse Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/active-advice-seeking/</link>
      <pubDate>Sat, 05 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/active-advice-seeking/</guid>
      <description>My notes on Phillip Odom and Sriraam Natarajan, &lt;a href=&#34;https://starling.utdallas.edu/assets/pdfs/aasirl_AAMAS.pdf&#34; target=&#34;_blank&#34;&gt;AAMAS 2016&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Minimal Sufficient Explanations for Factored Markov Decision Processes</title>
      <link>https://harshakokel.com/posts/minimal-sufficient-explanation/</link>
      <pubDate>Tue, 03 Sep 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/minimal-sufficient-explanation/</guid>
      <description>My notes on Omar Zia Khan, Pascal Poupart, and James P Black, &lt;a href=&#34;https://ojs.aaai.org/index.php/ICAPS/article/view/13365&#34; target=&#34;_blank&#34;&gt;ICAPS 2009&lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Advice based relational learning</title>
      <link>https://harshakokel.com/posts/advice-based-learning/</link>
      <pubDate>Sun, 04 Nov 2018 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/advice-based-learning/</guid>
      <description>As part of an independent study with Prof. Sriraam Natarajan, I read advice-based methods for data in relational first-order logic. Here are my notes on it.</description>
    </item>
    
    <item>
      <title>Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records</title>
      <link>https://harshakokel.com/posts/srl-for-myocardialinfarction/</link>
      <pubDate>Wed, 28 Feb 2018 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/srl-for-myocardialinfarction/</guid>
      <description>It is no secret that my motivation to do a Ph.D. is to uncover how AI can help improve healthcare. Weiss, Natarajan et al. &lt;a href=&#34;https://starling.utdallas.edu/assets/pdfs/weiss-MI.pdf&#34; target=&#34;_blank&#34;&gt;(IAAI 2012)&lt;/a&gt; is one of the first papers I read to start my Ph.D. journey.</description>
    </item>
    
    <item>
      <title>BLOG: Relational Modeling with Unknown Objects</title>
      <link>https://harshakokel.com/posts/bayesian-logic/</link>
      <pubDate>Wed, 31 Jan 2018 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/bayesian-logic/</guid>
      <description>BLOG by &lt;a href=&#34;https://people.eecs.berkeley.edu/~russell/papers/ijcai05-blog.pdf&#34; target=&#34;_blank&#34;&gt;Milch et al.&lt;/a&gt; provides a language which help us model random functions and probabilistic properties of unknown objects. Going beyong Herbrand Universe. This mainly reviews the key contributions and limitations of that paper. Written as part of the Statistical Relational Learning course by Prof.Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>CV</title>
      <link>https://harshakokel.com/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harshakokel.com/cv/</guid>
      <description>Curriculum Vitae Harsha Kokel,
hkokel@utdallas.edu
The University of Texas at Dallas,
Erik Jonsson School of Engineering and Computer Science,
StARLinG Lab: 3.214, 800 W. Campbell Road, Dallas, TX 75080  Google Scholar | Semantic Scholar |  DBLP |  ORCID |  Research Gate Resume     Education   Doctor of Philosophy (Ph.D.) Computer Science &amp;nbsp; [ Fall 2018 - present ]  The University of Texas at Dallas, TX, USA  Advisor: Prof.</description>
    </item>
    
    <item>
      <title>Misc</title>
      <link>https://harshakokel.com/misc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harshakokel.com/misc/</guid>
      <description>Here are some quotes that speak to me, stay with me, and motivate me. Time and again.
With a little coffee and sunlight, the troubles will get smaller and the world will keep standing. I promise. &amp;mdash; Richard Webber&amp;rsquo;s Character, in Grey&amp;rsquo;s Anatomy
The future enters into us, in order to transform itself in us, long before it happens. &amp;mdash; Rainer Maria Rilke
From [Grothendieck], I have also learned not to take glory in the difficulty of a proof: difficulty means we have not understood.</description>
    </item>
    
    <item>
      <title>Selected Projects</title>
      <link>https://harshakokel.com/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://harshakokel.com/projects/</guid>
      <description>Selected Projects   Communicating with Computers This is a DARPA funded project to build intelligent minecraft agent that can communicate and collaborate with humans through chat to build structures. [ video ]     Learning Sparse Graph for GNN Used meta-learning techniques to optimize the graph structure of obtain sparse graph for GNN.     Causal inference from Protein Expression Data Discovering causal molecular relationships from the evaluation of observational data using do-calculus.</description>
    </item>
    
  </channel>
</rss>
