[
    {
        "uri": "/cv",
        "content": "---\ntitle: \"CV\"\nlinktitle: cv\nmenu:\n  main:\n    weight: -400\nlayout: blank\ndescription: \"This is the CV or Resume or Curriculum Vitae of Harsha Kokel.\"\n---\n\n{{ cv-intro }}",
        "tags": []
    },
    {
        "uri": "/posts/abstraction-for-planning",
        "title": "Automatically Generating Abstractions for Planning",
        "content": "\n\nThis paper highlights a heuristic property called ordered monotonicity propery of hierarchical domains and provides a way to learn the hierarchy using the sufficient condition for that property.\n\nA problem space is defined as $\\Sigma = \\langle L, S, O \\rangle$, consisting of $L$ is set of first-order literals, $S$ is the set of finite states (described using literals), and $O$ is the set of operators in the domain. The authors propose to assign a $Level(l), \\forall l \\in L$, which indicates the hierarchy-level of the literal. Level 0 is the complete ground state and the i  0 is an abstraction. Any plan at abstraction level $i$ can only access  literals with level $i$ or higher.\n\nAn ordered monotonicity property says:\nFor any abstract plan $\\Pi^i$ at level $i$ with operators $\\alpha$, the order of operator will remain same for plan at level $i-1$.\n\n$$\\forall \\alpha, \\alpha^{\\prime} \\in \\Pi^{i},  \\alpha  \\preceq \\alpha^{\\prime} \\implies \\forall  c(\\alpha), c(\\alpha^{\\prime}) \\in \\Pi^{i-1} c(\\alpha)  \\preceq c(\\alpha^{\\prime})$$\n\nAny addition of an operator at level $i-1$ is allowed only if that operator achieves some precondition $p$ for some other operator which existed at level $i$, and $Level(p)=i-1$.\n\n$$\\exists \\beta \\in \\Pi^{i-1}, \\beta \\neq c(x) \\, \\forall x \\in \\Pi^{i} \\implies \\exists \\alpha \\in \\Pi^{i}, \\exists p \\in P{\\alpha}, p \\in E{\\beta}, Level(p)=i-1$$\n\nIf any operator at $i-1$ changes a literal with level $i$, then that operator should exist at level $i$.\n\n$$\\exists \\beta \\in \\Pi^{i-1}, p \\in e_{\\beta}, Level(p) \\geq i \\implies \\exists x \\in \\Pi^{i}, \\beta = c(x) $$\n\nThis heuristic property of hierarchy is motivated from the below observation.\n\n An effective partitioning of a problem requires that the subproblems can be solved without violating the conditions that were already achieved in the more abstract levels of the abstraction hierarchy. In other words, a hierarchical planner ideally\f finds a solution at one level and then maintains the structure of that solution while the remaining\nparts of a solution are\ffilled in.\n\nAuthors note that  following are some sufficient condition for the ordered monotonicity property:\nAll the relevant effects (that are required for goal or for some preconditions) have equal or higher level than other effects.\nAll the relevant effects have equal or higher level than the preconditions.\n\n$$\n\\begin{align}\n\\forall \\alpha \\in O, \\forall e, e^{\\prime} \\in E{\\alpha}, \\forall p \\in P{\\alpha}, \ne \\in Relevant \\implies Level(e) \\geq Level(e^{\\prime}) \\\\\\\\\n \\wedge \\ Level(e) \\geq Level(p) \\ \\\n \\end{align}\n$$\n\nAlthough these conditions are not necessary, they are sufficient to ensure the ordered monotonicity property.\n\nHence given the problem and description of the domain, the literals can be organized in an topological order. This order is  an abstraction hierarchy.\n",
        "tags": [
            "planning",
            "abstractions",
            "hierarchy"
        ]
    },
    {
        "uri": "/posts/active-advice-seeking",
        "title": "Active Advice Seeking for Inverse Reinforcement Learning",
        "content": "\nIn Kunapali et al 2013, authors present a way of incorporating advice in Inverse Reinforcement Learning (IRL) by extending IRL formulation to include constraints based on the expert's advice of preferred and avoided actions, state and reward. Odom et al 2017 expands on  Kunapali et al (2013)'s formulation of preferred and avoided actions by seeking the advice in active learning setting. The paper proposes a active advice-seeking framework, where instead of seeking mere-label from the expert for selected example as done in active learning, they seek advice (set of preferred and avoided labels) over set of examples. Two clear advantages of active advice-seeking framework over the traditional active learning setting is that expert can provide multiple advice and that advice is generalized over set of states.\n\nimg src=\"/images/workflow_activeadvice.png\" width=\"400\" alt=\"\"\n\n The workflow of the paper is shown in the figure above. First the states are clustered together, heuristic proposed is to cluster the states which has similar action distribution, and authors use the action distribution of the demonstrations as a proxy. Then, a policy is learnt only from the demonstrations. Next, query is generated by selecting the state cluster with the maximum uncertainty measure (equation below). This uncertainty measure is an artifact of the entropy of the distribution of actions in demonstration and the entropy of the policy\n\n\\\\[\nU{s}\\left(s{i}\\right)=\\underbrace{w{p} F\\left(s{i}\\right)}{\\text{demonstration}}+\\underbrace{\\left(1-w{p}\\right) G\\left(s{i}\\right)}{\\text{policy}}\n\\\\]\n\n Advice, set of preferred and avoid actions, is received on the query and a new policy is learnt based on the advice and the demonstrations. Query generation, advice seeking and policy learning steps are repeated again and again till the query budget surpasses.\n\n!-- % Eperiments --\n\nActive-advise seeking is compared against learning only from trajectories (standard IRL), advice over single state (active learning) and advice over random clusters. This paper show empirically (on 4 datasets) that the active-advice seeking framework proposed is superior to the other approaches, albeit sensitive to the quality of the cluster.\n\nCritique\n\nThe paper introduces first of its kind framework to actively solicit advice from the human experts and is a very important contribution towards showing the need for Human-In-The-Loop approaches. Abstract MDPs and Relational RL have a natural way to reference a set of states by abstract state and a horn clause respectively. This paper shows that in case of propositional RL, this can be achieved by clustering states using K-Means. This could be a valuable direction as it provides best of both worlds (Propositional \\& Relational RL). The clustering heuristic does not have a theoretical justification but empirical evaluations show that it performs well for IRL. This is reasonable considering that we are learning a policy from the demonstration and hence the states with similar actions are grouped together. However, this might not be the case always. So, a more matured clustering technique might be useful.\n\nThe clustering is done only once, based on the demonstrations which might be sub-optimal. An iterative clustering might have been more challenging but one wonders if that would have helped achieve optimality especially in the drone-flying domain where there is lot of scope to improve.\n\nIn this framework, the advice is generalized over the  clustered states, but in real-world I imagine an advice can be generalized beyond clusters and hence a way to generalize advice across states might be an interesting future direction.\n\nAlso, I would like to appreciate the complexity of the drone flying domain. Here, as the drone is supposed to visit the corners in a specific order, the preferred action at any location will be different based on the next corner being targeted. Paper lacks the details of how the state encoding was done, it would have been interesting to see some details. The expert advice will also have to be carefully tailored to achieve the corner sub-goal.\n\n References\n\nGautam Kunapuli, Phillip Odom, Jude W Shavlik, and Sriraam Natarajan.2013. Guiding autonomous agents to better behaviors through human advice. In ICDM",
        "tags": [
            "RL",
            "advice",
            "starling"
        ]
    },
    {
        "uri": "/posts/advice-based-learning",
        "title": "Advice based relational learning",
        "content": "\nArtificial Intelligence (AI) is rapidly becoming one of the most popular tool for solving various problems of humankind. Ranging from trivial day-to-day activity of switching on/off lights, to severe life-changing decision of detecting tumors in scans, all the problems have been tackled with this tool and hence it is no longer acceptable to have a black-box algorithm making calls. AI Community has realized this and hence has put lot of significance on Explainable AI (XAI) in recent years. AI research has taken a shift from Human vs Robot to Human-Allied AI or Human-in-the-loop AI. However, this is not an unexplored territory. Advice-based or Knowledge-based methods have been around since the conception of AI. As part of this independent study, I read advice-based methods for data in relational first-order logic.  \n\nAdvice based method\n\nLot of work has been done to incorporate knowledge in propositional domain and was surveyed by me as part of CS 7301 Recent Advances in Computing - Survey of Adv Research in CS in Fall of 2018. I summarize it here and then review the work done in relational domain.\n\nUsually advice-based methods encode human knowledge or domain information in various forms, a small list with examples is provided below.\n\nA set of if-then rules is represented as propositional horn clauses  \n    Advice: if $B1, B2, ..., B_n$ then $A$  \n    Horn clause: $A \\leftarrow B1, B2, ..., B_n$  \nQualitative influence between variables is rendered as Qualitative Probabilistic Network (QPN) (Wellman 1990)  \n   Advice: \"price of apple rises with an increase in demand\"   \n\n div align=\"center\"\n img align=\"center\" width=\"400\"  src=\"/images/abrl_1.png\"\n /div\n\nUncertainty between variables is formulated by conditional probability tables\n\n   Advice: \"When it is cloudy, it mostly rains\"\n\n   div align=\"center\"\n   img align=\"center\" width=\"400\"  src=\"/images/abrl_2.png\"\n   /div\n\nPreference statements under a ceteris paribus (all else being equal) is interpreted as CP-Net (Boutilier et al., 2004)\n\ndiv align=\"center\"\nimg align=\"center\" width=\"350\"  src=\"/images/abrl_3.png\"\n/div\n\n  Advice: Fish soup ($Sf$) is strictly preferred over Vegetable soup ($Sv$), and preference between red ($Wr$) and white ($Ww$) wine is conditioned on the soup. Red wine is preferred if served with a vegetable soup, and white wine is preferred if served with a fish soup\n\nOnce advice is encoded, it is used while learning the model in one of the following ways:\n\nTheory Refinement Advice is used as initial model with approximate domain knowledge and training examples are then used to refine this model.\n\nObjective Refinement Advice is used to constrain or modify the objective function and then training samples are used to learn a model that maximizes this objective.\n\nObservation Refinement Advice is used to refine, clean or augment part ( or all ) of the observed training samples and then this is used to learn the model.\n\n Relational and SRL methods\n\nMost of the real-world data is complex, attempts are made to represent them using object (as variables) and relations (as predicates) using first-order logic programs called Inductive Logic Programming (ILP). Add uncertainty to such complex data and they become convoluted. Multiple formalisms have been proposed for such  probabilistic-logic representations using statistical techniques, now popular known as field of  Statistical Relational Learning (SRL) (Getoor 2007). With power of complex representation, these model lose out on other tasks. Specifically, inference and learning (which involves inference) in SRL is NP-hard or harder. For that reason, learning SRL models have been a topic of interest among researchers for a long time and it called for advice-based learning.\n\nAdvice-based relational methods\n\nFirst-order logic unlike propositional logic or flat feature vectors have two important linguistic features, viz. variables and quantifiers. These features augment capability of providing a general high-level advice for example we can say \"Do not change lanes if there is no vehicle in front of you\" which can be easily written as  \n\n$$\n\\begin{align}\nlane(x, l)\\ \\wedge\\ &\\neg\\ \\exists\\ y,\\ (\\ lane(y, l)\\ \\wedge\\  ahead(y, x)\\ )\\\n\\\\\n&\\implies \\ \\neg\\ changelane(x)    \n\\end{align}\n$$\nOne straight forward way of  advice-based relational model is to hand-code the predicates or clauses without need of any data.  Pazzani  and  Kibler  (1992)  proposed one of the first successful algorithms for advice-based inductive learning called First Order Combined Learner (FOCL). FOCL is an extension of FOIL (Quinlan, 1990). FOCL uses both inductive as well as explanation-based component (a set of horn clauses provided by human expert) to propose an extension to a learned concept at every step. FOCL has been shown to generalize more accurately than the purely inductive FOIL algorithm. Bergadano and Giordana (1988)  had also proposed ML-SMART, to integrate explanation-based learning (EBL) and inductive techniques to construct operational concept definitions using three components: theory guided component, arbitrary component and a drop component. ML-SMART is able to handle both overly-general and overly-specific theories. The goal is to find rules that cover the positive examples of the concept and exclude the negatives. GRENDEL (Cohen, 1992) is another FOIL based system that uses advice in the form of an antecedent description grammar (ADG) to explicitly represent the inductive hypothesis space.  Mooney and Zelle (1994) reviews advice-based methods developed for ILP which integrate ILP with EBL.\n\nSRL models have probabilities (parameters) associated with the clauses (structure). Most raw form of advice in SRL can be direct probability values. But, there has been lot of work to incorporate richer form of advice. I briefly describe a few studies:\n\n--  Yang et al. 2014 proposed a cost-sensitive soft margin approach to learn from imbalanced domain. It builds up on the Relational Functional Gradient Boosting (RFGB) (Natarajan et al., 2012) approach of learning an SRL model by including a cost-augmented scoring function equation below that treats positive and negative examples differently in the objective function.\n\n$$\nc\\left(\\hat{y{i}}, y\\right)=\\alpha I\\left(\\hat{y{i}}=1 \\wedge y=0\\right)+\\beta I\\left(\\hat{y_{i}}=0 \\wedge y=1\\right)\n$$\n\nwhere $\\hat{y{i}}$ is the true label of $i^{th}$ instance and $y$ is the predicted label. $I(\\hat{y{i}} = 1 \\wedge y = 0)$ is 1 for false negatives and $I(\\hat{y{i}} = 0 \\wedge y = 1)$ is 1 for false positives. Hence, $c(\\hat{y{i}}, y) = \\alpha$ when a positive example is misclassified, while $c(\\hat{y_{i}},y) = \\beta$ when a negative example is misclassified.\n\n$$\n\\log J =\\sum{i} \\psi\\left(y{i} ; \\mathbf{X}{i}\\right)- \\log \\sum{y{i}^{\\prime}} \\exp \\left\\\\{ \\psi\\left(y{i}^{\\prime} ; \\mathbf{X}{i}\\right)+c\\left(\\hat{y}{i}, y_{i}^{\\prime}\\right) \\right\\\\}\n$$\n\nTaking derivation of this modified objective function gives a nice form of gradient\n\n$$\\Delta=I\\left(\\hat{y}{i}=1\\right)-\\lambda P\\left(yi=1 ; \\mathbf{x}_i\\right)$$\n\nwhere $\\lambda=\\frac{e^{c\\left(\\hat{y}\\{i}, y=1\\right)}}{\\sum\\{y^{\\prime}}\\left[P\\left(y^{\\prime} ; \\mathbf{X}\\{i}\\right) e^{c\\left(\\hat{y}\\{i}, y^{\\prime}\\right)}\\right]}$\n\nRelational regression trees are then learnt using this soft-gradient.\n\n --  Odom et al. 2015 uses preference advice encoded in Horn clause form as a relational advice constraint and use it as cost function while learning the model.\n\nA relational advice constraint (RAC), F is defined using a Horn clause $\\wedgei fi(xi) \\implies label(xe)$, where $\\wedgei fi(xi)$ specifies the conjunction of conditions under which the advice applies on the example arguments $xe$. Each \\textit{relational advice rules} (RAR) is defined using the triple $\\langle  F, l+, l- \\rangle$ where $F$ is the RAC clause specifying the subset of examples, $l+$ is the preferred label and $l-$ is the avoided label. And, each relational advice set (RAS), $R$ is specified as a set of RAR. Cost function is defined as:\n\n$$\n  c\\left(x{i}, \\psi\\right)=-\\lambda \\times \\psi\\left(x{i}\\right) \\times\\left[n{t}\\left(x{i}\\right)-n{f}\\left(x{i}\\right)\\right]\n$$\n\nwhere, $nt$ indicates the number of advice rules that prefer the example to be true, $nf$ the number of rules that prefer it to be false, $\\lambda$ is scaling factor of the cost function and $\\psi(x_i)$ is the current value of the $\\psi$ function for the $x$.\n\nThe modified log-likelihood function is:\n\n$$\n    MLL(\\mathbf{x}, \\mathbf{y})=\\sum{x{i} \\in \\mathbf{x}} \\log \\frac{\\exp \\left(\\psi\\left(x{i} ; y{i}\\right)\\right)}{\\sum{y^{\\prime}} \\exp \\left(\\psi\\left(x{i} ; y^{\\prime}\\right)+c\\left(y_{i}, y^{\\prime}, \\psi\\right)\\right)}\n$$\n\nScaled gradient of this is\n\n$$\n\\eta \\Delta\\left(x\\{i}\\right) =\\alpha \\cdot \\left[ I\\left(y\\{i}=1\\right)-P\\left(y\\{i}=1 ; \\psi\\right) \\right] + (1 - \\alpha) \\cdot\\left[n\\t\\left(x\\i\\right)-n\\f\\left(x\\_i\\right)\\right]\n$$\n\nSo, when the example label is preferred target in more advice models than the avoided target, $nt(xi)-nf(xi)$ is set to be positive. This will result in pushing the gradient of these examples in the positive direction. And, $\\alpha$ is the linear trade-off between data and advice.\n\nThis approach was successfully adapted in relation extraction and healthcare problems (Odom et al., 2015a; Soni et al., 2016; Natarajan et al., 2017).\n\n-- Later, Odom  and  Natarajan  (2018) extended that constraint-based framework from Odom  et  al.  (2015b) by redefining  $l+$ and $l-$ in relational advice rules (RAR) triple $\\langle  F, l+, l- \\rangle$ to weighted label. $\\betat$ weight for preferred label and $\\betaf$ for the avoided label and modifying the cost function.\n\n$$\nc{LP}\\left(x{i}, \\psi\\right)=-\\lambda \\times \\psi\\left(x{i}\\right) \\times\\left[\\beta{t} \\times n{t}\\left(x{i}\\right)-\\beta{f} \\times n{f}\\left(x_{i}\\right)\\right]\n$$\n\nThis enables them to include following types of advice:\n\nPreferential advice:\n This advice is handled similar to the previous paper, except now magnitude of each constraint is determined by the weights $\\beta s$\nCost-based advice:\nThe soft margin approach in Yang wt al. 2014 is contained in the framework by\ncontrolling $\\beta s$. Specifically, $\\betat$ handles the sensitivity to false positives and  $\\betaf$ for false negatives.\nQualitative constraints:\nIn relational domain advice on qualitative constraints can be viewed as providing multiple pieces of advice over a given feature. As the number of objects satisfying this constraint increases more advice will apply.  $nt$ (or $nf$ ) to scale based on that feature.\nPrivileged information:\nThis is the features that are available during training but not during testing. To handle this, they define a cost function that reduces the KL divergence between $P(y|x)$ and $P(y|x^{RP})$ where $x^{RP}$ is the set of all the features including privilege features.\n\n$$\nc{R P}\\left(x{i}, \\psi\\right)=-\\lambda \\times K L\\left(P{D}\\left(y{i} | \\mathbf{x}{i}^{\\mathrm{RP}}\\right) \\| P\\left(y{i} | \\mathbf{x}_{i}\\right)\\right)  \n$$\n\nThe gradients obtained for the modified log-likelihood is as follows:\n\n$$\n\\eta \\Delta{L P}\\left(x{t}\\right)= \\alpha \\cdot \\left[ I\\left(y{t}=1\\right)-P\\left(y{t}=1 ; \\psi\\right) \\right] + (1- \\alpha) \\cdot\\left[\\beta{t} \\cdot n{t}\\left(x{t}\\right)-\\beta{f} \\cdot n{f}\\left(x{t}\\right)\\right]\n$$\n\n$$\n\\Delta{R P}\\left(x{i}\\right)=I\\left(y{t}=1\\right)-P\\left(y{t}=1 ; \\psi\\right) \\\\\n-\\alpha \\cdot\\left(P\\left(y{t}=1 | \\mathrm{x}{l}^{\\mathrm{CF}}\\right)-P{D}\\left(y{t}=1 | \\mathrm{x}_{t}^{\\mathrm{RP}}\\right)\\right)\n$$\n\n-- Odom  and  Natarajan  (2016) also introduced active advice seeking a framework for PLMs where model can query the human expert in the most useful areas of the feature space takes full advantage of the data as well as the expert. Advice here is a set of Queries from the model and responses from the expert. Each query is a conjunction of literals which represents set of examples and expert response is a preferred or avoid labels for that query.\n\nKey idea here is how generate the query. Model calculates the score of each example from the entropy of prediction\n\n$$\n        H\\left(x{i}\\right)=\\sum{l \\in \\text {Labels}} P{l}\\left(y{i} | x{i}\\right) \\log \\left(P{l}\\left(y{i} | x{i}\\right)\\right)\n$$\n\n These scores are used as regression values and a set of weighted first-order-logic clauses are learned that group examples. These clauses are posed as query to the expert.\n\n Conclusion\n\nThere are ways to incorporate the rich domain knowledge gathered by humans over course of time and hence we should not reinvent the wheel by making our model learn solely from the data. There can be situations where learning from data is enough but for complex and rich models it would not be sufficient and such approaches can be very useful in that case.",
        "tags": [
            "advice",
            "course",
            "survey"
        ]
    },
    {
        "uri": "/posts/AFE",
        "title": "Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach",
        "content": "\n\nMotivation\n\nFor the success of clinical studies, it is important to recruit people with diverse features. Not all the features are readily available when the decision about recruitment is done. Some features such as demographic details are available at no additional cost while other details like the MRI Image which are costly can be elicitated if the patient is recruited. But since these costly features are not already available during the decision of recruitment, we need a principled approach to make the decision of recruitment in the absence of costly features.\n\n Objective\n\nThe objective of this paper is to define the active feature elicitation problem and provide an approach to this problem.\n\n Active feature elicitation is the problem setting where the goal is to select the best set of examples on whom the missing features can be queried on to best improve the classifier\nperformance.  \n  img width=\"450\" alt=\"image\" src=\"https://user-images.githubusercontent.com/858059/81445033-c9080500-913d-11ea-9f78-2ee807754017.png\"  \n The top part shows the part of the data that is fully observed ($\\mathbf{E}\\{o}$). The bottom left quadrant shows the observed features($\\mathbf{X}\\{o}$) of the potential cohorts and the right quadrant is the data that needs to be collected ($\\mathbf{X}\\{u}$)for the most useful potential recruits. Given, the labels of the potential recruits ($y$), the goal is to identify the most informative cohorts that would aid the study($\\mathbf{E}\\{a} \\subset \\mathbf{E}\\_{u}$).\n\nSummary\n\nFirst, let us look at some baseline approaches.\n\nRandom ($RND$):\n         The most common approach to recruit cohort from potential candidates is to randomly pick the patients.\n\nUncertainty Sampling:\n\na) observed feature only ($USobs$):  \n          An informed approach is to look at all the observed features -- $\\mathbf{X}\\{o}$ for all the examples $\\mathbf{E} = \\mathbf{E}\\{o} \\cup \\mathbf{E}\\{u}$ and learn a classifier that predicts the label ($y$) given only observed features. Now, the potient recruitment subset $\\mathbf{E}\\{a}$ can be obtained by picking $n$ most uncertain examples from this classifier.\n\nb) all features ($USAll$):\n         More informed approach will be to impute the unobserved features -- $\\mathbf{X}\\{u}$ using mean, mode or some strategy, learn a classifier and   sample the subset $\\mathbf{E}\\{a}$ which has maximum prediction uncertainty.\n\nSampling based on the maximum prediction uncertainty means we recruit the candidates who have the highest entropy i.e. probability distribution [0.5,0.5] for binary classification.\n\nAlthough this makes sense in an active learning setting, where we want to obtain a label for example which has maximum uncertainty. In the current setting of active feature elicitation, where we want to recruit the most diverse cohort, the entropy does not make sense. The authors argue that we should look for the most diverse prediction distribution and propose the following approach.\n\nFirst, build two classifiers:    \n$Mo = P(y | \\mathbf{X}\\{o})$ which predicts class label based on only the observed features . Train it on all the examples $\\mathbf{E} = \\mathbf{E}\\{o} \\cup \\mathbf{E}\\{u}$.  \n\n$Mu = P(y | \\mathbf{X}\\{o}, \\mathbf{X}\\{u})$ which predicts class label based on all the features. Train it only on example set $\\mathbf{E}\\{o}$\n\nSince both the models are trained to achieve same prediction for the set  $\\mathbf{E}\\{o}$, we assume that the probability distribution of both the model is similar of set $\\mathbf{E}\\{o}$.\n\n$$\nP\\left(y^{j} | \\mathbf{X}\\{o}^{j}, \\mathbf{X}\\{u}^{j}\\right) \\approx P\\left(y^{j} | \\mathbf{X}\\{o}^{j}\\right) \\quad \\forall i \\in \\mathbf{E}\\{o}\n$$\n\nNow, we want to recruit candidates from set $\\mathbf{E}\\{u}$ which are different than the existing candidate set $\\mathbf{E}\\{o}$. If we use the probability distribution $P(yj | \\mathbf{X}\\{o}^j, \\mathbf{X}\\{u}^j)$ as a representative of each example in the set $\\mathbf{E}\\{o}$, we want to find example $i$ which is at maximum distance from $\\mathbf{E}\\{o}$. Since, we do not have $P(yi | \\mathbf{X}\\{o}^i, \\mathbf{X}\\{u}^i)$, authors propose to use $P(yi | \\mathbf{X}\\{o}^i)$ to represent examples from set $\\mathbf{E}\\_{u}$\n\nNow, we can use any divergence/distance measure to compute the farthest example. For example, if we use the KL divergence,\n\n$$\nD\\{i j}=\\mathrm{KL}\\left(\\mathrm{P}\\left(y^{i} | \\mathbf{X}\\{o}^{i}\\right) \\| \\mathrm{P}\\left(y^{j} | \\mathbf{X}\\{o}^{j}, \\mathbf{X}\\{u}^{j}\\right)\\right)\n$$\n\nwe can compute the mean distance for each example $i \\in \\mathbf{E}\\_{u}$:\n\n$$\n\\mathrm{MD}\\{i}=\\frac{1}{\\left|\\mathbf{E}\\{o}\\right|} \\sum\\{j=1}^{\\left|\\mathbf{E}\\{o}\\right|} D\\_{i j}\n$$\n\nand recruit the $n$ examples with maximum $MD_i$\n\nThe proposed approach is agnostic of any classifier and distance measure. Experiments show that the recruitment done using this approach was more informative.\n\nimg width=\"1279\" alt=\"image\" src=\"https://user-images.githubusercontent.com/858059/81451912-2c4c6400-914b-11ea-9906-b89c68c24a58.png\"\n",
        "tags": [
            "active-learning",
            "starling",
            "healthcare"
        ]
    },
    {
        "uri": "/posts/batch-RL",
        "title": "Fitted Q and Batch Reinforcement Learning",
        "content": "\ndiv class=\"box\"\n\nTerminologies\n\n Offline Planning Problem (MDP)\nWe are given the full MDP model and the problem is solved using all the components of the MDP\nOnline Planning Problem (RL)\nWe have limited knowledge of the MDP. We can discover it by interacting with the system\n Model-based RL\nApproaches to solving Online planning problem (RL) by first estimating (when missing) or accessing the full MDP Model i.e. transition and reward function and then finding policy $\\pi$ is called Model-based RL\nModel-free RL\nOn the contrary, approaches to solving the Online RL Problem directly, i.e. solving for $\\pi$ directly with either value function $V$ or state-action function $Q$ is called Model-free RL.\n\n/div\n\n Batch RL\nThe simulation environment is not present and complete set of transition samples $\\langle s, a, r, s^{\\prime} \n\\rangle$ is given and the challenge is to learn without exploring.\n\nBackground on Q Learning\nBellman optimality equation for the action-value function ($Q$) is given as:/p\n\n$$Q^{\\pi}(s, a)=\\sum{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\sum{a^{\\prime}} \\pi\\left(s^{\\prime}, a^{\\prime}\\right) Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\n$$\n\nwhere $T\\left(s, a, s^{\\prime}\\right)$ is a transition probability of landing in state $s^{\\prime}$ on taking action $a$ in state $s$ and $R\\left(s, a, s^{\\prime}\\right)$ is a Reward at state $s^{\\prime}$ reached on taking action $a$ in state $s$.\n\nIn the dynamic programming setting, the $Q$ function for optimal policy is represented as:\n\n$$Q{k+1}(s, a) \\leftarrow \\sum{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max {a^{\\prime}} Q{k}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\n$$\n\nQ-Learning is a model-free approach to learn the $Q$ function by exploring the environment, i.e. performing actions based on some policy. A table of Q function for each state action pair $Q(s,a)$ is maintained and the table in updated after every action using the running average formula:\n\n$$Q(s, a) \\leftarrow(1-\\alpha) Q(s, a)+(\\alpha)[\nR\\left(s, a, s^{\\prime}\\right)+\\gamma \\max_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)]\n$$\n\nWith multiple episodes the Q values will eventually converge and the optimal policy might be retrieved from that.\n\n Drawbacks of Q Learning.\n\nThere are several draw backs of the Q-Learning. These drawbacks might be minor in typical reinforcement learning setting where we have simulators. But these drawbacks are serious limitations in the a href=\"#BatchRL\"Batch RL/a setting. In Batch RL, the simulation environment is not present and complete set of transition samples ($\\langle s, a, r, s^{\\prime} \\rangle$) is given and the challenge is to learn without exploring.\n\nExploration overhead\n\nAs we see it in the figure below, at some point in the top left cell $(1,3)$, agent explored the action of going north and because it landed in the same cell, it updated $Q(s,north)=0.11$ as per the $\\max{a} Q(s,a)$ of that cell during that episode. After that episode, even though the $\\max{a} Q(s,a)$ of that cell changes, the $Q(s,north)$ does not get updated till the agent explores going north.\n\nimg src=\"/images/QLearning.gif\" alt=\"Q Learning\"\nblockquote\nsource: a href=\"https://www.youtube.com/watch?v=IXuHxkpO5E8&amp;t=3492s\"UC Berkeley CS188: Lecture of Pieter Abbeel/a\n/blockquote\n\n!--  --\n!-- More detail on 'exploration overhead' is found in  Batch RL chapter. --\n!-- This is where experience replay comes in. --\n\nStability issue\n\nQ-Learning has 'asynchronous update' i.e. after each observation the value is updated locally only for the state at hand and all other states are left untouched. In the above figure, we know the value at the Red tile, but the Q value for the tile below it is not updated until we explore the action of going to red tile from that tile.\n\nSimilar idea of asynchronous update is also applicable in function approximations where $Q$ function is estimated by a function and at every time step the function is updated using:\n\n$$f(s, a) \\leftarrow(1-\\alpha) f(s, a)+\\alpha \\left( r +\\gamma \\max_{a^{\\prime} \\in A} f\\left(s^{\\prime}, a^{\\prime}\\right) \\right)\n$$\n\nInefficient approximation\n\nThe 'asynchronous update' in function approximation is particularly harmful with global approximation functions. An attempt to improve $Q$ value of a single state after every time step might impair all other approximations. Specially when approximation function used is like Neural Network, where a single example can impact changes in all the weights. a href=\"#Gordon1995\"Gordon 1995/a proves that using an impaired approximation in next iteration, the $f$ function may divergence from the optimal $Q$ function.\n\nThis is where emfitted/em methods come in.\n\nhr\n\nFitted Approaches\n\na href=\"Gordon1995\"Gordon 1995/a provided a stable function approximation approach by separating dynamic programming step from function approximation step. Effectively, the above function update equation is now split to two steps.\n\n$$f^{\\prime}(s, a) \\leftarrow  r +\\gamma \\max_{a^{\\prime} \\in A} f\\left(s^{\\prime}, a^{\\prime}\\right)  \\, \\, \\, \\, \\forall s,a\n\\\\\nf(s, a) \\leftarrow(1-\\alpha) f(s, a)+\\alpha f^{\\prime}(s, a)\n$$\n\nObservation: Splitting the function update from one to two steps is equivalent to changing the gram-schmidt orthonormalization to modified-gram schmidt orthonormalization.\n\na href=\"#Ernst2005\"Ernst 2005/a proposed fitted Q iteration by borrowing the splitted approach from Gordon. The approach proposes iterative approximation of Q value by reformulating the Q Learning as a supervised regression problem. Algorithm proposed for fitted Q iteration is mentioned below.\n\npre class=\"editor-colors lang-{tidy=FALSE,\"spanspan class=\"syntax--text syntax--plain syntax--null-grammar\"Given: tuples {&lt;s,a,r,s'&gt;}, stopping condition/span/span\nspan class=\"\"span class=\"syntax--text syntax--plain syntax--null-grammar\"/span/span\nspanspan class=\"syntax--text syntax--plain syntax--null-grammar\"1. Q(s, a) = 0/span/span\nspanspan class=\"syntax--text syntax--plain syntax--null-grammar\"2. while (!stopping condition):/span/span\nspanspan class=\"syntax--text syntax--plain syntax--null-grammar\"3.    Build a training set:/span/span\nspanspan class=\"syntax--text syntax--plain syntax--null-grammar\"span class=\"leading-whitespace\"         /span{feature; regression value} = {&lt;s,a&gt; ; r + max_a Q(s,a)}/span/span\nspanspan class=\"syntax--text syntax--plain syntax--null-grammar\"4.    Learn a function approximating the regression values Q (s,a)/span/span/pre\npThis is in principle similar to equations mentioned above, with $f^{\\prime}$ as regression value and  $\\alpha=1$./p\npFurther extensions to the fitted Q approaches have learnt $f$ function as some linear combination of the previous function and regression values./p\n\nReferences\na name=\"BatchRL\"/aLange, S., Gabel, T., &amp; Riedmiller, M. Batch Reinforcement Learning. 2012  \na name=\"Gordon1995\"/aGordon, G. J. Stable Function Approximation in Dynamic Programming. ICML. 1995.  \na name=\"Ernst2005\"/aErnst, D., Geurts, P., &amp; Wehenkel, L. Tree-Based Batch Mode Reinforcement Learning. In JMLR. 2005.  \na href=\"http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf\"http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf/a",
        "tags": [
            "RL",
            "offline",
            "summary"
        ]
    },
    {
        "uri": "/postsbayesian-logic",
        "title": "BLOG: Relational Modeling with Unknown Objects",
        "content": "\n\nMost of the First-Order Logic Models we have seen till now like Bayesian Logic Programs or MLNs works with an assumption that we are given a fixed set of objects, a Herbrand Universe and we have a possible world, Herbrand Interpretation on which we build our BLP model or MLN Network. However, in many practical problems we do not have a fixed Herbrand Universe, or we do not know how many objects are there in the universe. This paper introduces BLOG (Bayesian LOGic) which can perform inference even when the objects are unknown. BLOG is a representation language which can generate objects for such an unknown universe and create a Bayesian network which can then be used for inference. BLOG model describes a stochastic process which creates a world by generating objects in an order. The existence of a generated object is governed by the unknown number of objects in the universe and by the random functions defined in the model. The key idea here is that any property of the newly created object depends only on the objects that are already created. Using this and the concept of context specific independence, BLOG generates a countably infinite Bayesian network in which each node has finite set of parents. The finite set of parents and prior knowledge about the distribution makes the inference possible.\n\nKey contribution\n\nAccording to me, the key contribution of this paper is the representation language. This language helps us model random functions and probabilistic properties of unknown objects. Consider the Balls in Urn example provided in the paper. We  do not know the number of balls in the urn and we randomly draw balls and put it in.  We  have no way  to identify if the same ball was drawn twice. We can only detect the color of the ball and that too has false detection probability of 0.2. With this information, we are supposed to find out number of balls in the urn. Most other models will give up at the first step itself. BLOG provides a simple language which can model such a stochastic scenario by using just 6 kinds of statements: Type Declarations, Random Function Declarations, Non-random Function Definitions, Guaranteed Object Statements, Dependency Statements, Generating Functions, and Number Statements. Once we have the model, we can represent any possible instantiation of the world.\n\n Limitations\n\nAs BLOG creates a Random Variable for every function and object type, we potentially get a Bayesian Network with exponential number of Random Variables. Although context specific independence assumption helps us do the inference, it is computationally impossible to work with a such huge Bayesian Network. This makes the whole development less viable in practical scenarios as the purpose of the model was to provide inference over unknown or non-finite universe.\n\nRepresentation can have unnecessary variables. For example, if we have the evidence that balls drawn in Draw1 and Draw2 are equal, model creates a new constant for that. Also, we would still have all random variables for both Draw1 & Draw2 like ObsColor[Draw1], ObsColor[Draw2]. So, there will be 2n extra random variables where n is the number of functions for Draw object.\n\nTo make the Bayesian network inferable, BLOG makes the context specific independence assumption. But this assumption is not straight forward. For instance, in the aircraft example, a blip at time t is dependent on the state of all the aircraft at time t. To make it dependent only on its source aircraft, we would need evidence or prior knowledge about the source of the blip.\n\nAs the BLOG assumes that any newly created object depends only on the already existing object, it is assuming the universe to be acyclic and objects have topological order.\n\nThe model cannot learn any prior distributions. It assumes that it has all the prior distributions from the domain expert.\n\nAs each object is stored as a tuple, they can expand to considerable size. For example, in the aircraft model each blip is represented as (Blip, (Source, (Aircraft, 2)), (Time, 8), 1), if the object blip had k properties the tuple size would increase further.\n\nCritique\n\nPros:\n\nThe examples of the stochastic world taken are motivating.\nThe paper provides a beautiful explanation on the difference between conditioning on the constant versus conditioning on the existence.\nThese new constants resemble Skolem constants, but conditioning on assertions about the new constants is not the same as conditioning on an existential sentence. For example, suppose you go into a new wine shop, pick up a bottle at random, and observe that it costs USD 40. This scenario is correctly modeled by introducing a new constant Bottle1 with a Uniform CPD. Then observing that Bottle1 costs over USD 40 suggests that this is a fancy wine shop. On the other hand, the mere existence of a USD 40+ bottle does not suggest this, because almost every shop has some bottle at over USD 40.”\n\nCons:\n\nThe paper represents the model, functions, worlds, etc,. in mathematical expressions but doesn’t provide strong mathematical proof/explanation for the unique joint distribution.\nThere is inconsistency in the models. For instance, Aircraft example has a number function for creating the blip which assigns the source and time simultaneously. However, in the Ball example, true color of balls is not assigned when it is created using number function. Similarly, the number function of Publication assigns Author but the generating function for author is missing. Hence, it appears that there can be multiple models to represent the same scenario. However, there is no explanation if different representation would result in same joint distribution.\nPaper doesn’t put forward enough experimental evidence. The experiment conducted asserts that 10 balls were drawn and all appeared blue. The query was made about number of balls in the urn. Results showed that when the prior for the number of balls is uniform over 1, . . . , 8, the posterior puts more weight on small numbers of balls. And this experiment was validated by saying ”this makes sense because the more balls there are in the urn, the less likely it is that they are all blue”. Few more experiments would have helped.\n\n References\n\nGETOOR, LISE and TASKAR, BEN Introduction to Statistical Relational Learning (Adaptive Computa- tion and Machine Learning), The MIT Press.",
        "tags": [
            "SRL",
            "course"
        ]
    },
    {
        "uri": "/posts/causal-tools",
        "title": "Tools for Causal Inference",
        "content": "\nThere is enough motivation now showing the necessity of learning models that have a correct causal structure. The famous \"Correlation is not causation\" quote should ring a bell. Although the research on learning causal structure from observed data has not yet shown its potential. There are certain tasks from causal literature which can still be solved by just using observed data. Specifically, interventional and counterfactual queries can still be answered if we know the causal structure.\nAnd, to do that we need the following tools of causal inference popularized by Prof. Pearl.\n\nTool 1. Encoding causal assumptions: Transparency and testability.\n\nTool 1 emphasize the importance of graphical representation of causality using a Bayesian Network. By representing the causal influences using BN, we can leverage the three rules of conditional independences for inference.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/BN-CI-rules.png\"\n/div\n\nAlthough very powerful, the BN representation is limited. Only with graphical representation we will not be able to differentiate between $P(y|X=3)$ and $P(y|do(X=3))$. Ferenc Husz&aacute;r gives a beautiful explanation of the difference between the two in his post on causal inference supa href=\"ref1\"1/a/sup.\nGiven different causal BN and data generated from different distributions as shown in the figure below, we can see that the joint distribution of $x$ any $y$ is identical. Even the conditional probability $P(y|X=3)$ is identical.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/Causal-intervention1.png\"\nbr\nimg align=\"center\" width=\"300\"  src=\"/images/Causal-intervention5.png\"\n\npsrc: <a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\n\"Causal Inference by Ferenc Husz&aacute;r/a/p\n/div\n\nBut, what if we force the variable $x$ to take a fixed value $3$ in each of these cases, i.e. intervene on variable $x$? Should we still expect the distribution of y to be same? No. As expected the impact of intervening on variable $x$ has different impact on values of $y$. This is evident in the two distribution plotted below.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/Causal-intervention3.png\"\nbr\nimg align=\"center\" width=\"300\"  src=\"/images/Causal-intervention4.png\"\npsrc: <a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\n\"Causal Inference by Ferenc Husz&aacute;r/a/p\n/div\n\nAbove values of $p(y|do(X=3))$ is computed by generating the data after forcing $x=3$. In machine learning, we want to learn the impact of assigning value $x=3$ without re-generating the data. So, how do we compute $p(y|do(X=3))$ without generating the data? We need more advanced tools for that.\n\n Tool 2. Do-calculus and the control of confounding.\n\nIn the above example, intervening on variable $x$ and fixing it to $3$ is like making $x$ independent of all its ancestor. So, we remove all the incoming edges to $X$. Now, Computing $P(y|X=3)$ of the resulting graph will give us $P(y|do(X=3))$ of the original graph.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/Causal-intervention2.png\"\npsrc: <a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\n\"Causal Inference by Ferenc Husz&aacute;r/a/p\n/div\n\nThis is what do-calculus is. The process of mapping the do-expressions to the standard conditional probability equations using the three rules. The probability equations we get at the end are called estimand and the the value obtained after computing the estimand on the observed data is called estimate.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/causal-dorules.png\"\n/div\n\nOne of the most common adjustment in the causal inference is de-confounding i.e. adjusting for all the common causes (confounders) of two variables. This is the famous backdoor criterion. Back-door path for  $X \\rightarrow Y$ is any path that ends in $Y$ and has arrow pointing into $X$. To de-confound $X$ and $Y$, we have to block all the back-door path by adjusting for (i.e. conditioning on) variables which either follow chain-rule or common ancestor rule. We have to be very careful to not condition on any common descendent in the back-door path.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/causal-backdoor.png\"\n/div\n\nFor graphs where we are unable to block the back-door path (say, because of unobservable variable), do-calculus gives us a work-around. This is commonly called the front-door criterion.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/causal-frontdoor.png\"\n/div\n\nFor graph given above, the front-door formula is derived as follows:\n\n$$\n\\begin{align}\nP(Y |do(X= \\tilde{x})) & = \\sum_z P(Y|do(X = \\tilde{x}), Z=z) P(Z=z|do(X = \\tilde{x})) \\\\\\\\\n & \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\rhd \\text{probability axiom} \\\\\\\\\n & = \\sum_z P(Y|do(X= \\tilde{x}),do(Z=z)) P(Z=z|do(X= \\tilde{x})) \\\\\\\\\n &  \\quad \\quad  \\quad \\quad  \\quad \\rhd \\text{ back door-path between X and Y is blocked by X,}\\\\\\\\\n& \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad \\text{so we apply exchange do-rule } \\\\\\\\\n& = \\sum_z P(Y|do(X= \\tilde{x}),do(Z=z)) P(Z=z|X= \\tilde{x}) \\\\\\\\\n&  \\quad \\quad  \\quad \\quad  \\quad \\rhd \\text{exchange do-rule,  back door-path between X and Z }\\\\\\\\\n& \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad \\text{ is blocked by common descendent Y} \\\\\\\\\n& = \\sum_z P(Y|do(Z=z)) P(Z=z|X= \\tilde{x}) \\\\\\\\\n&  \\quad \\quad  \\quad \\quad  \\quad \\rhd \\text{action rule, No forward path from X to Y }\\\\\\\\\n& \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad \\text{ as do(Z) blocks it.} \\\\\\\\\n& = \\sumz  \\left( \\sumx P(Y|X=x, Z=z) P(X=x) \\right) P(Z=z|X= \\tilde{x}) \\\\\\\\\n&  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad  \\quad  \\quad \\quad  \\quad  \\rhd \\text{using back-door adjustment }\\\\\\\\\n& = \\sumz   P(Z=z|X= \\tilde{x}) \\sumx P(Y|X=x, Z=z) P(X=x) \\\\\\\\\n&  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad \\quad  \\quad  \\quad  \\quad \\quad  \\quad  \\rhd \\text{same as front-door criterion }\\\\\\\\\n\\end{align}\n$$\n\nIts amazing, to compute the $P(Y | do(X= \\tilde{x}))$, we have to sum over all the possible assignments of $x$. With only Tool 1, I would have mostly computed probability by only looking at data points where $X = \\tilde{x}$.\n\n<!--\nThe Book of Why mentions the following napkin problem where we cannot use make the front-door or back-door adjustments directly. However, we can still use do-calculus.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/causal-napkinproblem.jpg\"\n/div\n\n$$\nP(Y|do(X)) = \\sumw \\sumz P(y |do(X),W=w,Z=z)P(Z=z|do(X), W=w) P(W=w |do(X))\n$$\n\nSince, W and Z blocks backdoor between X & Y\n\nSince, W blocks the backdoor between X & Z, using exchange do-rule we see\n\n$$\nP(Z=z|do(X),W=w) = P (Z=z|X,W=w)\n$$\n\nSince, back-door for $X \\rightarrow W$ us blocked by Z, exchange do-rule gives\n\n$$\nP(W=w|do(X)) = \\sum_{z^{\\prime}} P(W=w|X, Z=z^{\\prime}) P(Z=z^{\\prime})\n$$ --\n\nTool 3. The algorithmitization of counterfactuals.\n\ndiv style=\"background-color:5ADDD7;padding:0.5px 10px\"\nh3Thumb rule of counterfactual/h3\nh3 style=\"font-weight:400\"counterfactuals talks about a specific individual/event/scenario./h3\n/div\n\nbr br\n\nCounterfactual queries are questions about a parallel universe where one(or few) feature(s) is(are) different for a specific example, not on the whole population. It is a question always asked in retrospection for a specific instance. For example, An interventional question to analyze importance of wearing mask during current pandemic can be asked as \"What would be the total number of COVID-19 positive patients, if we banned mask in public?\" i.e $P($ COVIDPos $| \\, do($ wearmask $= 0) )$. But a counterfactual question would be \"Given that I had worn mask when I went to buy grocery last week and I do not have COVID-19 today, what would have happened if I had not worn mask?\" This question is about me, and about a specific instance of not wearing mask. This counterfactual question has to account for every variable which is not a descendent of wearmask staying exactly the same. So, the fact that I drove to the TomThumb or the cart which I used should stay the same, because in my *assumed* causal graph, they are not descendants of wearmask variable. But on the other hand the fact that I washed my hands and face for exactly 12 secs after coming home, and that I entered the spice aisle might change. Because in my causal assumption these are descendent of wearing mask. Hey! I would have washed hands and face for 20 secs if I was not wearing mask and I would have definitely not entered the spice aisle which had like 3 people in there. I would have maintained six feet distance.\n\nSo, mathematically the counterfactual query is $P(hc^*=1|hm^\\ast=0,hc=0, hm=1,..)$, where, $hc$ is Harsha has COVID-19, $hm$ is Harsha wore mask, and \\ast indicates counterfactual universe. So, all the variable that are not descendants of $hm$ will stay same i.e $hi = hi^\\ast, \\forall i \\notin Des(hm)$. To compute the probability of the counterfactual query, we need more than the do-calculus from Tool-2.  We need structural equation models (SEMs) and an algorithm.\n\n I strongly recommend Ferenc Husz&aacute;r's post on counterfactuals for more clarity on how the interventional probability $P(Y| do(X))$ is the computation on population, but counterfactuals is for individuals.\n\nLet's see how we use tool 3 to answer the counterfactual query, \"What would Alice’s salary be if she had a college degree?\". This query is from the fictitious employee data reproduced below from The Book of Why. Here, the experience is mentioned in number of years and three levels of education are : 0 = high school degree, 1 = college degree, 2 = graduate degree. The assumed causal graph is also shown below.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/causal-employee.png\"\nimg align=\"center\" width=\"250\"  src=\"/images/causal-employeegraph.png\"\npsrc: The Book of Why/p\n/div\n\nThe SEM for this graph will be:\n\n$$\nf0(Ed) = \\, U{0} \\\\\nf1(Ex) = b1 \\, + \\, a1 Ed \\, + \\, U{1} \\\\\nf2(S) = b2 \\, + \\, a2 Ed \\, + a2 Ex \\, +  \\, U_{2}\n$$\n\nHere, U are unobserved idiosyncratic variables which capture the peculiarity of the individual in question. So, these values are different for each user. For instance, We can think of $U_2$ accounting for difference in salary as some artifact of individuals unobserved characteristics which is independent of level of education and experience (say, time management). In terms of graphical model, we can think of these variables as parents of the variable on the left side of the equation. Note that these idiosyncratic variables do not have incoming arrows. So, given the counterfactual explanation above, these values should remain unchanged in the parallel universe. If we were to do linear regression we would have an $\\varepsilon$ instead of $U$ to adjust for the noise. However, the noise is then a random parameter which is not necessarily kept constant across universe. This is where the SEM differ from linear regression.\n\nThe SEM coefficients can be estimated from the observed data, much like linear regression.\n\n$$\nf1(Ex) = 10 – 4 Ed + U1 \\\\\nf2(S) = $65,000 + 5,000 Ed +  2,500 Ex +  U2\n$$\n\nNow, the algorithm to compute the counterfactual query is these three steps:\n\nbr\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/causal-counterfactualalgo.png\"\n/div\n\nbr\n\nFollowing these steps for Alice here, in step 1 we find that $U1(Alice) = \\, –4$ and $U2(Alice) = $1,000$. We do not actually care for $U0(Alice)$ because in **step 2** we set $f0(Ed(Alice)) = 1$. 1 for college degree, and remove all incoming arrows to $Ed$ and perform do-calculus adjustments. Here, since there are no causal parents of $Ed$, our SEM model equations $f1$ and $f2$ do not change.\nThen, in step 3, we estimate the counterfactual parameters, $Ex^\\ast(Alice) = 2$ and $Ed^\\ast(Alice) =  76000$.\n\nRest of the tools.... to be added later.\n\nCritique\n\nI certainly believe that the advent of ML and AI necessitates the task of learning the causal structure from observed data. The three tools mentioned above do not do that. They assume the causal structure is provided. These tools provide us the correct way of utilizing the causal structure in machine learning. I believe most of the machine learning models build for classification/regression tasks assume some form of causal structure already. What is not clear is, if those models correctly leverage the causal structure that they are assuming. These tools help us answer that. The question about whether or not the causal structure assumed is correct or not can be subjective and task specific. The three tools mentioned above certainly do not answer that.\n\n References\n\nThe Book of Why: The New Science of Cause and Effect by Judea Pearl and Dana Mackenzie\nThe Seven Tools of Causal Inference, with Reflections on Machine Learning by Judea Pearl\nsupa id=\"ref1\" name=\"ref1\"1/a/suphttps://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\nCausality: Models, Reasoning and Inference by Judea Pearl\n",
        "tags": [
            "causal",
            "summary"
        ]
    },
    {
        "uri": "/posts/DRRL",
        "title": "Deep Reinforcement Learning With Relational Inductive Biases",
        "content": "\n\nDeep RL methods have been every effective but they have poor generalization capability, especially combinatorial generalization (for eg. if the number of blocks are changed in the blocks world). Recent advances in graph network literature have achieved combinatorial generalization by learning neural network that can reason about relationship of various nodes in graphs. Since this reasoning happens pairwise, the algorithms are able to scale to varying number of objects.\n\nIn this paper, authors introduce how multi-headed dot product attention can be used to perform relational reasoning in model-free deep RL and hence achieve combinatorial generalization.\n\nMulti-Head Dot Product Attention (MHDPA)\n\nThis is the self attention mechanism proposed in the paper Vaswani et al. NeurIPS 2017, Attention is all you need. In that paper, the MHDPA was used on an input of word embeddings but in general it can be any form of entities. Check out the neat explanation of MHDPA by Jay Alammar here\n\nOn a very high level, attention mechanism\n\nconverts these entities ($X$) to Queries ($Q$), Keys ($K$) and Values ($V$) supa href=\"stack-ref\"1/a/sup,  \ncomputes the similarity score between each query and key $QK^{T}$,  \nscales and normalizes it to a distribution: $\\operatorname{Softmax}(\\frac{Q.K^{T}}{\\sqrt{d}})$.\noutputs the weighted values based on this distribution: $Z = \\operatorname{Softmax}(\\frac{Q.K^{T}}{\\sqrt{d}})\\cdot V$\n\nMulti headed version of attention does two additional steps\n\nConcatenates all the attention outputs $(\\mathbin\\Verti Zi)$\nTransform it original $X$ dimension by multiplying it with weight matrix $W$\n\ndiv align=\"center\"\nimg align=\"center\" width=\"800\"  src=\"/images/MHDPA.png\"\npsrc: a href=\"https://jalammar.github.io/illustrated-transformer/\"The Illustrated Transformer by Jay Alammar/a\n/div\n\nSummary\n\nZambaldi et al. proposes to use the MHDPA (with image embeddings as entities) to perform relational-reasoning while training a network for distributed A2C model. First the images from the box-world domain are processed through a convolutional neural network in the \"input module\". The spatial representation learnt from the CNN is then used as embedding after concatenating $x$ and $y$ co-ordinate as additional features. MHDPA is used to perform manipulations between this entities a.k.a. relational-reasoning. Finally the multiple attention heads are aggregated by another multi-layered-perceptron $g_\\theta$ (instead of the weight matrix $W$ used in Vaswani et al. 2017). Then in output module max-pooling is performed and a FC layer converts it to actor policy $\\pi$ and critic's state-value (or advantage value) $B$.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"1000\"  src=\"/images/DRRL-architecture.png\"\n/div\n\n Authors mention that the use of a $g_\\theta$, a non-linear MLP, in the final stage is aligned with the use of MLP in relational-network paper where a MLP is used to manipulate the relation embeddings.\n\nQualitative Analysis of the attention heads show that they infact learn lock-key relationship and also a relationship between agent and entities.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"1000\"  src=\"/images/DRRL-attention.png\"\n/div\n\n References\n\nIllustrated Transformer\nsupa name=\"stack-ref\"1/a/sup[What exactly are keys, queries, and values in attention mechanisms?\n](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)\nThe idea behind Actor-Critics and how A2C and A3C improve them",
        "tags": [
            "RL",
            "GNN",
            "neurosymbolic"
        ]
    },
    {
        "uri": "/posts/few-shot-learning-GNN",
        "title": "Few-Shot Learning with GNN",
        "content": "\n\nThis paper focuses on $q$-shot $K$-way classification problem -- where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels.\n\nGiven dataset $\\\\{\\left(\\mathcal{T}\\{i}, y\\{i}\\right)\\\\}\\{i\\leq L}$ containing Task $\\mathcal{T}i$ and true label $y\\_{i}  \\in\\\\{1, K\\\\}$ for a single test image $\\bar{x}$.\n\n$$\n\\mathcal{T}=\\left\\\\{ \\underbrace{ \\left\\\\{\\left(x\\{1}, l\\{1}\\right), \\ldots\\left(x\\{s}, l\\{s}\\right)\\right\\\\}}\\{\\text{supervised samples}}, \\underbrace{\\left\\\\{\\bar{x}\\right\\\\}}\\{\\text{test samples}} ; \\right\\\\}\n$$\n\nEmbeddings$-\\phi(.)$ are obtained from a convolutional neural network (CNN) for all the images ($\\\\{xi\\\\}1^s \\cup \\bar{x}$),  as highlighted in the image below.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/kshotGNN-GNN_full.png\"\n/div\n\nThese embeddings are then concatenated with one-hot encoding of the labels $(h(l))$. Together they form nodes $(V)$ of the graph.\n\n$\\mathbf{x}\\{i}^{(0)}=\\left(\\phi\\left(x\\{i}\\right), h\\left(l\\_{i}\\right)\\right)$\n\n a name=\"a4\"i class=\"fa fa-bolt\"/i/a For $k$ labels, $h(l)$ is a binary vector of size k. One-hot encoding is obtained for training image $xi$ with the label $li=3$, by setting all the values in $h(li)$ to $0$ except for $3^{rd}$ element which is $1$ i.e. $[0,0,1,0,0...]$. For test image, since the label is not known in $\\mathcal{T}$, uniform distribution is used instead of one hot encoding i.e. $h(l) = k^{-1}\\mathbf{1}k$, for $\\bar{x}$.\n\n superscript $^{(p)}$ indicate the $(p+1)^{th}$ layer input. So, $x^{(0)}$ indicates the node embeddings for first layer in GNN.\n\nEdges $(E)$ of the graph are learnt as adjacency matrix $\\tilde{A}$ using a MLP.\n\n$$\n\\tilde{A}\\{i,j} = \\varphi\\{\\tilde{\\theta}}\\left(\\mathbf{x}\\{i}, \\mathbf{x}\\{j}\\right)= \\operatorname{MLP}\\{\\tilde{\\theta}}\\bigg( \\operatorname{abs}\\Big( \\phi(xi) - \\phi(x_j) \\Big) \\bigg)\n$$\n\nThen Graph Convolution ($Gc(.)$) is performed on the nodes $V$ and adjacency matrix $A$. a name=\"a10\"i class=\"fa fa-bolt\"/i/a.\n\n$$\n\\mathbf{x}\\{l}^{(k+1)}=\\operatorname{Gc}\\left(\\mathbf{x}^{(k)}\\right)=\\rho\\left(\\sum\\{B \\in \\mathcal{A}} B \\mathbf{x}^{(k)} \\theta\\_{B, l}^{(k)}\\right) \\\\\n\\mathcal{A} = \\\\{\\tilde{A}^{(k)}, \\mathbf{1}\\\\}\n$$\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/kshotGNN-blocks.png\"\np align=\"center\"Proposed block of GNN containing the Adjacency matrix and Graph Convolution./p\n/div\n\nThe block containing learning of adjacency matrix and graph convolution is repeated multiple times in the experiments and output of the last layer is passed through a sigmoid activation function to obtain class probabilities  a name=\"a8\"i class=\"fa fa-bolt\"/i/a.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"/images/kshotGNN-GNN.png\"\n/div\n\nThe complete network, including the adjacency matrix is trained end-to-end with following objective a name=\"a3\"i class=\"fa fa-bolt\"/i/a.\n\n$$\n\\min \\{\\Theta} \\frac{1}{L} \\sum\\{i \\leq L} \\underbrace{\\ell\\left(\\Phi\\left(\\mathcal{T}\\{i} ; \\Theta\\right), Y\\{i}\\right)}\\{\\text{loss w.r.t. test samples}}+\\underbrace{\\mathcal{R}(\\Theta)}\\{\\text{regularizer}} \\\\\n\\ell(\\Phi(\\mathcal{T} ; \\Theta), Y)= \\underbrace{-\\sum\\{k} y\\{k} \\log P\\left(Y=y\\{k} | \\mathcal{T}\\right)}\\{\\text{cross entropy loss}}\n$$\n\n Note that the labels/predictions of the training data are not used to compute the loss. Hence the GNN will not overfit to the $h(l)$ part of the embedding.\n\nThe over all idea of finding image embeddings and then using distance between the image embeddings to find the class label is explored in various other papers. Authors show equivalence of their approach to three other papers.\n\n1. Convolutional Siamese Neural Network\n\nKoch et al. 2015 proposed to use Convolutional Siamese Neural Network (CSNN) to learn similarity between two images, i.e probability that both images were drawn from same label set.\n\nThey used two parallel twin CNN to obtain embeddings to two input images $f\\_{\\theta}(x)$, computed absolute distance between these embeddings (d), and then computed probability by passing the distance from linear feedforward layer and sigmoid function.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/kshotGNN-siamese.png\"\n/div\n\nCSNN are trained to reduce the following loss function for all pairs of images.\n\n$$\n\\mathcal{L}(B)=\\sum\\{\\left(\\mathbf{x}\\{i}, \\mathbf{x}\\{j}, y\\{i}, y\\{j}\\right) \\in B} \\mathbf{1}\\{y\\{i}=y\\{j}} \\log p\\left(\\mathbf{x}\\{i}, \\mathbf{x}\\{j}\\right)+\\left(1-\\mathbf{1}\\{y\\{i}=y\\{j}}\\right) \\log \\left(1-p\\left(\\mathbf{x}\\{i}, \\mathbf{x}\\_{j}\\right)\\right)\n$$\n\nWhile testing, the test image ($\\mathbf{x}$) is compared with all the training images ($S$) and 1 Nearest-Neighbour approach is used to assign the class label.\n\n$$\n\\hat{c}\\{S}(\\mathbf{x})=c\\left(\\arg \\max \\{\\mathbf{x}\\{i} \\in S} P\\left(\\mathbf{x}, \\mathbf{x}\\{i}\\right)\\right)\n$$\n\nEquivalence to GNN\n\nGNN approach is similar to CSNN if we make following changes:\n\nRestrict node embeddings:\n\n$$\\operatorname{GNN}:  \\mathbf{x}\\{i}^{(0)}=\\left(\\phi\\left(x\\{i}\\right), h\\left(l\\_{i}\\right)\\right)$$\n$$\\operatorname{CSNN}: \\mathbf{x}\\{i}^{(0)}=\\phi\\left(x\\{i}\\right) = f\\{\\theta}(xi)$$\n\nFix Adjacency matrix:\n\n$$\\operatorname{GNN}:  \\tilde{A}\\{i,j} = \\operatorname{MLP}\\{\\tilde{\\theta}}\\bigg( \\operatorname{abs}\\Big( \\phi(xi) - \\phi(xj) \\Big) \\bigg)$$\n$$\\operatorname{CSNN}: \\tilde{A}\\{i,j} = \\operatorname{softmax}\\bigg( - || \\phi(xi) - \\phi(x_j) || \\bigg)$$\n\nReduce the convolution block:\n\n$$\\operatorname{GNN}: y = \\sigma \\Big( \\sum\\{B \\in A} B \\mathbf{x}^{(k)} \\theta\\{B,l}^{(k)} \\Big)$$\n$$\\operatorname{CSNN}: Y\\{\\ast} = \\sum\\{j} \\tilde{A}\\{\\ast, j}^{(0)}\\left\\langle\\mathbf{x}\\{j}^{(0)},  u\\right\\rangle$$\n\nwhere, $\\langle .,. \\rangle$ indicates elementwise multiplication and  $u$ is a binary vector for selection of labels. So, it is 1 only for elements which correspond to labels in $\\mathbf{x}$ and 0 otherwise. $\\\\{\\ast\\\\}$ indicates the test example. a name=\"a5\"i class=\"fa fa-bolt\"/i/a.\n\n 2. Matching Network\n\nVinyals et al., 2016 extended the SCNN approach of $2$-way comparison to $k$-way comparison by using an attention function $a(.,.)$ to compute cosine similarity of test image embedding $f\\{\\theta}(\\mathbf{x})$ with  input image embeddings $g\\{\\theta}(\\mathbf{x_i})$ and use it as a weighted sum to compute the class probability of the test image.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/kshotGNN-matchingNetwork.png\"\n/div\n\n$$\n\\begin{array}{l}c\\{S}(\\mathbf{x})=P(y | \\mathbf{x}, S)=\\sum\\{i=1}^{k} a\\left(\\mathbf{x}, \\mathbf{x}\\{i}\\right) y\\{i}, \\text { where } S=\\left\\\\{\\left(\\mathbf{x}\\{i}, y\\{i}\\right)\\right\\\\}\\{i=1}^{k} \\\\ a\\left(\\mathbf{x}, \\mathbf{x}\\{i}\\right)=\\frac{\\exp \\left(\\operatorname{cosine}\\left(f(\\mathbf{x}), g\\left(\\mathbf{x}\\{i}\\right)\\right)\\right.}{\\sum\\{j=1}^{k} \\exp \\left(\\operatorname{cosine}\\left(f(\\mathbf{x}), g\\left(\\mathbf{x}\\_{j}\\right)\\right)\\right.} \\\\\n\\end{array}\n$$\n\n Note here $k$ is the number of examples not the number of class labels.  \n\nParameters $\\theta$ are trained to maximize the loglikelihood of the test images.\n\n$$\n\\theta=\\arg \\max \\{\\theta} E\\{L \\sim T}\\left[E\\{S \\sim L, B \\sim L}\\left[\\sum\\{(x, y) \\in B} \\log P\\_{\\theta}(y | x, S)\\right]\\right]\n$$\n\nEquivalence to GNN\n\nGNN approach can be considered equivalent to Matching Network if $f = g$ and $\\mathbf{x}\\{i}^{(0)}=\\phi\\left(x\\{i}\\right) = f\\{\\theta}(xi)$ and $A\\{i,j} = a(\\mathbf{xi, x_j})$.\n\n2. Prototypical Network\n\nSnell et al. 2017 reduced the number of comparisons in Matching Network by computing a class representative for each class and instead of comparing with all available images, the test image is only compared with the class representatives.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/kshotGNN-prototypical.png\"\n/div\n\nThis paper first computes embeddings of all images $f\\{\\theta}(xi)$ and then computes a class representative/prototype $\\mathbf{v}_c$ for each class as mean of those embeddings.\n\n$$\n\\begin{array}{c}\\mathbf{v}\\{c}=\\frac{1}{\\left|S\\{c}\\right|} \\sum\\{\\left(\\mathbf{x}\\{i}, y\\{i}\\right) \\in S\\{c}} f\\{\\theta}\\left(\\mathbf{x}\\{i}\\right) \\\\\n\\end{array}\n$$\n\nThe class probability distribution of test example is then computed by taking softmax of the distance between the embedding of test image and the class prototype. The distance measure used here is squared euclidean distance.\n\n$$\nP(y=c | \\mathbf{x})=\\operatorname{softmax}\\left(-d\\{\\varphi}\\left(f\\{\\theta}(\\mathbf{x}), \\mathbf{v}\\{c}\\right)\\right)=\\frac{\\exp \\left(-d\\{\\varphi}\\left(f\\{\\theta}(\\mathbf{x}), \\mathbf{v}\\{c}\\right)\\right)}{\\sum\\{c^{\\prime} \\in \\mathcal{C}} \\exp \\left(-d\\{\\varphi}\\left(f\\{\\theta}(\\mathbf{x}), \\mathbf{v}\\{c^{\\prime}}\\right)\\right)}\n$$\n\nThe network is trained to reduce the negative loglikelihood.\n\n$$\\mathcal{L}(\\theta)=-\\log P\\_{\\theta}(y=c | \\mathbf{x})$$\n\nEquivalence to GNN\n\nGNN approach can be reduced to Prototypical Network if we make following changes.\n\nRestrict the node embeddings to $\\mathbf{x}\\{i}^{(0)}=\\phi\\left(x\\{i}\\right) = f\\{\\theta}(xi)$\n\nFix the Adjacency Metric:\n\n$$\n\\tilde{A}\\{i, j}^{(0)}=\\left\\\\{\\begin{array}{cc}q^{-1} & \\text {if } l\\{i}=l\\_{j} \\\\ 0 & \\text { otherwise }\\end{array}\\right.\n$$\n\n q is the number of examples in each class.\n\nReplace the convolution block by:\n\n$$\\hat{Y}\\{\\ast}=\\sum\\{j} \\tilde{A}\\{\\ast, j}^{(0)}\\left\\langle\\mathbf{x}\\{j}^{(0)}, u\\right\\rangle$$\n\n Extentions\n\nPaper also proposed a way to extend the few-shot learning setting to semi-supervised learning and active learning setting. Although straight forward I do not see the intuition clear enough or experiments strong enough to suggest that those setting have any added benefit on reducing uncertainty of the model.\n\nCritique\n\nAlthough the paper is a fascinating read since it connect various approaches of few-shot / one-shot learning with graph-convolutional network. I find the experiments very weak. Especially since authors only use 1 test example per task. Lack of intuition for the attention mechanism for active learning setting is also discouraging.\n\n Questions\n\nHow to decide $l$? Link to Answer  \n   I believe, the label $l$ were randomly sampled from the dataset for each task.\nExplain how are parameters $\\tilde{\\theta}$ trained? Link to Answer\nHow to compute the adjacency matrix for each layer of GNN? Link to Answer  \n   Question 2 and 3 are equivalent, since $A$ is a function of $\\tilde{\\theta}$.\nHow to generate one hot encoding in Figure 1. Link to Answer\nWhat is $u$? How to generate $u$? Link to Answer\nWhat is the meaning of $\\ast$? Link to Answer\nExplain how prototypers are generated in prototypical networks? Explain how they are equivalent?\nHow to calculate probability $P(Y|\\mathcal{T})$? Link to Answer\nHow is attention vector generated?  \n   Attention vector is also trained.\n10. What is $Gc(.)$? Link to Answer\n\nReferences\n\nTutorial on Meta-Learning by Dr. Lilian Weng\nKoch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. ICML deep learning workshop. 2015.\nVinyals, Oriol, et al. Matching networks for one shot learning. NeurIPS. 2016.\nSnell, Jake, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. NeurIPS. 2017.",
        "tags": [
            "few-shot",
            "GNN",
            "course"
        ]
    },
    {
        "uri": "/posts/GAT",
        "title": "Graph Attention Networks",
        "content": "\nIntroduction\n\nConvolutional Neural Networks (CNNs) can effectively transform grid-like structures and have been used for various image segmentation/classification tasks. Various approaches have not been proposed to extended CNNs graph structures. These approaches are broadly divided into two categories:\n\nSpectral appraoch leverages the spectral representations of graph and defines convolution in the Fourier domain. However, because such convolutions require eigen-decomposition of graph laplacian, they can not be directly generalized to different graph structures. Most famous of this is Convolutional Graph Networks by Kipf and Welling, ICLR 2017. One major drawback of CNN is that is assigns equal importance to all the neighbors.\n\nSpatial approach on other hand perform convolution directly on the graph, leveraging the neighborhood structure. This can be challenging because of varying neighborhood and most approaches perform some form of aggregation on the neighbors. Since the neighbors in graph have no particular order, the aggregation provides equal importance to all the neighbors.\n\nBesides, attention mechanisms allow for focusing on relevant parts of input and have attained state-of-the-art for various tasks. This paper proposes the use of attention mechanism to provide a relevant weights to different neighbors.\n\n Attention layer\n\nGenerally, each graph convolutional layer take input $\\mathbf{h}=\\left\\\\{\\vec{h}\\{1}, \\vec{h}\\{2}, \\ldots, \\vec{h}\\{N}\\right\\\\}, \\vec{h}\\{i} \\in \\mathbb{R}^{F}$ and generates output $\\mathbf{h}^{\\prime}=\\left\\\\{\\vec{h}\\{1}^{\\prime}, \\vec{h}\\{2}^{\\prime}, \\ldots, \\vec{h}\\{N}^{\\prime}\\right\\\\}, \\vec{h}\\{i}^{\\prime} \\in \\mathbb{R}^{F^{\\prime}}$ by linearly transforming the input vectors using weight matrix -- $\\mathbf{W} \\in \\mathbb{R}^{F^{\\prime} \\times F}$, taking weighted aggregate over the neighborhood -- $\\mathcal{N}$ (including the node itself), and finally passing the value from a non-linear activation function -- $\\sigma$.\n\n$$\n\\vec{h}\\{i}^{\\prime}=\\sigma\\left(\\sum\\{j \\in \\mathcal{N}\\{i}} \\alpha\\{i j} \\mathbf{W} \\vec{h}\\_{i}\\right)\n$$\n\nThis can be equivalently written in matrix form as\n\n$$\n\\mathbf{h}^{\\prime} = \\sigma \\left( \\tilde{A} \\mathbf{\\alpha} \\, \\mathbf{h} \\mathbf{W} \\right) \\\\\\\\\n\\text{with}, \\tilde{A} = A + I_N  \\\\\\\\\n\\text{(adjanceny matrix with self loop)}\n$$\n\n In GCN, the convolutional layer is $\\vec{h}\\{i}^{\\prime}=\\sigma\\left(\\sum\\{j = 1 }^{N} \\hat{A\\{i j}} \\mathbf{W} \\vec{h}\\{i}\\right)$ where $\\hat{A}$ is a renormalized laplacian which indicated neighbor. So, effectively $\\alpha\\{i j} = \\frac{1}{\\sqrt{d\\{i}d\\_{j}}}$ because the\n\nThis paper proposes to use the attention mechanism -- $a$ to compute the $\\alpha\\_{i j}$ in following way:\n\n$$\n\\begin{align}\n\\alpha\\{i j} & =\\operatorname{softmax}\\{j}\\left(e\\_{i j}\\right) \\\\\\\\\n& =\\frac{\\exp \\left(e\\{i j}\\right)}{\\sum\\{k \\in \\mathcal{N}\\{i}} \\exp \\left(e\\{i k}\\right)} \\\\\\\\\n\\text{with}, \\, \\, e\\{i j} & =a\\left(\\mathbf{W} \\vec{h}\\{i}, \\mathbf{W} \\vec{h}\\_{j}\\right)\n\\end{align}\n$$\n\nIntuitively, $e\\{i j}$ is importance of node $j$ on $i$ and $\\alpha\\{i j}$ is normalized importance.\n\nThe framework proposed is agnostic to the choice of attention mechanism -- $a$. However, in the paper authors use a single-layer feed-forward neural network parameterized by weight vector $\\overrightarrow{\\mathrm{a}} \\in \\mathbb{R}^{2 F^{\\prime}}$ and LeakyReLU nonlinearity, which takes the input $\\mathbf{W} \\vec{h}\\{i} \\| \\mathbf{W} \\vec{h}\\{j}$, where $\\|$ represents concatenation. The activation mechanism is represented in the figure below. Mathematically, the activation mechanism used in the experiments is:\n\n$$\n\\alpha\\{i j}=\\frac{\\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}\\{i} \\| \\mathbf{W} \\vec{h}\\{j}\\right]\\right)\\right)}{\\sum\\{k \\in \\mathcal{N}\\{i}} \\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{\\mathbf{a}} T\\left[\\mathbf{W} \\vec{h}\\{i} \\| \\mathbf{W} \\vec{h}\\_{k}\\right]\\right)\\right)}\n$$\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"../../images/attention.png\"\npsrc: Veličković et al. 2018 /p\n/div\n\nIntuitively,  $\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}\\{i} \\| \\mathbf{W} \\vec{h}\\{j}\\right]$ is  a linear combination of transformed $hi$ and $hj$. It can be thought of as a distance between node $i$ and $j$ or some aggregate. a name=\"a1\"(Answers Q-1)/a\n\n Since the slope of ReLU is zero for -ve values, the ability to train the model is compromised in that region. This is called dying ReLU problem. LeakyReLU activation function is used instead of ReLU with a=0.01 in the figure below to avoid that problem. LeakyReLU helps speed up learning and is more balanced. a name=\"a2\"(Answers Q-2)/a\n div align=\"center\" img align=\"center\" width=\"500\"  src=\"../../images/leakyReLU.jpg\" psrc: a href=\"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\"https://towardsdatascience.com/activation-.../a/p /div\n\nFollowing Vaswani et al. 2017, authors proposes to use $K$ independent attention mechanism to employ multi-head attention. So the mathematical form of each layer is:\n\n$$\n\\vec{h}\\{i}^{\\prime}=\\Biggm\\|\\{k=1}^{K} \\sigma\\left(\\sum\\{j \\in \\mathcal{N}\\{i}} \\alpha\\{i j}^{k} \\mathbf{W}^{k} \\vec{h}\\{j}\\right)\n$$\n\nand the output $\\mathbf{h}^{\\prime}$ consists $K F^{\\prime}$ features instead of $F^{\\prime}$.\n\nOnly, in last layer the multi-head attentions are aggregated instead of concatenation. So, the mathematical form of last layer is:\n\n$$\n\\vec{h}\\{i}^{\\prime}=\\sigma\\left(\\frac{1}{K} \\sum\\{k=1}^{K} \\sum\\{j \\in \\mathcal{N}\\{i}} \\alpha\\{i j}^{k} \\mathbf{W}^{k} \\vec{h}\\{j}\\right)\n$$\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"../../images/gat.jpg\"\npsrc: Veličković et al. 2018 /p\n/div\n\nComplexity\n\nTime complexity of computing single attention head is sum of complexity of computing the $e\\{i j}$ for each node and then computing the softmax. The feed forward neural network, computing $e\\{i j} = \\text{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}\\{i} \\| \\mathbf{W} \\vec{h}\\{j}\\right]\\right)$ is effectively equivalent to complexity of matrix multiplication  $\\underbrace{\\mathbf{W}}\\{F^{\\prime} \\times F} \\underbrace{\\vec{h}\\{i}}\\{F \\times 1} + \\underbrace{\\overrightarrow{\\mathbf{a}}^{T}}\\{1 \\times 2F^{\\prime}} \\underbrace{\\hat{h}}\\{2F^{\\prime} \\times 1}$ for each node, $\\mathcal{O}\\left(|\\mathcal{V}|(FF^{\\prime} + 2F^{\\prime})\\right) = \\mathcal{O}\\left(|\\mathcal{V}|FF^{\\prime}\\right)$, where $\\mathcal{V}$ is number of nodes. Complexity of computing $\\alpha\\{i j} =\\operatorname{softmax}\\{j}\\left(e\\{i j}\\right)$ for all the edges is $\\mathcal{O}(|\\mathcal{E}|F^{\\prime})$, where $\\mathcal{E}$ is number of edges. So, total complexity of a single attention head is $\\mathcal{O}(|\\mathcal{V}|FF^{\\prime} + |\\mathcal{E}|F^{\\prime})$.  a name=\"a3\"(Answers Q-3)/a\n\nMemory complexity of GAT for sparse matrix is linear in terms of nodes and edges. a name=\"a4\"(Answers Q-4)/a\n\n Experiments\n\nAuthors evaluate the GAT architecture for two type of learning tasks on graph structures:a name=\"a5\"(Answers Q-5)/a\n\nTransductive learning task is where the algorithm sees whole graph but labels of only few nodes are available. Algorithm is trained on these nodes and whole graph and the task is to produce labels for nodes which do not have labels while training. GCN works only for transductive setting.\n\nInductive learning task is when the algorithm sees only training nodes and edges between those nodes. Labels of all the training nodes are available and the task is to predict label for test nodes, which are unseen during training.\n\nTransductive learning\n\nFor transductive learning task, 10 baseline architectures are compared (including GCN, MoNet, Chebyshev, MLP etc), for three datasets Cora, Citeseer and pubmed. Two layer GAT architecture was used as shown in figure below, with 8 attention heads in first layer and 1 attention head in second layer. Softmax was used in final layer and ELU activation is used in the first layer.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"../../images/Transductive.png\"\n/div\n\nMathematical form of the architecture:\n\n$$\nZ = \\operatorname{softmax}\\left( \\tilde{A} \\mathbf{\\alpha}\\{2}\\mathbf{W}2\\left({\\biggm\\|}\\{k=1}^{8} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}\\{1}^{k}\\mathbf{W}_1 H \\right)\\right)  \\right)\n$$\n\na name=\"a6\"(Answers Q-6)/a\n\n Inductive Learning\n\nFor inductive learning task, 6 baseline architectures are compared (including GraphSAGE, MLP etc), for PPI dataset. Three layer architecture was used as shown in the figure below.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"../../images/inductive.png\"\n/div\n\nMathematical form of the architecture:\n\n$$\nZ = \\operatorname{LogisticSigmoid}\\left( \\frac{1}{6} \\sum\\{z=1}^{6} \\tilde{A} \\mathbf{\\alpha}\\{3}^{z}\\mathbf{W}3\\left({\\biggm\\|}\\{y=1}^{4} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}\\{2}^{y}\\mathbf{W}2 \\left({\\biggm\\|}\\{k=1}^{4} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}\\{1}^{k}\\mathbf{W}_1 H \\right) \\right)\\right)\\right)  \\right)\n$$\n\na name=\"a7\"(Answers Q-7)/a\n\nExtensions\n\nThe GAT network can be extended to use for Graph classification by simply appending a pooling layer at the end. Figure below represents one such architecture.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"../../images/GraphClassification.png\"\n/div\n\na name=\"a9\"(Answers Q-9)/a\n\nGAT can also be used for embedding the nodes to two-dimensional space. One such architecture is presented below, where instead of Softmax, the last layer outputs 2-D vector for each node.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"../../images/2D.png\"\n/div\n\nMathematical form of this 2D embedding GAT network is\n\n$$\nZ = \\operatorname{ELU}\\left( \\tilde{A} \\mathbf{\\alpha}\\{2}\\mathbf{W}2\\left({\\biggm\\|}\\{k=1}^{8} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}\\{1}^{k}\\mathbf{W}_1 H \\right)\\right)  \\right)\n$$\n\na name=\"a8\"(Answers Q-8)/a\n\n Questions\n\nWhat attention mechanism is used in the experiments?  Link to Answer\n\nWhy LeakyReLU but not the standard ReLU ?  Link to Answer\n\nProof complexity of GAT layer is $O\\left(|V| F F^{\\prime}+|E| F^{\\prime}\\right)$. Link to Answer\n\nWhat is the memory complexity of GAT layer?  Link to Answer\n\nExplain difference between transductive and inductive learning.  Link to Answer\n\nDraw architecture of two layer GAT model for transductive learning. What is the Mathematical formulation? Link to Answer\n\nDraw architecture of three layer GAT model for inductive learning. What is the Mathematical formulation?  Link to Answer\n\nDesign a GAT model that embed the nodes of the cora network in a two-dimensional space. Draw the architecture and give the mathematic form. Link to Answer\n\nDesign a GAT model for graph classification. Draw the architecture and\ngive the mathematic form.  Link to Answer\n\nReferences\n\nVeličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio, *Graph Attention Networks *, ICLR 2018\nGraph Attention Networks overview by Peter Veličković\nML Paper explained - AISC by Karim Khayrat: Graph Attention Networks Explained\nA Tutorial on Attention in Deep Learning, by Alex Smola and Aston Zhang, ICML 2019\nhttps://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\nhttps://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6",
        "tags": [
            "GNN",
            "course"
        ]
    },
    {
        "uri": "/posts/GCN",
        "title": "Graph Convolutional Networks",
        "content": "\nIntroduction\n\nKipf et al. 2017 introduces Graph Convolutional Networks (GCN) which uses features of each node and leverages edges of the graph to derive class similarity between nodes in semi-supervised setting.\n\nTraditionally semi-supervised learning in a graph-structured data heavily relied on the assumption that the edges in the graph represent class similarities (i.e. nodes with similar classes have edge between them). For example, in an image segmentation task, an image can be thought as a grid (Graph with node for every pixel and edges between neighboring pixels as shown in figure below). Such representation of an image represents the assumption that the neighboring pixels belong to the same class (hence an edge between them).\n\ndiv align=\"center\"\nimg align=\"center\" width=\"200\"  src=\"/images/grid.png\"\npsrc: a href=\"https://medium.com/@BorisAKnyazev/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-1-3d9fada3b80d\"https://medium.com/@BorisAKnyazev/.../a/p\n/div\n\nWith this assumption, semi-supervised learning task is then posed as an optimization problem with following loss function:\n\n\\\\[\n\\mathcal{L}=\\mathcal{L}\\{0}+\\lambda \\mathcal{L}\\{\\mathrm{reg}}, \\quad \\text { with } \\quad \\mathcal{L}\\{\\mathrm{reg}}=\\sum\\{i, j} A\\{i j}\\left\\|f\\left(X\\{i}\\right)-f\\left(X\\_{j}\\right)\\right\\|^{2}\n\\\\]\n\nHere $\\mathcal{L}\\{0}$ is some form of cross-entropy loss for the supervised examples and the $\\mathcal{L}\\{\\mathrm{reg}}$ is a regularizing function which tries to reduce difference in labels of connected nodes.  $\\mathcal{L}_{\\mathrm{reg}}$ is laplacian quadratic form.\n\n\\\\[\n\\begin{aligned}\nf\\left(X\\right)^{\\top} \\Delta  f\\left(X\\right) &= f\\left(X\\right)^{\\top}(D-A) f\\left(X\\right) \\\\\\\\\n&=f\\left(X\\right)^{\\top} D f\\left(X\\right)-f\\left(X\\right)^{\\top} A f\\left(X\\right) \\\\\\\\\n&=\\sum{i=1}^{n} D{ii} f\\left(X{i}\\right)^{2}-\\sum{i=1}^{n} \\sum{j=1}^{n} A{ij} f\\left(X{i}\\right)f\\left(X{j}\\right) \\\\\\\\\n&=\\sum{i=1}^{n} \\sum{j=1}^{n} A{ij} f\\left(X{i}\\right)^{2}-\\sum{i=1}^{n} \\sum{j=1}^{n} A{ij} f\\left(X{i}\\right) f\\left(X_{j}\\right) \\\\\\\\\n&=\\sum{i=1}^{n} \\sum{j=1}^{n} A{ij}\\left(f\\left(X{i}\\right)^{2}-f\\left(X{i}\\right) f\\left(X{j}\\right)\\right) \\\\\\\\\n&=\\sum{\\{i, j\\} \\in E}\\left(f\\left(X{i}\\right)-f\\left(X_{j}\\right)\\right)^{2} \\\\\\\\\n&= \\sum{i \\leq j} A{i j}\\left\\|f\\left(X{i}\\right)-f\\left(X{j}\\right)\\right\\|^{2}\n\\end{aligned}\n\\\\]\n\n Graph Convolutional networks\n\nGCN proposes a way to do graph convolutions by using the following layer wise propagation rule:\n\n\\\\[\n\\begin{align}\nH^{(l+1)} & =\\sigma\\left( \\hat{A}  H^{(l)} W^{(l)}\\right), \\\\\\\\\n\\quad \\text { with } \\quad\\hat{A} & = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}, \\\\\\\\\n\\tilde{A} & = (A + I\\_{N}), \\\\\\\\\n\\end{align}\n\\\\]\n\n\\\\[\n\\tilde{D\\{ij}}=\\left\\\\{\\begin{array}{ll}{\\sum\\{k=1}\\tilde{A}\\_{ik},} & {\\text { if } i = j} \\\\\\\\ {\n0,} & {\\text { otherwise }}\\end{array}\\right.\n\\\\]\n\nThe paper proposes a two layer Graph Convolutional Network which amounts to following model:\n\n\\\\[\nZ=f(X, A)=\\operatorname{softmax}\\left(\\hat{A} \\operatorname{ReLU}\\left(\\hat{A} X W^{(0)}\\right) W^{(1)}\\right)\n\\\\]\n\nReLU activation function is used in the first layer because the gradient of ReLU is $0/1$, so with multiple iterations, the gradient values do not vanish (tends to zero), which is the case with other non-linear functions. Softmax is used in the top layer, because the final output expected are class probabilities.\n\nGCN uses gradient descent to learn weight matrices -- $W^{(0)}$ and $W^{(1)}$ that minimizes the following cross-entropy error for all the supervised nodes ($\\mathcal{Y}_{L}$):\n\n\\\\[\n\\mathcal{L}=-\\sum{l \\in \\mathcal{Y}{L}} \\sum{f=1}^{F} Y{l f} \\ln Z_{l f}\n\\\\]\n\nLoss function is then a combination of cross-entropy loss for the supervised labels and some regularization.\n\n\\\\[\n{Loss}=\\mathcal{L} + \\mathcal{L}\\_{reg}, \\\\\\\\\n\\begin{align}\n\\text{with } \\mathcal{L}\\_{reg} = &  \\frac{\\lambda}{2} * \\sum\\|W\\|^{2} \\quad \\text{ for L2-regularization} \\\\\\\\\n& \\frac{\\lambda}{2} * \\sum\\|W\\| \\quad \\text{ for L1-regularization}\n\\end{align}\n\\\\]\n\nIn the paper, authors observe that L2-regularization of weight matrix at the first layers alone is sufficient.\n\nRegularization is used to avoid overfitting by penalizing the weight matrices of hidden layers. L2-regularization in particular uses 2-norm of weight matrix for penalty. This pushes the weight matrix close to zero. a name=\"a14\"(Answers Q-14)/a\n\nExperiments\n\nConnection between CNN and GCN\n\nIn CNN, input feature map (blue grid in below image) is convolved with discrete kernel ($W$) to produce output feature map (green grid). This can be seen as a message passing algorithm where the messages from the neighboring nodes ($hi, i = 1 \\text{ to } 8$ in the example below) and the node itself ($h0$) are multiplied with weight $W_i$ and the output feature map is obtained by summing up these messages.\n\ndiv align=\"center\" \n    div style=\"float:left;\"\n        img src=\"/images/CNN.gif\"  width=\"320\"  /\n        p style=\"text-align:center;\"src: a href=\"https://github.com/vdumoulin/convarithmetic\"https://github.com/vdumoulin/convarithmetic/a/p\n    /div\n    div style=\"margin-right:5px;\"\n        img class=\"middle-img\" src=\"/images/CNN-mp.png\" width=\"200\" /\n        p style=\"text-align:center;\"/p\n    /div\n/div  \n\nGCN similarly generates output feature map ($H^{(l+1)}$) by convolving the input feature map ($H^{(l)}$) with weight matrix ($W^{(l)}$).\n\n\\\\[\n\\begin{align}\nH^{(l+1)} & =\\sigma\\left( \\hat{A}  H^{(l)} W^{(l)}\\right), \\\\\\\\\n\\text{ with } \\quad \\hat{A} & :  n \\times n \\text{( nodes)}, \\\\\\\\\nH^{(l)} & : n \\times f^{i} \\text{(# input feature map)}, \\\\\\\\\nW^{(l)} & : f^{i} \\times f^{o} \\text{(# output feature map)}, \\\\\\\\\n\\end{align}\n\\\\]\n\n This can be seen as a message passing algorithm where messages or input features ($H{ik}$) of each node in the graph are multiplied by weights ($W{jk}$) and summation is stored in $B$ and finally, the output feature map ($M$) is generated by summing the weighted messages of all the neighboring nodes along with the message from the node itself (Note: $\\tilde{A} = A + I_N$ takes care of message from node itself).\n\n\\\\[\n\\begin{align}\nM & = \\hat{A}  H^{(l)} W^{(l)} \\\\\\\\\n& = \\hat{A} B, \\\\\\\\\n\\text{ with } B{ij} & = \\sum{k=1}^{f^{i}} H{ik}W{jk} \\\\\\\\\nM{ij} & = \\sum{k=1}^{n} \\hat{A}{ik} B{kj}\n\\end{align}\n\\\\]\n\nIn CNN the number of neighbors for each node are fixed ($8$ in above example), so the number of messages received by all the nodes in the output later are same. In GCN, since the structure of graph is dictated by the adjacency matrix, the number of messages received at each node ($M_{ij}$) is not the same. Hence, the need for normalizing $\\tilde{A}$ (i.e, $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$) arises in GCN but not in CNN.\n\nAlthough theoretically there are no limitations on number of convolution layers, GCN paper proposes two layered network. Since every layer convolves the neighboring node, $k$ layers effectively convolves $k^{th}$ order neighbors. GCN convolve only upto $2^{nd}$ order neighbors. Their empirical evaluations suggest 2nd order neighbor is enough for most of the domains. a name=\"a13\"(Answers Q-13)/a\n\nIn traditional approach, since we use $\\mathcal{L}_{reg}$, which is a function of adjacency matrix, the nodes with dense neighborhood will have high penalty and hence the model will overfit on the local neighborhoods of such nodes (consequently it will under-fit the nodes with sparse neighborhood). Normalization of adjacency matrix helps us alleviate this problem.\n\nExtending GCN to graph classification and supervised learning task\n\na name=\"a11\"Answers Q-11/a\n\nGCN formulation can be leveraged for graph classification problem, where the task is to predict a single variable for the whole graph. Adjacency matrix of each graph ($Q, V, W$ in the figure below) is concatenated into a sparse block-diagonal matrix ($A$) as shown in the figure below. This adjacency matrix and the feature matrix ($X: n \\times c$, n = total number of nodes in all the graphs and c =  input features) can be used to train GCN and the output matrix Z can then be pooled to obtain class labels for each graph.\n\ndiv align=\"center\" \nimg src=\"/images/GCN-extention.png\"  width=\"500\"  /\npsrc: a href=\"https://github.com/tkipf/gcn\"https://github.com/tkipf/gcn/a/p\n/div\n\nFor supervised learning task, two graphs can be created from training set (labeled nodes) and the testing set (unlabeled nodes). Adjacency matrix of two graphs can again be concatenated as block-diagonal matrix and GCN can be trained on it. The output matrix Z will have the class probabilities for test set as well.\n\nSpectral Graph Convolutions\n\nConvolution theorem states that convolution of two matrices is equivalent to point-wise multiplication in the fourier domain i.e. $\\mathcal{F}(x \\ast y) =  \\mathcal{F}(x) \\odot \\mathcal{F}(y)$ when $\\mathcal{F}(\\ \\ )$ denotes fourier transform operator. In signal processing, the spectral transformation usually refers to transformation from time to frequency dimension (fourier domain), but in graph theory spectral transformation usually refers transformation to eigen-vector dimension ($U^{\\top}$). So, the convolution theorem in graphs represents following equation:\n\n\\\\[\n\\begin{align}\n\\text {with } L & = U \\Lambda U^{\\top}, \\\\\\\\  \n\\hat{\\mathrm{x}} & = U^{\\top}\\mathrm{x}, \\\\\\\\   \n\\hat{\\mathrm{y}} & = U^{\\top}\\mathrm{y}, \\\\\\\\  \nU^{\\top} (\\mathrm{x} \\ast \\mathrm{y}) & =  \\hat{\\mathrm{x}} \\odot \\hat{\\mathrm{y}} \\\\\\\\   \n \\mathrm{x} \\ast \\mathrm{y} = U ( \\hat{\\mathrm{x}}  & \\odot \\hat{\\mathrm{y}}) = U ({ \\mathrm{diag}(\\hat{\\mathrm{y}})\\hat{x}} ) \\\\\\\\  \n\\mathrm{y} \\ast \\mathrm{x}  = \\mathrm{x} \\ast \\mathrm{y} = & U \\ \\mathrm{diag}(\\hat{\\mathrm{y}})\\ U^{\\top} \\ \\mathrm{x}\n\\end{align}\n\\\\]\n\n - $\\text {with } L = U \\Lambda U^{\\top}$ is eigen-decomposition of graph laplacian matrix. Complexity of finding eigen-decomposition if $\\mathcal{O}(n^3)$  a name=\"a2\"(Answers Q-2)/a  \n - Remember, matrix-vector multiplication ($A \\mathrm{b}$) is effectively transformation of the vector ($b$) to another dimension dictated by the matrix ($A$).  \n - Since, L is a square matrix, $U^T = U^{-1}$.\n\nFor spectral graph convolution, we directly estimate the convolution filter in the eigen-vector dimension as some function of eigen-vectors ($\\Lambda$). With filter $g_{\\theta}(\\Lambda) = \\mathrm{diag}(\\theta(\\Lambda))$ spectral convolution on graph is defined as follows:\n\n\\\\[\ng{\\theta}(\\Lambda) \\ \\star \\  \\mathrm{x} = U g{\\theta}(\\Lambda) U^{\\top} \\mathrm{x}\n\\\\]\n\n Note the difference between convolution symbol ($\\ast$) and the spectral convolution symbol used in paper ($\\star$) signifies that the $g_{\\theta}$ is already in eigen-vector dimension.\n\n$g{\\theta}$ can be approximated by truncated expansion in terms of Chebyshev polynomials $T{k}(x)$ as:\n\n\\\\[\n\\begin{align}\ng{\\theta^{\\prime}}(\\Lambda) \\approx \\sum{k=0}^{K} \\theta{k}^{\\prime} T{k}(\\tilde{\\Lambda}) \\\\\\\\\n\\text{ with } \\quad \\tilde{\\Lambda}=\\frac{2}{\\lambda{\\max }} \\Lambda-I{N}, \\\\\\\\\nT{k}(x) = 2 x T{k-1}(x)-T_{k-2}(x), \\\\\\\\\nT_{0}(x)=1\n\\text { and } \\quad T_{1}(x) =x \\\\\\\\\n\\end{align}\n\\\\]\n\nChebyshev polynomial $T{k}(\\tilde{\\Lambda})$ is a matrix with dimensions same as $\\tilde{\\Lambda}$ (i.e. $n \\times n$, where n =  nodes ). Elements of matrix $T{k}(\\tilde{\\Lambda})$ are obtained by applying Chebyshev polynomial definition element wise. a name=\"a1\"(Answers Q-1)/a\n\nSo,\n\n\\\\[\n\\begin{align}\nT0(\\tilde{\\Lambda}) & = \\left[\\begin{array}{cccc}{T{0}\\left(\\frac{2}{\\lambda{\\max }}\\left(\\lambda{1}-1\\right)\\right)} & {} & {} & {} \\\\ {} & {T{0}\\left(\\frac{2}{\\lambda{\\max }}\\left(\\lambda{2}-1\\right)\\right)} & {} & {} \\\\ {} & {} & {\\ddots} & {} \\\\ {} & {} & {} & {T{0}\\left(\\frac{2}{T{\\max }}\\left(\\lambda{N}\\right)\\right)}\\end{array}\\right] \\\\\\\\\n& = \\left[ \\begin{array}{cccc}{1} & {} & {} & {} \\\\\\\\\n  {} & {1} & {} & {} \\\\\\\\\n    {} & {} & {\\ddots} & {} \\\\ {} & {} & {} & {1}\\end{array} \\right] \\\\\\\\\n& = I_N\n\\\\\\\\\n\\\\\\\\\n& \\therefore\\boxed{ \\quad T{0}(\\tilde{\\Lambda})= IN \\text{ and } T_{1}(\\tilde{\\Lambda}) =\\tilde{\\Lambda} }\n\\end{align}\n\\\\]\n\nObserve that $\\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top}$ and $U T{k}(\\tilde{\\Lambda}) U^{\\top} = T{k}(\\tilde{L})$.\n\nProofs a name=\"a4\"(Answers Q-4)/a:\n\\\\[\n\\begin{align}\n(U \\Lambda U^{\\top})^{2} & = ( U \\Lambda U^{\\top} )( U \\Lambda U^{\\top} ) \\\\\\\\\n& =  U \\Lambda ( U^{\\top}  U ) \\Lambda U^{\\top} \\\\\\\\\n& =  U \\Lambda I \\Lambda U^{\\top} \\\\\\\\\n& =  U \\Lambda ^{2} U^{\\top} \\\\\\\\\n\\end{align}\n\\\\]\n\\\\[ \n\\text{ Similarly, } \\boxed{ \\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top} }\n\\\\]\n\nand\n\n\\\\[\n\\begin{align}\n{U T{0}(\\tilde{\\Lambda}) U^{\\top}=U I{N} U^{\\top}=I{N}=T{0}(\\tilde{L})}, \\\\\\\\\n{U T{1}(\\tilde{\\Lambda}) U^{\\top}=U \\tilde{\\Lambda} U=\\tilde{L}=T{1}(\\tilde{L})}, \\\\\\\\\n\\end{align}\n\\\\]\n\\\\[\n\\begin{align}\nU T{2}(\\tilde{\\Lambda}) U^{\\top} & = U \\Big(2 \\tilde{\\Lambda} T{1}(\\tilde{\\Lambda})- T_{0}(\\tilde{\\Lambda}) \\Big) U^{\\top} \\\\\\\\\n& = 2 U \\tilde{\\Lambda} T{1}(\\tilde{\\Lambda}) U^{\\top}- U T{0}(\\tilde{\\Lambda}) U^{\\top} \\\\\\\\\n& = 2 U \\tilde{\\Lambda} U^{\\top} U T{1}(\\tilde{\\Lambda}) U^{\\top}- U T{0}(\\tilde{\\Lambda}) U^{\\top} \\\\\\\\\n& = 2 \\tilde{L} T{1}(\\tilde{L}) - T{0}(\\tilde{L})\\\\\\\\\n& = T_2(\\tilde{L}) \\\\\\\\\n\\end{align}\n\\\\]\n\\\\[\n\\text{ Similarly, } \\boxed { U T{k}(\\tilde{\\Lambda}) U^{\\top} = T{k}(\\tilde{L}) } \\\\\\\\\n\\\\]\n\nUsing the above two results, we can approximate the convolution without needing eigen-decomposition, directly using graph laplacian,\n\n\\\\[\n\\begin{align}\ng{\\theta^{\\prime}}(\\Lambda) \\star x  & \\approx U \\sum{k=0}^{K} \\theta{k}^{\\prime} T{k}(\\tilde{\\Lambda}) U^{\\top} \\mathrm{x} \\\\\\\\\n& = \\sum{k=0}^{K} \\theta{k}^{\\prime} U T_{k}(\\tilde{\\Lambda}) U^{\\top} \\mathrm{x} \\\\\\\\\n& = \\sum{k=0}^{K} \\theta{k}^{\\prime} T_{k}(\\tilde{L}) \\mathrm{x}\n\\end{align}\n\\\\]\n\nSo, we effectively reduced the complexity of convolution from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(|\\mathcal{E}|)$, i.e. from cube of # nodes ($n$)  to  linear in # edges ($\\mathcal{E}$).\nSince $Ti(\\tilde{L})$ has $2\\mathcal{E}$ non-zero elements, multiplication $\\underbrace{\\theta\\k^{\\prime}}\\{scalar} \\underbrace{T\\i(\\tilde{L})}\\{n \\times n} \\underbrace{\\mathrm{x}}\\{n \\times 1}$ is a sparse matrix multiplication which can be done in $\\mathcal{O}(|\\mathcal{E}|)$. a name=\"a5\"(Answers Q-5)a:\n\nConnection between GCN and spectral convolutions\n\na name=\"a3\"Answers Q-3/a\n\nGCN can be seen as approximation of spectral convolution with $K = 1$, $\\lambda{max}=2$ and $\\theta = \\theta0^{\\prime} = - \\theta_1^{\\prime}$ (a name=\"a8\"Answers Q-8/a)\n\n\\\\[\n\\begin{align}\ng{\\theta^{\\prime}}(\\Lambda) \\star x  & \\approx  \\sum{k=0}^{1} \\theta{k}^{\\prime} T{k}(\\tilde{L}) x \\\\\\\\\n& = \\theta{0}^{\\prime} T{0}(\\tilde{L}) x+\\theta{1}^{\\prime} T{1}(\\tilde{L}) x\\\\\\\\\n& = \\theta{0}^{\\prime} I{N} x+\\theta{1}^{\\prime}\\left(\\frac{2}{\\lambda{\\max }} L-I_{N}\\right) \\\\\\\\\n & =\\theta{0}^{\\prime} x+\\theta{1}^{\\prime}\\left(L-I{N}\\right) \\quad   \\boxed{\\because \\lambda{max}=2}  \\\\\\\\\n & =\\theta{0}^{\\prime} x-\\theta{1}^{\\prime} D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\quad   \\boxed{\\because L=I_{N}-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}} \\\\\\\\\n & = \\theta \\left(I{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) x  \\quad \\boxed{\\because \\theta = \\theta0^{\\prime} = - \\theta_1^{\\prime}} \\\\\\\\\n & =  \\theta \\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}\\right) x \\quad \\boxed{ \\because \\underbrace{I{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\rightarrow \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}}{\\text{renormalization trick }}, \\text { with } \\tilde{A}=A+I_{N} } \\\\\\\\\n & =  \\theta \\hat{A} x\n\\end{align}\n\\\\]\n\n If we use convolution formula $g{\\theta} \\star x \\approx \\theta\\left(I{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) x$ repeatedly over multiple layers, values keep increasing in every layer since we have $I_N$. Renormalization trick is used to avoid this.  a name=\"a9\"(Answers Q-9 and 10)/a\n\n Since GCN uses $K =1$ and approximates $\\theta = \\theta0^{\\prime} = - \\theta1^{\\prime}$, which reduces the number of parameters we intuitively determine that GCN will not overfit the graphs. a name=\"a6\"(Answers Q-6)/a\n\n When we use $\\theta = \\theta0^{\\prime} = - \\theta1^{\\prime}$ we are assigning same weights to all the 1st and 2nd order neighbors. If we used different $\\theta$, we will assign different weights to different hop neighbors.\n\n Renormalization Trick: With $I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, the eigen values are in range [0,2]. When the largest eigenvalue is less than $1$\nthe vanishing problem may occur even in case of two layers. With the renormalization trick, we can ensure that the eigen values are between [0,1] and the maximum eigenvalue is $1$. So, we avoid vanishing problem.\n\nSo,\n\\\\[\n\\begin{align}\ng{\\theta^{\\prime}}(\\Lambda) \\star x   & = diag\\(\\theta\\)  T{k}(\\tilde{L}) X \\\\\\\\\n& = T_{k}(\\tilde{L}) \\  X \\ diag\\(\\theta\\)\n\\end{align}\n\\\\]\n\nSince this will still give us $n \\times f^{i}$ matrix, we add a weight matrix ($W^{\\prime}_{f^{i} \\times f^{o}}$) to linearly transform the result to $n \\times f^{o}$\n\nSo,\n\\\\[\n\\begin{align}\ng_{\\theta^{\\prime}}(\\Lambda) \\star x\n& = T_{k}(\\tilde{L}) \\ X \\ diag\\(\\theta\\) \\ W^{\\prime} \\\\\\\\ \n& = \\hat{A}  X   W\n\\end{align}\n\\\\]\n\nWith $X \\in \\mathbb{R}^{n \\times f^{i}}$, we have $\\underbrace{Z}{n \\times f^{o}} = \\underbrace{\\hat{A}}\\{n \\times n}  \\underbrace{X}\\{n \\times f^{i}}  \\underbrace{W}\\{f^{i} \\times f^{o}}$ which is graph convolution.\n Complexity of matrix multiplication is $\\mathcal{O}(n f^{i}f^{o})$ since $\\hat{A}$ is a sparse matrix with $\\mathcal{E}$ non-zero elements -- Not sure how paper gets $\\mathcal{O}(|\\mathcal{E}| f^{i}f^{o})$ a name=\"a12\"(Answers Q-12)/a  \n\n!-- Need to explain theta to W --\n\n Limitations\n\nGCN provides equal importance to self node as well as the neighboring introduces. It also does not allow to different weights for different neighbors, which is allowed in CCN.\n\nGCN being a graph convolution in the spectral domain, the Graph structure if fixed. Spatial domain convolutions on other hand allow for graph modifications.\n\nAlthough GCN considers node features, it still heavily rely on the node locality.\n\nQuestions\n\nWhat is the dimensionality of $T{k}(\\tilde{\\Lambda})$, how to calculate $i^{th}$ row and $j^{th}$ column of matrix $T{k}(\\tilde{\\Lambda})$?  Link to Answer\n\nWhat is the time complexity to compute the eigen-decomposition Link to Answer\n\nWhat is the relation between $g \\star x$ and GCN? Link to Answer\n\nProve $\\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top}$.  Link to Answer\n\nProve complexity of $\\sum{k=0}^{K} \\theta{k}^{\\prime} T_{k}(\\tilde{L}) x$ is $\\mathcal{O}(|\\mathcal{E}|)$. Link to Answer\n\nDescribe the intuition, why GCN can alleviate the problem of overfitting on\nlocal neighborhood structures for graphs with very wide node degree distributions? Link to Answer\n\nWhy is $\\lambda_{max} = 2$ a valid approximation?\n\n Probably because eigen values of A $\\in$ [0,1] and $\\lambda{max}$ is the eigen value of $A + IN$, so the eigen values are in range [0,2]. We need this rescaloing because the Chebyshev polynomial needs input in range [-1,1]\n\nProve $g{\\theta^{\\prime}} \\star x \\approx \\theta{0}^{\\prime} x+\\theta{1}^{\\prime}\\left(L-I{N}\\right) x$.  Link to Answer\n\nWhy does repeated application of  $g{\\theta} \\star x \\approx \\theta\\left(I{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) {x}$ result in numerical instability? Link to Answer\n\n10. Why does renormalization trick help? Link to Answer\n\n11. Show in detail how to apply GCN for graph classification and supervised learning problem?\nLink to Answer\n\n12. Derive time and memory complexity of $Z=\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X \\Theta$ Link to Answer\n\n13. Why use 2 layers in GCN? Link to Answer\n\n14. Explain L-2 regularization. Link to Answer\n\n References\nThomas N Kipf and Max Welling, Semi-supervised classification with Graph Convolutional Networks, ICLR 2017\nNotes by Prof. Feng Chen\nhttps://tkipf.github.io/graph-convolutional-networks/\nFerenc Huszár's GCN post\nhttps://dtsbourg.me/thoughts/posts/semi-supervised-classification-gcn\nhttps://medium.com/@BorisAKnyazev",
        "tags": [
            "GNN",
            "course"
        ]
    },
    {
        "uri": "/posts/HER",
        "title": "Hindsight Experience Replay",
        "content": "\nbr\n\nRemember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.\n\nIn standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states. Popular solution for this problem is to use reward shaping functions but even they have some unforeseen consequences.\n\nThis paper highlights that even though the current trajectory $Ti = s0, a0, s1, a1, ... s{ti}, a{ti}$ did not reach the achieved goal state $gi$, it reached the state $s{ti}$ and hence if the goal state was $s{ti}$ this would have been a useful trajectory. With the advent of goal-conditioned policy learning, policies $\\pi$ are no longer learnt for a single goal, rather goal state $g$ is taken as input along with state and action. i.e instead of $\\pi:S \\times A \\rightarrow [0,1]$, goal-conditioned policies are $\\pi:S \\times A \\times S_G \\rightarrow [0,1]$. So, we can engineer different goal for trajectories which do not reach their pre-determined goal and add it to the replay buffer with engineered goal state as well as the pre-determined goal. This increases the buffer size, providing more transition samples to learn.\n\nCritique\n\nAlthough the idea of the using the existing sampled trajectory by engineering the goal seems useful, it is not clear if there exists a principled approach to do this and how is it better than engineering reward-shaping functions?\n\n References\n\nhttps://openai.com/blog/ingredients-for-robotics-research/",
        "tags": [
            "RL"
        ]
    },
    {
        "uri": "/posts/hierarchical-RL",
        "title": "Hierarchical Reinforcement Learning",
        "content": "\n\nStandard RL planning suffers from the curse of dimensionality when the action space is too large and/or state space is infeasible to enumerate. Humans simplify the problem of planning in such complex conditions by abstracting away details which are not relevant at a given time and decomposing actions into hierarchies. Several researchers have proposed to model the temporal-abstraction in RL by composing some form of hierarchy over actions space (Dietterich 1998, Sutton et al 1998, Parr and Russell 1998). By modeling actions as hierarchies, researchers extended the primitive action space by adding abstract actions. Options framework (Sutton et al 1998), refer the abstract actions as options, MAXQ (Dietterich 1998) refer to them as tasks and Hierarchical Abstract Machines (HAM) (Parr and Russell 1998) refers to them as choices.\n\nCommon theme among these papers is to extend the Markov Decision Process (MDP) to Semi-Markov Decision Process (SMDP), where actions can take multiple time steps. As compared to MDP, which only allow actions of a discrete time-steps, SMDP allows modeling temporally abstract actions of varying length over a continuous time. As represented in first two trajectories of figure below. By constraining/extending the action space of the MDP over primitive and abstract actions, hierarchical RL approaches superimpose MDPs and SMDPs as shown in last trajectory.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/HRL-SMDP.png\"\n\npSemi-Markov Decision Process/p\n/div\n\n<!-- % Here, we mainly focus on the three hierarchical RL papers:\n% \\begin{enumerate}\n%     \\item Options framework {Sutton et al 1998}\n%     \\item MAXQ {Dietterich 1998}\n%     \\item Hierarchical Abstract Machines (HAM) {Parr and Russell 1998}\n% \\end{enumerate} --\n\nHRL is appealing because the abstraction of actions facilitate accelerated-learning and generalization while exploiting the structure of the domain.\n\nFaster learning is possible because of the compact-representation. Original MDP is broken into sub-MDP with less states (abstracted states hide irrelevant details and hence reduce the number of states) and less actions. For example, in the Taxi Domain introduced in (Dietterich 1998), if the agent is learning to navigate to a location it does not matter if the passenger is being picked or dropped. Details about location of passenger are irrelevant and hence the state space is reduced.\n\nBetter generalization is possible because of the abstracted actions. In the taxi domain, because we define an abstract action called $Navigation$, agent learns a policy to navigate the taxi to a location. Once that policy is learned for navigation to pick up a passenger, the same policy can be leveraged when then agent is navigating to drop the passenger.\n\nTwo important promises of HRL are prior-knowledge and transfer-learning. A complex task in HRL is decomposed into hierarchy (usually by humans). Hence, it is easier for humans to provide some prior on actions from their domain knowledge. Different levels of hierarchy encompass different knowledge and hence ideally it would be easier to transfer that knowledge across different problems.\n\nOne minor limitation of HRL is that all the hierarchical methods converge to hierarchically optimal policy, which can be a sub-optimal policy. For example in the taxi domain, if the hierarchy decomposition states first navigate to the passenger location and then navigate to the fuel location, the HRL agent will find an optimal policy to do that in exactly that order. This policy might be sub-optimal given an initial state which is closer to the fuel location. This limitation is an artifact of restricting the action space while solving sub-MDPs. If full action space is available in all the MDPs, the exponential increase in computational overhead makes the learning infeasible.\n\nMax-Q framework has a clear hierarchical decomposition of tasks, while the options-framework do not have clear hierarchy. Options framework achieves temporal abstraction of actions, Max-Q framework additionally also achieves state abstractions. While there has been an attempt on discovering and transferring the Max-Q hierarchies (Mehta et al. 2008), learning Max-Q hierarchies directly from the trajectories is still an open problem. For large and complex problem it might be a challenge to provide the task hierarchy or options and their termination conditions.\n\nReferences\n\na name=\"Dietterich-1998\"[Dietterich 1998]/a  Dietterich, T. G.  1998.  The maxq methodfor hierarchical reinforcement learning. In ICML.\na name=\"sutton-1998\"[Sutton, Precup, and Singh 1998]/a  Sutton, R. S.; Precup, D.;and Singh, S. P.  1998.  Intra-option learning about tempo-rally abstract actions. In ICML.\na name=\"parr-1998\"[Parr and Russell 1998]/a   Parr,  R.,  and  Russell,  S.  J. 1998. Reinforcement  learning  with  hierarchies  of  machines.   In NeurIPS\na name=\"mehta-2008\"[Mehta et al. 2008]/a  Mehta, N.; Ray, S.; Tadepalli, P.; and Di-etterich, T. 2008. Automatic discovery and transfer of maxq hierarchies. In ICML.\nThe Promise of Hierarchical Reinforcement Learning by Yannis Flet-Berlia in The Gradient\nHierarchical Reinforcement Learning lecture by Doina Precup on YouTube",
        "tags": [
            "RL",
            "hierarchy"
        ]
    },
    {
        "uri": "/posts/logic-program-policies",
        "title": "Few-Shot Bayesian Imitation Learning with Logical Program Policies",
        "content": "\n\nThis paper introduces a bayesian imitation learning approach to learn policies from few demonstrations. They call these policies Logical Program Policies (LPP) which are essentially policies learnt as combination of logical and programmatic policies. Logical because these are relational and programmatic because they are features are automatically learned.\n\nThe bayesian prior used here is the prior probability distribution over the Probablistic Context Free Grammer (P-CFG). Paper proposes to generate a dataset ($\\mathcal{D}$) where each state action pair $(s,a)$ is an example. Feature set of each example is obtained by initializing all the P-CFGs for each example. The target variable $y$ for each example is $1$ if $(s,a) \\in \\mathcal{D}$, $0$ otherwise.\n\nNext, all the features are arranged in decreasing order of their prior probabilities and iteratively decision-trees ($DT$s) are learnt with incremental feature size. So, at iteration $i$, features used are $fo, f1 , ... f_i$. The DT learned are converted to logical representation (i.e. disjunction of conjunctions of the P-CFG feature) and each DT is evaluated on the dataset $\\mathcal{D}$ and finally the top-$K$ DTs are used as weighted mixture model for testing.\n\nCritique\n\nThe paper is a difficult read and doesn't convey the actual procedure followed in the code. Algorithm in the paper suggest that the posterior $q$ is iteratively refined however digging into the code suggests that the posterior $q$\nis independently computed for each tree and there is no carry forward from one iteration to another.\n\nThe paper introduced a good set of 2D grid domains with generalizable domain-specific language. Code is very neat and easy to read.\n\nThe baselines used in the paper CNN and FCN are not meant for few-shot learning so it is not a surprise that they did not work. Some comparision to meta-learning approaches would have been useful.\n\n References\n\nhttps://github.com/tomsilver/policieslogicprograms",
        "tags": [
            "RL",
            "imitation",
            "generalization"
        ]
    },
    {
        "uri": "/posts/logical-nn",
        "title": "Logical Neural Network",
        "content": "\n This article summarizies Ryan Riegel, et al. arxiv 2020 and is written jointly with Siwen Yan\n\nThis paper proposes Logical Neural Network (LNN), a neural framework to perform logical inference. They propose to build a neural network with 1-to-1 correspondence with logical formulae. So, every neuron in the LNN is either a logical literal or logical gate. Given set of logical formulae, a LNN is a graph with one neuron for every unique proposition occurring in any formula and one neuron for each logical operation occurring in each formula, as shown in the figure below. Each neuron outputs a lower and upper bound on the truth values of the corresponding sub-formulae or proposition.  A logical neural accepts the output of their corresponding neurons and propositional neurons accepts the bound on propositions' truth value.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"../../images/LNN.png\"\n/div\n\nFor purpose of demonstrating the contributions authors use proposition formulae in the paper and extend it to FOL in the appendix. Hence, the class presentation focused on propositional formulae only.  It is long known that a neuron can be treated as logical gate and hence a neural network can approximate any boolean formula. Authors claim that although this fact is known, it has not been explored much. For example, KBANNsupa href=\"#ref1\"1/a/sup uses the symbolic knowledge to create an initial neural network but then the parameters of the neural network are learned from examples, so the notion of neuron being the logical gate is lost. Literature from differential ILP like Tensor Logsupa href=\"#ref2\"2/a/sup, which also uses symbolic knowledge/clauses to build the structure of neural network, do not use neurons as logical gates. Perhaps one exception is the CIL2Psupa href=\"#ref3\"3/a/sup work of Garcez.\n\nSince the truth values can be any value between [0,1], the choice of activation function for the logical operators must implement real-valued logic. There can be multiple ways of achieving that using importance weighting, this paper proposes weighted nonlinear logic using &#0321;ukasiewicz-like logic. Other types of real-valued logic functions can be used and the framework is able to accommodate that. The activation function defined using the weighted generalization of the &#0321;ukasiewicz logic still follows the logic properties and the DeMorgan's laws. Augmenting NN with FOL papersupa href=\"#ref4\"4/a/sup also uses  &Lstrok;ukasiewicz logic as activation function in the augmented network. The difference is that paper does not consider real-valued logic and hence does not have upper or lower bounds. It also does not use weighted generalization. This weighted generalization of logic is the main contribution of the paper.\n\nInference is performed using the iterative upward and downward pass of the network and learning is strictly restricted to the parameter learning since the structure is built using formulae. Hence the LNN model retains the interpretability of the original logical formulae in the network.\n\nLNN is empirically evaluated on 3 benchmark datasets: Smokers and friends, Lehigh University Benchmark (LUBM), and Thousands of Problems for Theorem Proving (TPTP). These experiments show that the LNN performs comparative to the LTNs (Logic Tensor Networks) and better than MLNs (Markov Logic Networks). LNN is better able to handle the contradictions and is the only neural model that was able to solve any common sense reasoning problem of TPTP. This clearly demonstrates the power of LNN as neural theorem prover.\n\nThe use of many-valued logic in AI or ML has been scarce. It has mainly been used in the philosophical studies and its usefulness in the field of AI is unexplored, except for the use in fuzzy logic. Perhaps this paper opens door for use of many valued logic in neuro-symbolic studies.\n\nStrengths\n\nThe differentiable way of using weighted nonlinear logic for activation function has huge potential.\nFirst neural theorem prover which doesn't need vector embeddings\nThe learnt model remains interpretable since representation is disentangled from neural parameters, as against Neural Theorem Proversupa href=\"ref5\"5/a/sup.\nThe compositionality or modularity of the network structure can potentially enable transfer\nOpens door for use of many-valued logic in neural setting.\nIt enables the open-world assumption by probabilistic bounds, yielding resilience to incomplete knowledge\n\nDrawbacks\nNeeds handcrafting of all the rules upfront, no structure learning\nDoes not support equality and functions\nStill needs grounding\nLNN should be compared with Neural Theorem Proversupa href=\"ref5\"5/a/sup, since they both are theorem provers.\nPaper is dense and difficult to read\n\nPotential improvements\nEnhance LNN with structure learning ability (rule induction)\n\n References\n\nsupa name=\"ref1\"^/a/supG. G. Towell and J. W. Shavlik. Knowledge-based artificial neural networks. Artificial intelligence, 70(1-2):119–165, 1994.\nsupa name=\"ref2\"^/a/supW. W. Cohen. Tensorlog: A differentiable deductive database, 2016.\nsupa name=\"ref3\"^/a/supA. S. d. Garcez and G. Zaverucha. The connectionist inductive learning and logic programming system. Applied Intelligence, 11(1):59–77, 1999.\nsupa name=\"ref4\"^/a/supTao Li, Vivek Srikumar, Augmenting Neural Networks with First-order Logic, ACL 2019\nsupa name=\"ref5\"^/a/supTim Rockt&auml;schel and Sebastian Riedel. End-to-end differentiable proving. NeurIPS, 2017.\n",
        "tags": [
            "neurosymbolic",
            "course"
        ]
    },
    {
        "uri": "/posts/MAML",
        "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
        "content": "\nMeta-Learning a.k.a the ''Learning to Learn'' problem, is the field of study where the researchers are trying to learn the parts of model which in standard machine learning setting are decided by researchers/humans/users. To elaborate, consider for example a standard gradient based machine learning problem. Given a training data and test data, to solve a problem the researches first decide what loss function to optimize and based on existing literature or their expertise they figure out various meta-information of the model. In the figure below, for a standard gradient based machine learning model meta-information like network structure, initialization parameters ($\\theta^0$), update method etc are all decided manually.\n\nMeta-learning research aims to learn a model which can help decide such meta-information (all or subset) for any new task.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/MAML-stdlearning.png\"\n\npsrc: a href=\"http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html\"Prof. Hung-yi Lee's slides/a/p\n/div\n\na name=\"a4\"i class=\"fa fa-bolt\"/i/a One of the use-case of meta-learning is in a field called few-shot learning. In few-shot learning, machine learning algorithm is supposed to learn a model for a task from few supervised examples. Meta-Learning can help in few-shot learning by providing a better initialization parameters. Few-shot learning is the problem of learning a model from few examples, meta learning is the problem of learning a model that can easily adapt to the new task from few examples.\n\nThis is also the premise of the Model-Agnostic Meta-Learning (MAML) paper by Finn et al 2017.\n\na name=\"a5\"i class=\"fa fa-bolt\"/i/a Transfer-Learning is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problemsup1/sup. For Deep Neural Network models, one of the the popular approaches to transfer-learning is by using a pre-trained model. Pre-trained model essentially transfers the knowledge of network parameters between different tasks. This is essentially equivalent to providing initialization parameters for the new task. Transfer-learning via pre-trained model as well as meta-learning use the network parameters of one model as initialization parameters for another model. The difference is in the optimization of the network parameters. While pre-trained model are optimize for some predefined task, meta-learning model are optimized so that they can adapt to new tasks quickly.\n\nThe key idea of Model-Agnostic Meta-Learning (MAML) algorithm is to optimize model which can adapt to new task quickly. Consider (pretty similar) tasks $\\{\\mathcal{T}i \\, | \\, i \\in \\{ 1,2,3,4\\}\\}$ with optimal parameters $\\{ \\theta^{\\star}i \\, | \\, i \\in \\{1,2,3,4\\} \\}$. Say, for $\\mathcal{T}4$ we have only $k$ supervised examples but we have large number of supervised examples for rest of the tasks i.e. $\\mathcal{T}1, \\mathcal{T}2$ and $\\mathcal{T}3$.\n\nA transfer learning approach will train $3$ different models (with parameters $\\theta1, \\theta2$ and $\\theta3$). Try all three as pretrained model for $\\mathcal{T}4$, compare the performance and pick one that works the best i.e. closest to $\\theta_4^{\\star}$.\n\nMAML on the other hand uses tasks $\\mathcal{T}1, \\mathcal{T}2$ and $\\mathcal{T}3$ for meta-training and treats them same as task $\\mathcal{T}4$ i.e. only uses $k$ example from each task. MAML learns a single model with parameter $\\theta$ in meta-training such that for each task $\\mathcal{T}i$ the gradient step using $k$ examples from that parameter $\\theta$ in the direction of $\\thetai^{\\star}$ reaches a $\\theta^{\\prime}i$. The meta-training objective to bring $\\theta^{\\prime}i$ close to $\\thetai^{\\star}$. So, the next update of the parameter $\\theta$ is a gradient step in a direction calculated as a linear combination of gradient step from $\\thetai^{\\prime}$ to $\\theta_i^{*}$. This is represented in the figure below, albeit not visibly.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/MAML-gradient.png\"\npa name=\"a1\"Finn et al. 2017 ICLR, Figure 1./a/p\n/div\n\nMeta-learning (bold line: &mdash;) is performing a search in parameter space such that a gradient step (gray line: &rarr;) for any of the training tasks $\\mathcal{T}i, i\\in \\{1,2,3\\}$ is close to optimal parameters $\\thetai^{\\star}$. The parameter $\\theta$ is then used as initialization value and fine-tuned for a specific task, this is called learning or adaptation (broken line: - - -).\n\nDuring meta-training, MAML adapts the parameter $\\theta$ for training tasks $\\mathcal{T}i, i\\in \\{ 1,2,3\\}$ to compute the $\\theta$ update. In meta-testing, MAML adapts the parameter $\\theta$ for test task $\\mathcal{T}4$. We obtain  $\\theta_4^{\\prime}$ by taking gradient step using the $k$ examples.\n\nParameter $\\thetai^{\\prime}$ is computed for any task $\\mathcal{T}i$ using following fine-tuning/learning/adaptation equation.\n\n$$\n\\theta\\{i}^{\\prime}=\\theta-\\alpha \\nabla\\{\\theta} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\_{\\theta}\\right)\n$$\n\nMeta-learning aims to reduce the distance between $\\thetai^{\\prime}$ and $\\thetai^{\\star}$. Since, $\\thetai^{\\star}$ is unknown it instead tries to minimize $\\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\_{i}^{\\prime}}\\right)$ for all the tasks. So, meta-objective is:\n\n$$\n\\min \\{\\theta} \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)=\\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta-\\alpha \\nabla\\{\\theta} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta}\\right)}\\right)\n$$\n\n Note that we restrict our model to minimize the objective of tasks from a distribution $p(\\mathcal{T})$.\n\nMeta-optimization is hence done with following update equation:\n\n$$\n\\theta \\leftarrow \\theta-\\beta \\nabla\\{\\theta} \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\_{i}^{\\prime}}\\right)\n$$\n\nNotice that the update equation above depends on the gradient of loss function $\\mathcal{L}\\{\\mathcal{T}i}\\left(f\\{\\thetai^{\\prime}}\\right)$, but $\\thetai^{\\prime}$ depends on the gradient of loss function $\\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta}\\right)$. So evidently MAML involves second level gradients.\n\nFull algorithm of MAML is quite easy to follow from the above three equations.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/MAML-algo.png\"\npa name=\"a2\"Finn et al. 2017 ICLR, Algorithm 1./a/p\n/div\n\nInstead of doing the search of $\\theta$ for all the tasks in training, like we did in the example above. MAML samples tasks from distribution $p(\\mathcal{T})$. a name=\"a3\"i class=\"fa fa-bolt\"/i/a In theory, this might just be a distribution of task based on available sample size of each task or distribution based on the similarity to the test task. In practice, they randomly sampled label set from images corpus and then sampled few examples for training and few for testing. To update the parameters $\\theta$ in line 8, code computes the second gradient using tensor flow optimizers.\n\nBelow figure  explains the MAML update equation used in practice. The first arrow for each task is the gradient from fine-tuning equation and the second arrow is from the meta-optimization equation.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"200\"  src=\"/images/MAML-update.png\"\npa href=\"http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html\"Adapted from slides of Prof. Hung-yi Lee./a/p\n/div\n\nMAML vs Pretrained\n\ndiv align=\"center\"\nimg align=\"center\" width=\"700\"  src=\"/images/MAML-fig4.png\"\npa name=\"a7\"Finn et al, ICLR 2017 Figure 4./a/p\n/div\n\nThe above image highlights the difference between MAML and pretrained models for the MAML-RL in 2D Navigation task. While the MAML model can adapt to the new task quickly, the pre-trained models take longer.\n\n First-order MAML\n\nSince the MAML involves second-order derivative, it can be computationally expensive. Authors propose a first-order approximation for such scenarios, by omitting the second order derivatives.\n\nSince,\n\n$$\n\\nabla\\{\\theta} \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)  =   \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\nabla\\{\\theta} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right) \\\\\\\\\n= \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\nabla\\{\\thetai^{\\prime}} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right) \\cdot \\nabla\\{\\theta} (\\thetai^{\\prime}) $$\n\nIn first-order MAML, authors use $\\nabla\\{\\theta} (\\thetai^{\\prime}) \\approx 1$\n\n<!-- $$\n\\nabla\\{\\theta} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)  = \\left[\\begin{array}{c}\\partial \\mathcal{L}\\mathcal{Ti}({\\theta^{\\prime}i}) / \\partial \\phi\\{1} \\\\ \\partial \\mathcal{L}\\mathcal{Ti}({\\theta^{\\prime}i}) / \\partial \\phi\\{2} \\\\ \\vdots \\\\ \\partial \\mathcal{L}\\mathcal{Ti}({\\theta^{\\prime}i}) / \\partial \\phi\\_{i} \\\\ \\vdots\\end{array}\\right]\n$$ --\n\nReptile\n\nReptile further simplifies the gradient computation of MAML by proposing following algorithm.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/reptile-algo.png\"\npa name=\"a2\"Nichol et al. 2018, Algorithm 1./a/p\n/div\n\n Notice that the initial parameter $\\theta$ used in MAML is equivalent to $\\phi$ in the reptile algorithm.\n\na name=\"a6\"i class=\"fa fa-bolt\"/i/a Instead of computing the $\\theta^{\\prime}_i$ for each task with one step gradient, Reptile computes parameter $W$ by running stochastic gradient descent for $k$ steps. Then instead of computing the gradient w.r.t the task for updating the initial parameter $theta$ (as done in line 8 of MAML), Reptile recommend to just shift the initial parameter in the direction of $W$ by using $(W - \\phi)$ as gradient. Below figure explains the Reptile update process.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/Reptile-update.png\"\npa href=\"http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html\"Adapted from slides of Prof. Hung-yi Lee./a/p\n/div\n\n<!--\n 2D Navigation domain\n\na name=\"a8\"i class=\"fa fa-bolt\"/i/a\n\nThe task in 2D Navigation domain is to direct the agent in the 2D space to a particular goal point. These goal points are randomly sampled for each task. So, for each task $\\mathcal{T}_i$ in 2D domain, the MDP  $S, A, T, R, \\gamma$ is defined by $S \\in \\mathbb{R}^2$, $A \\in \\mathbb{R}^2$ is velocity in range $[-0.1, 0.1]$, $R$ is the negative squared distance to the goal, and $T$ is deterministic movement between states.\n\n MDP is a set of $S, A, T, R, \\gamma$,  States, Actions, Transition function, Reward function, discount factor resp. It can also have initial distribution of state.   --\n\n<!-- ## Questions\n\nExplain Fig 1. What is meta learning and what is adaptation? Link to Answer\nExplain how gradient is calculated in the code? Link to Answer\nHow is $p(\\mathcal{T})$ : distribution of task decided in practice for supervised learning and reinforcement learning setting? Link to Answer\nWhat is few-shot learning and what is the relation-ship between meta-learning and few-shot learning. Link to Answer\nDifference between meta-learning and model pre-training. Link to Answer\nDifference between MAML and reptile. Link to Answer\nExplain two subfigures in Fig-4. Link to Answer\n!-- 1. Use the example of 2D Navigation to define all the components of MDP and explain algorithm 3 step by step , particularly the steps related to MAML. Link to Answer --\n\nReferences\n\na name=\"myfootnote1\"Definition of transfer learning from Wikipedia/a\nICML 2019 Meta leraning tutorial  \nProf. Hung-yi Lee's slides on meta learning\nAlex Nichol, Joshua Achiam, John Schulman On First-Order Meta-Learning Algorithms 2018 -- Reptile paper\nReptile Blog\nPaper repro: Deep Metalearning using “MAML” and “Reptile”\n\n Further resources\n\nTutorial on Meta-Learning by Dr. Lilian Weng\nAwesome meta learning lists",
        "tags": [
            "metalearning",
            "course"
        ]
    },
    {
        "uri": "/posts/meta-attack-GNN",
        "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning",
        "content": "\nPaper highlights the weakness of Graph Neural Networks. Since the iid assumption does not hold in the graph data, any perturbation at a single node might have major impacts. This weakness can be leveraged by adversaries to attack GNN. The paper is set in a transductive learning setting where a graph is given along with label to some of the nodes and the task is to predict labels of the remaining nodes. The objective of the adversarial attack is to reduce the overall performance of the model by modifying the training data. So the objective can be formulated as a max-min problem where the attacker wants modify the graph to maximize the loss while the training agent will learn parameters to minimize the loss.\n\n$$\n\\underset{{G}}{\\arg\\max } \\  \\  \\underset{\\theta}{\\arg\\min} \\  \\mathcal{L}\\{\\text{train}} \\left(f\\{\\theta}({G})\\right) ,\n$$\n\nThis can be reformulated as following optimization function:\n\n$$\n\\underset{\\hat{G} \\in \\Phi(G)}{\\arg \\min} \\  {\\mathcal{L}\\{\\mathrm{atk}}}\\left(f\\{\\theta^{\\ast}}(\\hat{G})\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg \\min } \\quad \\mathcal{L}\\{\\operatorname{train}}\\left(f\\{\\theta}(\\hat{G})\\right)\n$$\n\nIdeally $\\mathcal{L}\\{\\mathrm{atk}} = - \\mathcal{L}\\{\\mathrm{train}}$ but authors also use another option $\\mathcal{L}\\{\\mathrm{atk}} = - \\mathcal{L}\\{\\mathrm{self}}$ where they learn a model to predict the labels of the unlabeled dataset and then try to maximize the prediction error for those nodes. Since the above mentioned objective is a bilevel optimization problem it is a difficult to solve and hence authors propose a meta-learning approach.\n\nMeta-Learning\n\nRecall that in MAML we saw a similar objective where we wanted to optimize parameter $\\theta$ such that the loss on individual tasks $\\mathcal{T}_i$ is also minimum when adapted from parameter $\\theta$:\n\n$$\n\\underset{\\theta}{\\arg \\min} \\sum\\{\\mathcal{T}\\{i} \\sim p(\\mathcal{T})} \\mathcal{L}\\{\\mathcal{T}\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)  \\quad \\text { s.t. } \\theta\\{i}^{\\prime}  = \\underset{\\theta\\{i}^{\\prime} }{\\arg \\min } \\quad \\mathcal{L}\\{\\operatorname{\\mathcal{T}\\{i}}}\\left(f\\_{\\theta}\\right)\n$$\n\nBelow is a quick review of MAML equations. Map the colors with the image visualize to the equations.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"600\"  src=\"../../images/meta-attack-maml.jpeg\"\n/div\n\nsimulated adaptation / learning / training $(\\nabla\\{\\theta}\\mathcal{L}\\{T\\_{i}})$ update performed by the meta-learner is as follows:\n\ndiv style=\"color:B92D5D\"\n\n$$\\theta\\{i}^{\\prime}=\\theta-\\alpha \\nabla\\{\\theta} \\mathcal{L}\\{T\\{i}}\\left(f\\_{\\theta}\\right)$$\n\n/div\n\nmeta objective is $\\theta\\{i}^{\\prime}$ should be close to $\\thetai^{\\ast}$\n\ndiv style=\"color:#1200F4\"\n\n$$\\underset{\\theta}{\\arg \\min} \\  \\sum\\{T\\{i}} \\mathcal{L}\\{T\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)$$\n\n/div\n\n$\\therefore$ meta update $\\left(\\nabla \\mathcal{L}\\_{\\text {meta }}\\right)$  is:\ndiv style=\"color:#1200F4\"\n\n$$\n\\begin{align}\n\\theta & =\\theta-\\beta \\sum\\{T\\{i}} \\nabla\\{\\theta} \\mathcal{L}\\{T\\{i}}\\left(f\\{\\theta\\_{i}^{\\prime}}\\right)\\\\\n&=\\theta- \\beta\\sum\\{T\\{i}} \\nabla\\{\\theta\\{i}^{\\prime}} \\mathcal{L}\\{T\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right) \\cdot \\nabla\\{\\theta} \\theta\\{i}^{\\prime}\n\\end{align}\n$$\n\n/div\n\nfirst  order approximation assumes  $\\nabla\\{\\theta} \\theta\\{i}^{\\prime}=1$  \n\ndiv style=\"color:#1200F4\"\n\n$$=\\theta-\\beta \\sum\\{T\\{i}} \\mathbin{\\color{#EA6B2D}{\\nabla\\{\\theta\\{i}^{\\prime}}\\mathcal{L}\\{T\\{i}}\\left(f\\{\\theta\\{i}^{\\prime}}\\right)}}$$\n\n/div\n\nAdversarial Meta-Learning\n\nIn MAML both the meta objective and the adaptation/learning phase optimized over the same parameters. In this paper, the adaptation/learning phase optimizes model parameters but the meta-objective optimizes the graph $G$. So the equations look quite similar with meta objective optimizing $G$ instead of $\\theta$.\n\nAdversarial objective is\n\n$$\\underset{\\hat{G}}{\\arg\\min }  \\  \\mathcal{L}\\{\\text{atk}}\\left(f\\{\\theta^{\\ast}}(\\hat{G})\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg\\min} \\  \\mathcal{L}\\{\\text {train }}\\left(f\\{\\theta}(\\hat{G})\\right)$$\n\nsimulated adaptation / learning / training update by the attacker is\n\ndiv style=\"color:B92D5D\"\n\n$$\n\\theta =\\theta-\\alpha \\nabla\\{\\theta} \\mathcal{L}\\{\\text{train}} f\\_{\\theta}(G)\n$$\n\n/div\n\nmeta update:\ndiv style=\"color:#1200F4\"  \n\n$$\n\\begin{align}\nG & = G -\\beta \\nabla\\{G} \\mathcal{L}\\{\\text{atk}}\\left(f\\_{\\theta}(G)\\right)\\\\\n& =G-\\beta \\nabla\\{f} \\mathcal{L}\\{\\text{atk}}\\left(f\\{\\theta}(G)\\right) \\bigg[ \\nabla\\{G} f\\{\\theta}(G)+\\nabla\\{\\theta} f\\{\\theta}(G) \\nabla\\{G} \\theta \\bigg]\n\\end{align}\n$$  \n\n/div\n\nfirst order approximation $\\theta \\approx \\tilde{\\theta}$ where $\\left(\\nabla\\_{G} \\tilde{\\theta}=0\\right)$\n\ndiv style=\"color:#1200F4\"\n\n$$\n=G-\\beta \\nabla\\{f} \\mathcal{L}\\{\\text{atk}}\\left(f\\{\\tilde{\\theta}}(G)\\right) \\nabla\\{G} f\\_{\\tilde{\\theta}}(G)\n$$\n\n/div\n\nAnother major difference between MAML and this paper is the assumption in the first order approximation of the meta-gradients. In MAML, the first order approximations assumes $\\nabla\\{\\theta}\\thetai^{\\prime} = 1$ i.e. it assumes the parameters $\\theta$ and $\\theta^{\\prime}$ are essentially the same. Here, in the first order approximation, authors assume that the $\\nabla\\_{\\tilde{\\theta}}G= 0$ i.e. the graph $G$ is constant  (i.e.  independent of $\\tilde{\\theta}$).\n\nIn the exact meta-attack version, the graph $G$ used in the simulated learning/adaptation/training phase is constantly optimized by the attacker w.r.t parameters and hence the $\\nabla\\{{\\theta}}G \\neq 0$. But for the approximate version, the graph $G$ is optimized only after $t$ steps of simulated adaptation/training/learning iterations. So, the for most of the $\\tilde{\\theta}$ updates the graph $G$ is gonna be constant. Hence, the parameter $\\nabla\\{\\tilde{\\theta}}G = 0$.\n\nSince, the graph modifications are limited to edge manipulations, the optimization objective replaces graph $G$ with adjacency matrix $A$. And since the gradient direction is the direction of maximizing the function and we want to maximize the $L_\\text{train/self}$, we use +ve sign for gradient update.\n\n$$\nA = A - \\beta \\  \\nabla\\{A}\\mathcal{L}\\\\text{atk}(f\\_\\theta(A)) \\\\\n= A + \\beta \\  \\nabla\\{A}\\mathcal{L}\\\\text{train/self}(f\\_\\theta(A)) \\\\\n$$\n\nGraph admissibility:  $\\Phi(G)$\n\nSince the inherent objective of an attack is be unnoticeable there are certain constraints on modifications that the attacker can do on the graph $\\hat{G} \\in \\Phi(G)$\nThere is a budget on the number of perturbations allowed. So, $||A−\\hat{A}||_0 \\leq \\Delta$, where $\\Delta$ is a budget and $A$ and $\\hat{A}$ are adjacency matrix of original and modified graph $G$\nNodes which was initially connected should remain connected, so no singleton node created as a result of perturbations\nDegree distribution of the graph remains the same.\n\n Extension to graph attributes\n\nTo extend the meta learning formulation to modify the graph attributes we can treat the node feature matrix of the graph $X$ as hyper-parameter and reformulate the attack objective as below.\n\n$$\n\\underset{{X}}{\\arg \\min} \\  {\\mathcal{L}\\{\\mathrm{atk}}}\\left(f\\{\\theta^{\\ast}}({X}, A)\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg \\min } \\quad \\mathcal{L}\\{\\operatorname{train}}\\left(f\\{\\theta}({X}, A)\\right)\n$$\n\nSo, the meta gradient equation will be as follows:\n\n$$\nX = X + \\beta \\  \\nabla\\{X}\\mathcal{L}\\\\text{train/self}(f\\_\\theta(X, A)) \\\\\n$$\n\nEmpirical evaluations\n\nExperiments show that the proposed method is indeed able to reduce the classification accuracy of the model from around 82% to 60% by making change in 15% of the edges (Fig 1). Interesting insight is in table 3 is that if parameters $W$ trained with a clean graph $A$ are used on the modified graph $\\hat{A}$, it is still able to achieve 83% accuracy on the perturbed graph. But, if the parameters $\\hat{W}$ are trained on the perturbed graph $\\hat{A}$ the accuracy on the clean as well as perturbed graph is reduced significantly.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"700\"  src=\"../../images/meta-attack-t3.png\"\n/div\n\nThe analysis of perturbed graphs reveals that the majority of the perturbations in the graph are edge insertions (table 5). Yet, the mean shortest path of the adversarial graph is higher than the original graph. This might mean that the edges which are removed in the perturbations were some of the key connections.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"700\"  src=\"../../images/meta-attack-f2.png\"\n/div\n\n Critique\n\nThe paper brings forth a novel application of meta-learning in the bilevel optimization problems and demonstrates a successfully use case of adversarial attacks. They show both the exact and approximate formulations and their results. The approach was successful in reducing the classification accuracy.\n\nThe attacker here is making an assumption about the learning algorithm that will be used for classification, which might not be true in general. In meta-learning since the parameters are shared between the meta-learning and the actual classification model, the assumption on the learning algorithm is valid. However, here the attacker is modifying the training data before the classifier is learned and a different entity is gonna learn classifier. I feel the assumption of the attacker is not justified.\n\nIt would be interesting to see the classification accuracy of different GNN models on the $A$ and $\\hat{A}$, other than the ones which were used while attacking.\n\nQuestions\n\n Q1. How is min max problem $\\operatorname{max}\\x\\operatorname{min}\\\\theta f(x, \\theta)$ solved by this approach?\n\nThe problem $\\operatorname{max}\\x\\operatorname{min}\\\\theta f(x, \\theta)$ is solved using meta-gradients by replacing the $\\operatorname{min}\\_\\theta$ with meta optimization step, $\\theta^\\ast$.\n\nQ2. List types of attacks and summarize it one line and discuss differences\n\nTargeted Attacks: Attacks that are aimed to change prediction of a single example\nGlobal Attack: Attacks which aim at changing the overall performance of the model\nEvasion Attacks: Exploratory attacks done during test time for e.g. to understand the thresholds of a classifier.\nPoisoning Attacks: Causative attacks done during training to mess up the learning process.\n\n Q3. How is the linear surrogate model obtained from two layer GCN?\n\nReplacing the $\\sigma$ activation function with an Identity function $I$, the non-linearity is removed and we can achieve a linear function.\n\n$$\n\\begin{align}\nf\\{\\theta}(A, X) & =\\sigma\\left(\\hat{A} \\sigma \\left(\\hat{A} X W1 \\right) W_2 \\right)\\\\\\\\\n& = \\mathbf{I}\\left(\\hat{A} \\mathbf{I} \\left(\\hat{A} X W1 \\right) W2 \\right) \\\\\\\\\n& = \\left(\\hat{A}^2 X W \\right) \\\\\\\\\n\\text{where, } W & = W1W2 \n\\end{align}\n$$\n\nQ4. How is the following equation derived from the  MAML paper?\n\n$$\\nabla\\{A}^{\\text {meta }}= \\nabla\\{f} \\mathcal{L}\\{\\text {atk }}\\left(f\\{\\tilde{\\theta}\\{T}}(A)\\right) \\cdot \\nabla\\{A} f\\{\\tilde{\\theta}\\{T}}(A)$$\n\nThis equation is derived using the first order approximations discussed above.\n\n Q5. What is the difference between $\\thetat$ and $\\tilde{\\theta}t$?\n\nThe difference between $\\thetat$ and $\\tilde{\\theta}t$ is the notion of constant $G$ vs dynamic $G$ as explained above.\n\nQ6. How can meta-gradient problem formulation be modified to attack node features.\n\nMentioned above in extension to graph attributes section\n\n Q7/8. Summarize the impact and analysis of the attack\n\nMentioned above in Emprical evaluations section\n\nQ9.",
        "tags": [
            "GNN",
            "adversary",
            "course"
        ]
    },
    {
        "uri": "/postsminimal-sufficient-explanation",
        "title": "Minimal Sufficient Explanations for Factored Markov Decision Processes",
        "content": "\nAutomated planning problems have long been attempted using Markov Decision Processes (MDPs). MDPs are capable of handling the probabilistic and sequential nature of planning problem. They solve the problem by providing a policy which is a mapping from states to actions. However, to use this policy in the real-world, we first need users to trust the policy. The issue of trust can be ameliorated if the policy provides an explanation for its recommeded actions. This is the pretext of khan et al 2009.\n\nPolicy in MDP is usually obtained by optimizing Bellman's equation which considers value of a state as expected discounted total reward. This paper uses an alternate formulation of the value function introduced by  and extends it to the factored MDPs. In the alternate formulation, value function is expressed as a dot product of reward and the discounted occupancy frequencies of all the states. With this formulation, an optimal policy for a given state can explain the recommended action by simply showing that the recommended action has highest occupancy frequency of highly rewarded state(s). To this effect the paper propose three explanation templates:\n\nActionName is the only action that is likely to take you to Var1 = Val1, Var2 = Val2, ... about $\\lambda$ times, which is higher (or lower) than any other action.\nActionName is likely to take you to Var1 = Val1, Var2 = Val2, ... about $\\lambda$ times, which is high (or low) as any other action''\nActionName is likely to take you to Var1 = Val1, Var2 = Val2, ... about $\\lambda$ times.\n\nVar1 = Val1, Var2 = Val2, ... in the template represents a single state (in case of MDPs) or a scenario\\footnote{In the case of factored MDPs, the reward function is also factored and the scenario used is defined as set of states which have similar rewards.} (in case of Factored MDPs). Multiple templates can be used for explaining a recommended action.\n\nPower of these templates can be realized if they provide minimal-sufficient explanations (MSE). An explanation is sufficient if it can prove that the action recommended is optimal (or no other action has better utility than the recommended action). Sufficient explanation is minimal if the number of templates used are minimum possible.\n\nWhen the policy ($\\pi^{\\ast}$) is optimal, the value of the recommended action for state $s0$~$\\left(V^{\\pi^\\ast}\\right.$ or $\\left.Q^{\\pi^\\ast}(s0, \\pi^\\ast(s0))\\right)$ will be higher than value of all other actions $\\left(Q^{\\pi^\\ast}(s0,a)\\, \\,  \\forall a \\neq \\pi^\\ast(s0)\\right)$. Khan et al. 2009 define $V{MSE}$ (eqn.  below) which has two components: (a) expected utility from all the states(or scenarios) included in the explanation, and (b) worst case utility of all the states (or scenarios) not included in the explanation. By this definition $V{MSE}$ cannot exceed the value of optimal action. If the explanation is minimal and sufficient, $V{MSE}$ will be higher than the value of any other action.\n\n\\\\[\nV{MSE} = \\sum{i \\leq k}\\mathbb{E}[\\operatorname{utility}(si)] + \\sum{i  k} \\min(\\operatorname{utility}(s_i))\n\\\\]\n\n\\\\[\nV^{\\pi^{\\star}} \\geq V{MSE}  Q^{\\pi^\\star}(s0,a)\\, \\,  \\forall a \\neq \\pi^\\star(s_0)\n\\\\]\n\nWith that definition of $V{MSE}$, Khan et al. propose an algorithm to produce MSE at any state $s0$ for a given optimal policy in factored MDP setting. They evaluate the MSE on two experimental domains of \"course-advising\" and \"handwashing\". The experimental results show that the number of templates in MSE is high (or low) when the ratio of the expected utility for the optimal and the second best optimal policy is high (or low). User study with advisors in the course-advising domain show that the advisors found the explanations useful and perhaps beneficial for grade-conscious or poor performing students. User study with students show that more than 50\\% of the students found the explanations easy to understand, accurate and informative. However, most of them needed further information to trust the action recommended.\n\nCritique\n\nThe paper addresses a very important topic of explaining the recommendations of a Markov Decision Process. The work is especially commendable as it was done way ahead of the Explainable AI Hype. They provide a very principled and straight forward way to explain the recommendations from the perspective of how the policy is constructed. However, it might not be intuitive enough to the users. As seen in the user-study, most of the users wanted to compare the explanation of the recommended action with their preferred actions or need more information about occupancy frequencies.\n\nIn complex domains, Bellman's optimality equation is not solved exactly and hence, the value function of each state is only discovered approximately by the RL agent. In that scenario, as I understand, the optimality of the policy is only with respect to that approximate value function and hence the explanation of optimality is also only with respect to that approximate value function. To end user, this values might not mean anything and hence the sufficiency of the explanation might be questioned.\n\nThe explanations are only useful when it has less number of templates, the user-study in course-advising domain shown in the paper has less than 3 templates for the explanations. But in complex domain, as seen in handwashing experiments the number of templates will eventually increase and I conjecture that with increase in the number of templates the explanations will be less meaningful to the users.\n\nAlso, as raised by Dr. Natarajan in the class discussion the template only explains the action with respect to the states it visits but not with respect to the states it helps avoid. For domain where we want to avoid a certain states, we encode high negative rewards for that state. Even though the optimal policy learns to avoid that negative state, the MSE framework will only look at the occupancy frequency of highly rewarding states and try to explain positive reward states. As the occupancy frequency of the highly negative state is $0$ with the recommended action, it will not come up in the explanation. Although the templates proposed does mention \"or lower\" in brackets, it is not quite clear how the avoided state will be added in the explanation.\n\nFinally, in RL we usually assume the policy is mapping of states to a distribution over actions. Explaining a distribution might be complex than a single action. This paper concentrates on explaining the best action out of that distribution. However, there might be two equally good actions, or there might be more than one optimal policy. One has to extend this framework for that use cases before using it in a real system.\n\n References\n\nPascal Poupart. 2005. Exploiting structure to efficiently solve large scalepartially observable Markov decision processes. Citeseer.",
        "tags": [
            "planning",
            "XAI",
            "RL"
        ]
    },
    {
        "uri": "/posts/neurosymbolic-systems",
        "title": " The 6 Types of Neuro-Symbolic Systems",
        "content": "\n\nProf. Henry Kautz, in his Robert S. Engelmore Memorial Lecture at AAAI 2020(https://montrealartificialintelligence.com/aidebate/) between Prof. Gary Marcus and Prof. Yoshua Benjio. In this regard, he brings forth a taxonomy of Neuro-Symbolic Systems that I aim to elaborate upon.\n\n1. symbolic Neuro symbolic  \n\nThis is the current standard operating procedure of deep learning, where any/all the symbols in the problem are converted to vector embeddings. Vectors are then processed by neural models, which spits out another vector which is then converted to the required symbol.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/symbolicNeurosymbolic.png\"\npsymbolic Neuro symbolic/p\n/div\n\nMost of the NLP systems fall under this category since words are converted to vectors before the neural manipulations are conducted. Personally, I would not call this a Neuro-Symbolic integration.\n\n 2. Symbolic[Neuro]  \n\nThis is an over all symbolic solver which uses Neural model internally as subroutine for one or more function.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/Symbolic[Neuro].png\"\npSymbolic[Neuro]/p\n/div\n\nThe example provided by Prof. Kautz includes Alpha Go system which uses a symbolic Monte-Carlo Tree search algorithm while using a neural state estimator.\n\n3. Neuro;Symbolic  \n\nThis is a more refined integration of Neural and Symbolic approaches where the Neural and Symbolic systems are leveraged for different tasks in a big pipeline. Both systems communicate with each other either to extract information or to improve the individual/collective systems performance.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/Neuro;Symbolic.png\"  \nbr\nbr\npNeuro;Symbolic/p\n/div\n\nThe Neuro Symbolic Concept Learner by Mao et al.^ is presented as an example of this type of Neuro-Symbolic integrations since the Neural model does object detection and the symbolic reasoner (which is a GNN) does the relational reasoning. I am not quite comfortable with categorizing the GNN as symbolic systems. So, I would not consider the Mao et al. as Type 3.\n\nI believe the Illanes et al. ICAPS 2020^ and other related work are a better candidate for Type 3 Neuro-Symbolic integration. Illanes et al. integrate the symbolic planner with the neural RL agent and both systems feed off of each other.\n\n 4. Neuro:Symbolic $\\rightarrow$ Neuro  \n\nThis type is categorized as Neuro-Symbolic systems where the symbolic knowledge is compiled into the structure of Neural models. Kautz and Lamb et al. both provide example of Lample and Charton 2020, but I fail to understand how Lample et al. is compiling the symbolic knowledge into the neural model. However, the other examples provided by Lamb et al. are quite easy to follow. Arabshahi et al. compiles the symbolic expression tree to tree LSTMs. Other works like Hitzler et al. 2004(#references) compiled the logic program or if..then.. rules to a Neural Network architecture.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/Neurosymbolic-neuro.png\"  \nbrbr\n\nNeuro:Symbolic $\\rightarrow$ Neuro\n\n/div\n\n5. NeurosubSymbolic/sub\n\nThis category was not listed during the AAAI 2020 Lecture, but added in Prof. Kautz' slides later(references)], Tensor Product Representation [Smolensky et al. 2016, Neural Tensor Network [Socher et al. 2013, relational embeddings etc.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/Neuro_symbolic.png\"  \nbrbr\nNeurosubSymbolic/sub\n/div\nbr\n\nThe difference between this and type 1 is that in type 1 only the objects are represented as vector embeddings. Here  complete first-order language is represented by tensor embedding, including object, relation, clauses/rules, functions, predicates.\n\n6. Neuro[Symbolic]\n\nFinally, the last one is the transpose of Type 2. Here, the overall Neural model performs symbolic reasoning by either learning the relations between the symbols or paying attention to selected symbols at certain point. This is exactly what the Graph Neural Network do. So I think GNN is synonymous to Type 6 neuro-symbolic integration. This is also the argument made by Lamb et al. 2020^.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/Neuro[Symbolic].png\"  \nbrbr\nNeuro[Symbolic]\n/div\nbr\n\nI prefer calling the GNN as type 6 Neuro-Symbolic integration then calling GNN as symbolic reasoners. I believe symbolic reasoners, in general, have more reasoning capability than GNN. So, calling GNN as symbolic reasoner in my opinion is a stretch.\n\n a name=\"refereces\"References/a\n\nRobert S. Engelmore Memorial Lecture at AAAI 2020 by Henry Kautz. The Third AI Summer\nLamb et al. Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. IJCAI, 2020\nArabshahi et al. Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs.\tICLR, 2018\nHitzler et al. Logic programs and connectionist networks. Journal of Applied Logic, 2004\nGarcez et al. Neural-symbolic learning and reasoning: contributions and challenges. AAAI Spring Symposium Series. 2015\nSmolensky et al. Basic Reasoning with Tensor Product Representations. arXiv:1601.02745, 2016  \nSerafini et al. Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge. arXiv:1606.04422, 2016\nSocher et al. Reasoning With Neural Tensor Networks For Knowledge Base Completion. NeurIPS, 2013\nIllanes et al. Symbolic Plans as High-Level Instructions for Reinforcement Learning. ICAPS 2020\nMao et al. The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. ICLR 2019\n!-- * https://www.digitaltrends.com/cool-tech/neuro-symbolic-ai-the-future/ --\n",
        "tags": [
            "neurosymbolic",
            "summary"
        ]
    },
    {
        "uri": "/posts/NN-with-FOL",
        "title": "Augmenting Neural Networks with First-order Logic",
        "content": "\n\nThis paper addresses the problem of incorporating declarative  knowledge into a Neural Network. They propose converting the (easily available) first-order logic representation of the knowledge into a network and provide a framework to augment this network to any neural network of choice. The main motivation to use the declarative knowledge as an inductive bias is to reduce the dependency of the data, to achieve comparative performance with less examples.\n\nTo convert the FOL rules to a network, each predicate in the rule is mapped to a named neuron. For example, given a rule $A1 \\wedge A2 \\rightarrow B1$, the network will have 3 named neurons: $a1, a2,$ and $b1$ with arrow from $a1$ and $a2$ to $b_1$. The &Lstrok;ukasiewicz T-norm and T-conorm are used as functions for the logical operators, inspired by probabilistic soft logic literature. Auxiliary variables and auxiliary named neurons are included as needed to compute logical operations. For example, $(\\lnot A \\vee B) \\wedge (C \\vee D)$ is converted to $P \\wedge Q$ with $(\\lnot A \\vee B) \\leftrightarrow P$ and $(C \\vee D) \\leftrightarrow Q$. The benefit of using &Lstrok;ukasiewicz functions is that they are differentiable. This network doesn't have any parameters hence do not require any learning.\n\nTo ensure that the network is acyclic, the authors recommend using contrapositive statements when needed. For example, if the rule $B1 \\rightarrow A1$ is introducing cycle in the network, then use its contrapositive equivalent $\\lnot A1 \\rightarrow \\lnot B1$ instead.\n\nThis rule network is added as constraint to some layer $y=g(\\mathbf{Wx})$ of the original neural network. The constrained neural layer is defined as follows with hyperparameter $\\rho$ handling the importance factor.\n\n$$y = g(\\mathbf{Wx} + \\rho \\underbrace{d(\\mathbf{z})}_{knowledge})$$\n\nAuthors empirically evaluate their proposed augmented NN for three tasks: machine comprehension, natural language inference, and text chunking. In each of these tasks the augmentation is performed at different layers. In machine comprehension task, where the use BiDAF as the base neural network, the constrained augmentation is done for attention nodes. In natural language inference task, they use L-DAtt as the base method and augment attention node as well as label nodes. In the text chunking task, they augment the label layer. These experiments confirm their hypothesis that using the knowledge improve the performance, but only when the data is less. With more data, the augmented knowledge does not improve performance significantly.\n\nCritique\n\nThe framework of augmenting NN proposed is very general and hence can be potentially used in any task where deep neural networks are used.\nI haven't quite understood the emphasis on the differentiability of the augmented network since there are no parameters to be learnt there. The hyperparameter $\\rho$ is tuned.\nThe right hand side of the rule looks pretty limited. The rules used in the experiments are also very simple.\nIn the text chunking task we would assume that the bidirectional LSTM would be able to learn rules $C_{1:4}$. It is not clear from experiments which rule improves the results in this task.\n",
        "tags": [
            "neurosymbolic",
            "course"
        ]
    },
    {
        "uri": "/posts/relational-inductive-bias",
        "title": "Relational Inductive Bias",
        "content": "\n\nThere is a recent surge in papers which are using Relational Inductive Bias with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic.\n\n(Inductive) Bias refers to any basis for choosing one generalization over another, other than strict consistency with the observed training instances [Mitchell 80]. Relational inductive bias refers to biases which impose constraints on relationships and interactions among entities. Logic Programming extensively deals with the entities and relationships. So, it seems to me there should be some equivalence between the biases used in ILP with the Relational Inductive biases.\n\nBias in ILP\n\n ... bias is anything which influences how the concept learner draws inductive inferences based on the evidence. There are two fundamentally different forms of bias: declarative bias, which defines the space of hypotheses to be considered by the learner, i.e., what to search, and preference bias, which determines how to search that space, which hypotheses to focus on, and which ones to prune, etc.  \n...  \nILP systems distinguish two kinds of declarative bias: syntactic bias (sometimes also called\nlanguage bias) and semantic bias. Syntactic bias imposes restrictions on the form (syntax)\nof clauses allowed in hypothesis. ... Semantic bias\nimposes restrictions on the meaning, or the behavior of hypotheses.  \n-- Muggleton and De Raedt, 1994\n\nAnother typology of biases include following categorizations\n\nLanguage Bias - specifies set of syntactically acceptable clauses.  \nSearch Bias - specifies which region of hypothesis set should be searched.  \nValidation Bias - specifies when the search should stop.  \n\n Language Bias\n\nSome straight forward syntactic constraints can be maximum length of clause, number of new variables that can be introduced in the clause, number of function operators used in horn clauses, etc. All of these are form of language-bias.\n\nMore evolved language biases include  \nspecify the set of clauses allowed in hypotheses  \neach clause in this set can further contain set of literals which are allowed    \n$\\quad$ e.g, $\\operatorname{father}(X, Y) \\leftarrow \\{ \\operatorname{male}(X), \\operatorname{female}(X) \\}, \\operatorname{parent}(X, Y);$  \n\nsecond-order schemata  \nSecond-order schemata defines a language bias as the set of all clauses that can be obtained by instantiating a second-order schema with a second-order substitution.  \nA second-order schema is a clause, where some of the predicate names are (existentially quantified) predicate variables.  \n$\\quad$ e.g.,\n$S = \\exists p, q, r : p(X, Y) \\leftarrow q(X, XW), q(YW, Y), r(XW, YW);$  \nA second-order substitution is a substitution that replaces predicate-variables by predicate names.  \n$\\quad$ e.g., $\\theta = \\{ p /\\operatorname{connected}, q/\\operatorname{part-oj}, r/\\operatorname{touches}\\}$   \ngives  \n$\\quad$ $S\\theta = \\operatorname{connected(X, Y)} \\leftarrow \\operatorname{part-of}(X,XW), \\operatorname{part-ox}(YW, Y), \\operatorname{touches}(XW, YW)$\n\nSearch Bias\n\nSearch bias can be a restriction or preference. With restriction, some hypothesis space is ignored. While with preference, some space is prioritized while searching. Search bias is given by $types$ and $modes$ in ILP. This can also be semantic bias\n\nOne important search/semantic bias used in most ILP systems is notion of determinate clause. *\"A clause is determinate if all of its literals are determinate; and a literal\nis determinate if each of its variables that does not appear in preceding literals has only one\npossible binding given the bindings of its variables that appear in preceding literals.\"* (from Muggleton and De Raedt, 94).\n\nSearch biases can be broadly categorized as:\nOrdering  \n  Specification of ordering for pre-compiled hypotheses or relations to be considered for learning.\nExample selection criteria  \n  How the examples are selected to verify some candidate hypothesis\nCoverage function\nIntermediate validation criteria\n\n Bias in Graph Networks\n\nIn Graph Networks, inductive biases used include non-relational inductive biases like choice of activation function, dropout, weight decay, training curricula, algorithm optimization etc. Relational inductive bias, on other hand are the biases that are constrain the interactions of entities, relationships and rules while learning the model. Entities/Nodes are elements with attributes, relations/edges are property between entities and rule is a function mapping set of entities and relations to another entity and relation.\n\nThree main relational inductive bias introduced in graph networks are\nLocality\nTranslation invariance\nPermutation invariance\n\nFor fully-connected (FC) layers, all the entities (nodes in network) are connected to all other entities and there is no restriction on rules that can be used. So, relational-inductive bias is very weak for FC layers.  \n\nFor Convolutional layer, the nodes are the pixels of images and edges are the relation between neighbors. Rules are defined by the weights and biases of the hidden network.\nOne relational-inductive bias in here is constraint of locality. Rules should use related nodes, i.e. convolution function should only use the neighbors, it can not use arbitrary pixels.\nAnother relational-inductive bias used is that the kernel matrix is same for all the neighborhoods, i.e. all the localities are related in same way so rule is translation invariant.\n\nIn ILP, the locality bias is, perhaps, induced by use of $\\operatorname{mode}$. A $+ve \\, \\operatorname{mode}$ is preferred in a literal when adding it as candidate to a hypothesis clause. In a sense, we prefer edges/connected nodes from previously selected nodes in the hypothesis clause.\nThe translation invariance bias is, perhaps, integral in ILP since horn clause by definition use $\\forall$ operator.\n\nIn images, there is a natural ordering of the pixels and hence the nodes are ordered. But in most graphs, nodes do not have a natural ordering, nor does edges. One of the most important contribution of the Graph Neural Networks and Graph Convolutional Networks is to provide a way to operate on nodes in graph which is permutation invariant. So, this becomes the third and the most important relational inductive bias.\n\nILP does not assume any natural ordering of the entities or relations. Hence there is no need of an equivalent bias.\n\nConclusion\n\nThere are many more biases in ILP which are not yet accounted for in Graph Networks and hence if we find a way to include those biases for Graph Networks, we might be able to achieve sample efficient and effective learning in Graph Networks.\n\n References\n\nInductive Logic Programming: theory and methods, Stephen Muggleton and Luc De Raedt, J. Logic Programming 1994\nClaire N&eacute;dellec, C&eacute;line Rouveirol, Hilde Ad&eacute;, Francesco Bergadano, and Birgit Tausend. Declarative Bias in ILP, 1996.\nTom Mitchell, The need for biases in learning generalizations, 1980.\nBattaglia et al. Relational inductive biases, deep learning, and graph network, arXiv:1806.01261, 2018\n",
        "tags": [
            "GNN",
            "ILP"
        ]
    },
    {
        "uri": "/posts/relational-network",
        "title": "A simple neural network module for relational reasoning",
        "content": "\nConsidering that most of the data is some form of graph, there has been lot of focus on improving neural networks to work with graph data. Amidst this, Santoro et al, paper focuses on neural network's ability of doing relational-reasoning i.e. manipulating structured representations of entities and relations. What separates this paper from the other graph network papers is two things: a) the graph or relation between entities is not provided rather learned and  b) the edges between entities can be of different types. Most graph network papers that learn edges focus on approximating a  distance metric between entities. This paper instead focuses on learning relations between entities.\n\nSummary\n\nThe paper proposes a computational block which they call relational network (RN), which takes a set of object ($O$) as input and outputs a vector. The main computational unit in RN are functions: $f\\phi$ and $g\\theta$, which are both Multi Layered Perceptrons. $g\\theta$  approximates the relation between each pair of object and $f\\phi$ performs the reasoning over these entities.  \n\n$$\n\\operatorname{RN}(O)=f\\{\\phi}\\left(\\sum\\{i, j} g\\{\\theta}\\left(o\\{i}, o\\_{j}\\right)\\right)\n$$\n\nTo achieve combinatorial generalization, i.e. be able to use $f\\phi$ over varying number of objects, authors use the sum of $g\\theta$ as input to $f\\phi$. So, the the input dimension of $f\\phi$ is equal to the output dimension of the $g_\\theta$, which are both constant and independent of order or number of objects in the input.\n\n RN can also be thought of as a MLP with parameter typing for first few hidden layers, which is equivalent to $g_\\theta$.\n\nMaking the input and output dimensions independent of the number of objects has a big advantage in terms of data efficiency. In standard MLP, when the number of objects increase, the input dimension may increase and hence the number of parameters would also increase.\n\nThe whole RN network is end-to-end differentiable and hence trainable by back propagation.\n\n RN for VQA\n\nAuthors show the utility of RN on Visual Question Answering task of CLEVR dataset. In CLEVR, a model needs to reason about relations between different objects in the image and then answer the question.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"400\"  src=\"/images/DeepRRL-CLEVR.png\"\n/div\n\n In the figure above, even though the question ''What is the size of the brown sphere?'' is shown as non-relational, if the answer of this question is going to be 'small', 'medium' or 'large', I would consider it as a relational question. Because the size is relative. On other hand, if the answer is '2 cm in diameter' it is non-relational. I strongly believe the dataset is aiming for the former answer.\n\nInput image is processed through a CNN to obtain object embedding. Input questions are processed though LSTM to obtain question embedding. The proposed $g\\theta$ function is then modified to predict the relationship between objects in context of the question asked: $g\\theta(oi, oj, q)$. $Here, q\\theta$ generates a fixed length vector which are aggregated and forwarded to $f\\phi$ which outputs softmax over all the possible answers.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"800\"  src=\"/images/RN-CLEVR-Arch.png\"\n/div\n\nRN's success in Sort-of-CLEVR dataset show that it is able to do better relational reasoning then MLP.\n\nCritique\n\nIn my opinion since the embedding of the relation i.e. output of $g_\\theta$ are not evaluated here, the claim of RN being able to do relational reasoning is not accurate. Experiments clearly show the benefit of the CNN+RN over CNN+MLP but that just means RN is better at fitting the curve.\n\nI do not quite understand what entails relational reasoning and how can one purely test that ability.\n\n References\n\nhttps://rasmusbergpalm.github.io/recurrent-relational-networks/\nhttps://medium.com/apache-mxnet/relation-networks-for-visual-question-answering-using-mxnet-gluon-f029fde8f863\nhttps://medium.com/intuitionmachine/intuitive-relational-reasoning-for-deep-learning-3ae164f9f5cd",
        "tags": [
            "VQA",
            "relational",
            "GNN"
        ]
    },
    {
        "uri": "/posts/RRL",
        "title": "Relational Reinforcement Learning",
        "content": "\n\nThe paper came before the goal-conditioned RL, Multi-task RL or Graph Neural Network literature. Major motivation of this paper is to learn a generalizable policy. Generalization in terms of varying number of objects in the domain (for example, in blocks-world number of blocks can change) or change in the goal state (for example, stack red block on blue block instead if green on yellow).\n\nAuthors demonstrate that by using approaches from inductive logic programming literature, first-order policy can be learnt which naturally supports both the generalization discussed above.\n\nIn particular, author propose to learn Q-Tree i.e. TILDE-RT (Top-down Induction of Logical decision trees for regression) as Q-Function which take the state and action pair and predict q values. The policy function, P-Tree, can then be induced from Q-Tree.\n\nFollowing Q-RRL algorithm is proposed to learn Q-Tree, which updates the TILDE-RT after every episode. Data set used to learn the TILDE-RT is generated by exploring the environment and P-RRL algorithm is proposed for inducing P-Tree from Q-Tree\n\ndiv align=\"center\"\nimg align=\"center\" width=\"800\"  src=\"/images/RRL-QRRL.png\"\n/div\n\ndiv align=\"center\"\nimg align=\"center\" width=\"800\"  src=\"/images/RRL-PRRL.png\"\n/div\n\n Critique\n\nProposed solution does not seem to scale well specifically because the Logical programs do not scale with higher number of  data points or high dimensional data.   \nThe relational trees might be able to generalize to various number of blocks but I think it will not generalize to different goals. For e.g. if all the training examples had goal(on(.,.)) and if the test examples have goal(clear(.)), I do not think the TILDE-RT will be able to achieve that goal.\nWith Graph Neural Network and goal-conditioned RL, both the generalizations  targeted by Q-RRL are achieved in a scalable manner. So, the only additional benefit RRL really has is the use of domain knowledge, which comes from ILP.",
        "tags": [
            "RL",
            "relational",
            "SRL"
        ]
    },
    {
        "uri": "/posts/skills-to-symbols",
        "title": "From Skills to Symbols:\u000bLearning Symbolic Representations for Abstract High-Level Planning",
        "content": "\nThis paper learns abstract symbolic representations from lower level trajectories for planning at a high-level. Big Idea of this paper is that given different domains of increasing difficulty at lower level but similar high level tasks, if we are able to segregate the low-level and high-level tasks, the tasks can be considered equivalent at higher level and hence can be solved in a uniform manner.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/S2S-bigidea.png\"\n/div\n\nFor example, given three tasks as shown above:\nA 5 x 5 grid world were agent has to take the key and open the lock or\nSame task for 10 x 10 grid or\nRobot running around the room trying to unlock a cupboard in continuous state and action space.\n\nIf we are given low level skills in each of the domain like navigating to a location and picking up the key or opening a lock, then high level plan in all these domains is equivalent. So, how to learn the abstractions which can help us plan at higher level? And ensure that the plan generated from such abstractions are executable with certainty?\n\nTo build a high level abstract planner, they use the options framework at the low-level. Operators in planning is equivalent to the options, precondition of an operator can be seen as initiation set and the effects of the operator can be seen as termination condition.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/S2S-operatoraction.png\"\n/div\n\n  Since operators in planning only impact subset of variables and leave others unchange, this can be seen as factored MDP.\n\nA plan is feasible if it can be executed and it is satisfiable if it is feasible and reaches goal state. To ensure plan is feasible and satisfiable, we need to learn precondition, effect and remainder for each abstract operator.\nPrecondition indicates the necessary conditions for taking action, effect indicates the changes in the state because of taking action, and remainder indicates variables which are unaffected by the action.\n\n I appreciate the details authors provide to prove the above intuition. May be I will add the proof here later.\n\nTo infer these three things, authors prove that learning following classifier for each abstract action is sufficient:\n$precondition(X,o)$ -- a classifier that indicates if option $o$ can be performed in the state $X$. This is equivalent to initiation set $I_o$.\n$effects(X,o)$ -- a classifier that indicates if state $X$ belongs to effect set of the option $o$\n$mask(o)$ --  a set of variables that are modified by option $o$\n\nFrom the above three functions, we can infer the remainder and image set of option $o$ as follows:\n\n Image $Im(X,o)$ – set of state obtained after executing option $o$ in state set $X$  \n$Remainder(X,o)$ – possible states with same values as $X$ for all variables except $mask(o)$\n\n$$ Remainder(X,o) = Project(X, mask(o))$$\n$$Im(X,o) = Effects(X,o) \\cap Remainder(X,o)$$\n\nThus we can learn PDDL operators directly from the trajectories and leverage the rich planning literature to perform high level planning in RL tasks.",
        "tags": [
            "planning",
            "abstractions",
            "RL"
        ]
    },
    {
        "uri": "/posts/SRL-for-myocardialinfarction",
        "title": "Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records",
        "content": "\n\nMyocardial Infarctions (generally known as heart attacks) causes one in three deaths in the United States and unsurprisingly have the most mysterious trajectory. It has been established that the prediction of future MIs is a challenging task and hence there have been extensive studies to identify and/or quantify the risk factors that contribute to MIs. Few common risk factors that have been identified are age, gender, blood pressure, low-density lipoprotein (LDL) cholesterol, diabetes, obesity, inactivity, alcohol and smoking. The canonical method of study in this field using: case-control studies, cohort studies, and randomized controlled trials concentrated on one risk factor at a time. So, the natural way forward is to analyze effects of multiple factors at a time and question is can we do it using machine learning? Also, Electronic Health Records (EHR) is used to over come the limitation of the data collected in previous studies.\n\nThe limitation of the previous data is that in these studies the risk factors are established at t0 and data is collected at the onset of the study, and then annual check-ups are conducted to assess patient health and determine occurrence of an MI event. The patients who did not possess risk factors at time t0 and developed them at later time were considered as not possessing that risk throughout the analysis. On the contrary, EHRs provides information of the development of risk factors as it tracks the health trajectories of its patients through time and hence provides an unique advantage for risk modelling. It can help us create a risk profile similar to Framingham Risk Score (FRS) without medical interventions (like additional laboratory tests). Also, as FRS works better for Caucasians than other populations it is biased. Hence, EHR risk profiles would be more reliable score than FRS.\n\nThis paper approaches the task of prediction and risk stratification of MI from EHRs using two Statistical Relational Learning (SRL) methods Relational Probability Trees (RPT) and Relational Functional Gradient Boosting (RFGB). RPTs upgrade decision trees to relational setting and RFGBs upgrade FGBs to relational setting. For RPT, paper uses Tilde relational regression learner (RRT) for positive examples to learn tree whose leaves have regression values which gives the probability of MI occurrence. Inner nodes in this tree represent conjunctions of literals (maximum two literals). FGBs fit the regression tree on training examples at each gradient step, for RFGBs propositional regression trees are replaced with relational regression trees.\n\nExperiments were performed on de-identified EHR data of 18,386 subjects. Total of 1,528 binary features were chosen a priori from relational tables for diagnoses, medications, labs, procedures, vitals, and demographics. This included major risk factors, common risk factors, drugs and patient relations. Also, features were discretized (e.g. for blood pressure, we created five binary features by mapping the real value to critically high, high, normal, low, and critically low). The training data was split in a way that 1:1 ratio is achieved for positive to negative examples. The paper  compares RFGB, RPT, Boosted Decision Trees, Decision Trees, Naive Bayes, Tree Augmented Naive Bayes, Support Vector Machines (SVM) with linear kernel, SVMs with radial basis function kernel and Random Forests. The results show that accuracies of all the algorithms are comparable. However, in the medical domain as it is more important to avoid false negative than false positive, better precision @ high recall is much more significant measure than accuracy. Experiments show that RFGB performs way better than other approaches for precision at  high recall.\n\nKey contribution\n\nThey approach the problem of predicting MIs in real patients and identifying ways in which machine learning can contribute to clinical studies.\nIt establishes that the each relational learner can out perform its propositional variants even for the large scale domains like EHRs and provide interpretable results.\nIt introduces task of MI prediction to SRL community.\n\n Limitations\n\nFeatures in this experiment were chosen a priori even for the algorithms that employ feature selection for computational reasons and to compare it with algorithms that do not employ feature selection.\nFeatures were discretized and not used in their natural form.\nRelational information such as hierarchies present in the EHR for diagnoses, drugs, and laboratory values are not considered in this experiments.\n\nCritique\n\nThe paper highlights a very important limitation of the data collected in clinical trials and signifies the importance of EHRs.\nThe interpretability achieved by these models is very important, especially in the medical domain.\n\nThe  paper  claims  that  its  key  contribution  is  that they address the challenging problem of predicting MI in real patients and identify ways in which machine learning can augment current methodologies in clinical studies.Though this is the first paper to use SRL methods to predict MI in real patients, it is not the first paper to use machine learning to predict/detect MI. ”Kernel-based Support Vector Machine classifiers for early detection of myocardial infarction” by D. Conforti, and R. Guido pre-dates this paper. Though it is unclear to me whether or not prediction and early detection are synonymous, I would have liked to see some mention of a previous ML works as a related work section.",
        "tags": [
            "SRL",
            "healthcare"
        ]
    },
    {
        "uri": "/posts/understanding-node-attention",
        "title": "Understanding Attention and Generalization in Graph Neural Networks",
        "content": " \nAttention in CNNs\n\na name=\"a1\"(Answer Q-1)/a\n\nAttention in CNNs is reweighting the feature map $X \\in \\mathbb{R}^{N \\times C}$, to provide attention to some nodes.\n\n$$\n\\begin{align}\nZ & =\\alpha \\odot X  \\quad  (Z\\{i}=\\alpha\\{i} X\\_{i}) \\\\\\\\\n\\text{such that,} \\quad \\quad &  \\sum\\{i}^{N} \\alpha\\{i} = 1 \\\\\\\\\n \\odot & \\text{ is element-wise multiplication}\n\\end{align}\n$$\n\n Note: $\\alphai$ is a scalar and $Xi$ is vector of size C. So, $Z_i$ is also a vector of size C. $Z \\in \\mathbb{R}^{N \\times C}$\n\n Pooling in CNNs\n\nPooling in CNNs divide the grid into local regions uniformly (not neighbors) and aggregate them to reduce the dimension.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/pooling.png\"\npsrc: a href=\"https://stackoverflow.com/questions/44287965/trying-to-confirm-average-pooling-is-equal-to-dropping-high-frequency-fourier-co\"stackoverflow/a/p\n/div\n\nSo, there is no parallelism between attention and pooling in the CCNs.\n\nBut in GNN, pooling also use the neighborhoods.\n\nTop K Pooling\n\ndiv align=\"center\"\nimg align=\"center\" width=\"300\"  src=\"/images/k-max.png\"\n/div\n\nTop K pooling was proposed by Gao and Ji, Graph U-Nets, ICML 2018, it is supposed to be a equivalent of k-max pooling (generalization of max-pooling) in the CNN where each feature map is reduced to size k by picking units with highest values. Since, in GNN the k-highest values can possibly come from different node for each feature-map the straight forward extention of k-max pooling does not work. So, Gao and Ji propose to project all the nodes to 1D and then select top K from that.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"700\"  src=\"/images/TopKPooling.png\"\n/div\n\nGiven feature matrix $X^{\\ell} \\in \\mathbb{R}^{N \\times C}$ and adjacency matrix $A^{\\ell} \\in \\mathbb{R}^{N \\times N}$, first project the feature matrix to 1D using projection vector $\\mathbf{p}$ and normalize it,  \n\n$$\n\\mathbf{y}=\\frac{X^{\\ell} \\mathbf{p}^{\\ell}}{\\left\\|\\mathbf{p}^{\\ell}\\right\\|}\n$$\n\nFrom this normalized 1D representation of each node ($\\mathbf{y}$), filter top K nodes and use indexes ( $\\mathrm{idx}=\\operatorname{rank}(\\mathbf{y}, k)$) to retrieve relevant feature matrix and adjacency matrix.\n\n$$\n\\begin{aligned} \\tilde{X}^{\\ell} &=X^{\\ell}(\\text { idx}, :) \\\\\\\\A^{\\ell+1} &=A^{\\ell}(\\mathrm{idx}, \\mathrm{idx}) \\end{aligned}\n$$\n\nHowever, since the $\\mathbf{y}$ is discrete valued, authors use gate operation ($\\operatorname{sigmoid}$) to convert $\\mathbf{y}$ to real value and make it eligible for back-propagation $(\\tilde{\\mathbf{y}} = \\operatorname{sigmoid}(\\mathbf{y}(\\mathrm{idx}))$. The final feature matrix for the next layer is obtained by element wise multiplication of feature vectors of selected nodes and  $\\tilde{\\mathbf{y}}$,\n\na name=\"a2\"(Answer Q-2)/a\n\nOver all,\n\n$$\n\\begin{align}\nX^{\\ell+1} & =\\tilde{X}^{\\ell} \\odot\\left( \\tilde{\\mathbf{y}}\\mathbf{1}\\_{C}^{T}\\right) \\\\\\\\\n\\text{with} \\quad \\tilde{\\mathbf{y}}  & = \\operatorname{sigmoid}(\\frac{X^{\\ell}  \\mathbf{p}^{\\ell}}{\\left\\|\\mathbf{p}^{\\ell}\\right\\|}(\\mathrm{idx}))\n\\end{align}\n$$\n\nIn GNNs, there is a parallelism between pooling and attention. Node attention $\\alpha$ can be thought of as $\\mathbf{\\tilde{y}}$.\n\n$$\nZ\\{i}=\\left\\\\{\\begin{array}{ll}{\\alpha\\{i} X\\_{i},} & {\\forall i \\in P} \\\\\\\\{\\emptyset,} & {\\text { otherwise }}\\end{array}\\right.\n$$\n\nwhere, $P = \\\\{\\text{idx}\\\\}$  and $|P| = k$. $P$ is obtained by finding the indices of top-k values of $\\mathbf{y}$, which is computed by learning projection vector $\\mathbf{p}$ using back-propagation on input graph.  \n\nThis paper proposes to combine the attention and pooling to a single computational block, which does not have a fixed $k$. Instead, set $P$ is determined by threshold $\\tilde{\\alpha}$:\n\n$$\nZ\\{i}=\\left\\\\{\\begin{array}{ll}\\alpha\\{i} X\\{i}, & \\forall i: \\alpha\\{i}\\tilde{\\alpha} \\\\\\\\\\emptyset, & \\text { otherwise }\\end{array}\\right.\n$$\n\nFurther, they also propose a combination of GIN and ChebyNet called ChebyGIN to be used for convolution after pooling.\n\n ChebyGIN\n\nGraph Convolutional Network (GCN), Graph Isomorphism Network (GIN), ChebyNet have similar formulation with minor changes. The proposed ChebyGIN formulation is an extention of these changes. This section highlights the equivalence and differences in the mathematical forms of these networks. We compare the Convolution layer of these networks, each take input $H^{(\\ell)}$ (equivalently $hi^{(\\ell)}, \\forall i \\in V$) and outputs $H^{{\\ell+1}}$ (equivalently $hi^{(\\ell+1)}$). And, $\\mathcal{N}(i)$ indicates neighborhood of the node $i$.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"350\"  src=\"/images/layer.png\"\n/div\n\nGCN:\n\n$$\n\\begin{align}\n H^{(\\ell+1)} &= \\sigma\\left(\\hat{A} H^{(\\ell)}\\mathbf{W}^{(\\ell)}\\right)\n\\\\\\\\\n\\mathbf{h}\\{i}^{(\\ell+1)} & = \\sigma\\left(  \\frac{1}{\\mathcal{c}\\{ii}} \\mathbf{h}\\{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} + \\sum\\{j \\in \\mathcal{N}(i)} \\frac{1}{\\mathcal{c}\\{ij}} \\mathbf{h}\\{j}^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right) \\\\\\\\\n\\text{with, } c\\{ij} = \\sqrt{di, dj} & \\text{ and }  di = |\\mathcal{N}(i)|\n\\end{align}\n$$\n\nGIN:  \nReplaces $\\sigma$ with multi-layer perceptron (MLP) and since, the MLP has weights and does rescaling from $f^i$ to $f^o$, we do not need $\\mathbf{W}^{(\\ell)}$ and the normalization $(\\frac{1}{\\mathcal{c}\\_{ij}})$.\n\n$$\n\\mathbf{h}\\{i}^{(\\ell+1)} = \\operatorname{MLP}\\left( \\mathbf{h}\\{i}^{(\\ell)}(1 + \\varepsilon^{(\\ell)}) + \\sum\\{j \\in \\mathcal{N}(i)} \\mathbf{h}\\{j}^{(\\ell)}\\right) \\\\\\\\\n$$\n\nHere, when $\\varepsilon  0$ the current node is given more importance, when $\\varepsilon = 0$ the current node has same importance.\n\nChebyNet:  \nGeneralization of GCN to $k^{th}$ order approximation of Chebyshev polynomial.\n\n$$\n\\mathbf{h}\\{i}^{(\\ell+1)} = \\sigma\\left(  \\sum\\{k =1}^{K-1} \\left( \\left( \\frac{1}{\\mathcal{c}\\{ii}^{k}} \\mathbf{h}\\{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) + \\thetak \\sum\\{j \\in \\mathcal{N}\\{k}(i)} \\frac{1}{\\mathcal{c}\\{ij}^{k}} \\mathbf{h}\\_{j}^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right) \\right)\\\\\\\\\n$$\n\nChebyGIN:  \n\na name=\"a3\"(Answer Q-3)/a\n\nReplaces $\\sigma$, $\\mathbf{W}$ and $\\frac{1}{\\mathcal{c}\\_{ij}^{k}}$ of ChebyNet, same as GCN.\n\n$$\n\\mathbf{h}\\{i}^{(\\ell+1)} = \\underbrace{\\operatorname{MLP}}\\{\\text{FC Layer}}\\left( \\underbrace{ \\sum\\{k =1}^{K-1} \\mathbf{h}\\{i}^{(\\ell)}di(1 + \\varepsilon^{(\\ell)}) + \\thetak \\sum\\{j \\in \\mathcal{N}\\{k}(i)} \\mathbf{h}\\{j}^{(\\ell)} dj}\\_{\\text{GNN Layer}} \\right) \\\\\\\\\n$$\n\n$\\thetak$ is still multiplied at the neighborhood level to obtain different weights for each neighborhood. **All the node feature vectors ($hi$) are multiplied by node degree ($d_i$) for first layer**.\n\nProposed architecture\n\ndiv align=\"center\"\nimg align=\"center\" width=\"350\"  src=\"/images/ChebyGIN.png\"\npsrc: a href=\"https://github.com/bknyaz/graphattentionpool\"Boris Knyazev's slides/a/p\n/div\n\nThe proposed architecture is as follows: first layer is a attention/pooling of input graph, second layer is GNN which aggregates features from local neighborhoods, and third layer is a fully connected (FC) layer, which can also do global pooling and finally an output layer which will be used for training. A separate fully connected MLP called attention network is trained to obtain attention values on each node.\n\n Attention Network\n\nFor supervised learning of the attention network, the ground truth of attention values for each node ($\\alpha_i^{GT} \\in [0,1]$) in the graph is obtained by heuristic.\n\nFor example, in experimental dataset for graph color count, attention on each node is defined as follows:\n\n$$\n\\alpha\\{i}^{GT}=\\left\\\\{\\begin{array}{ll}{\\frac{1}{|N\\{green}|},} & {\\forall i \\in N\\_{green}} \\\\\\\\{0,} & {\\text { otherwise }}\\end{array}\\right.\n$$\n\nIn experimental dataset of graph triangle count, following heuristic is used;\n\n$$\n\\alpha\\{i}^{GT}=\\left\\\\{\\begin{array}{ll}{\\frac{T\\{i} }{\\sum\\{i} T\\{i}},} & {\\forall i \\in \\text{Triangle } } \\\\\\\\{0,} & {\\text { otherwise }}\\end{array}\\right. \\\\\\\\\n\\text{with, } T_i = \\text{number of triangles that include node } i\n$$\n\nFor MNIST-$75SP$ dataset where each node is a superpixel and edges are formed based on spatial distance between superpixel centers, following heuristic was used:\n\n$$\n\\alpha\\{i}^{GT}=\\left\\\\{\\begin{array}{ll}{\\frac{1}{N\\{nonzero}},} & {\\forall i \\in \\text{Nonzero intensity superpixel} }\\\\\\\\{0,} & {\\text { otherwise }}\\end{array}\\right. \\\\\\\\\n\\text{with, } N\\_{nonzero} = \\text{number of such pixels } i\n$$\n\nTraining\n\nThese networks are trained using back-propagation to minimize the Mean-Squared Error (MSE) loss or the Cross-Entropy loss (CE) of the over all prediction and minimize the Kullback-Leibler (KL) divergence loss between ground truth attention $\\alpha^{GT}$ and predicted coefficients\u000b$\\alpha$. The KL term is weighted by scale $\\beta$ and number of nodes $N$.\n\n$$\n\\mathcal{L}=\\mathcal{L}\\{M S E / C E}+\\frac{\\beta}{N} \\sum\\{i} \\alpha\\{i}^{G T} \\log \\left(\\frac{\\alpha\\{i}^{G T}}{\\alpha\\_{i}}\\right)\n$$\n\na name=\"a4\"(Answer Q-4)/a\n\nSince $\\sumi \\alphai = 0$, $\\alpha$ can be thought of as a probability distribution of attention over all the nodes and so, minimizing the KL-divergence is an obvious first choice. Below equation shows relationship between cross-entropy, entropy and KL Divergence. a name=\"a5\"(Answer Q-5)/a\n\n$$\nH(p, q)=H(p)+D\\_{\\mathrm{KL}}(p | q)\n$$\n\n Weakly supervised model\n\nFor domains where the ground truth of attention is hard to obtain for each node, authors propose a weakly supervised learning setting as follows. Train an attention network (model B), which has same structure as the proposed architecture (model A) except for attention/pooling layer. Model B is trained to reduce the $\\mathcal{L}\\_{MSE}$ for $y$ prediction. Then, the $\\alpha^{WS}$ is calculated using the trained model and input graph $G$.\n\n$$\n\\begin{align}\n{\\alpha}\\{i}^{W S} & =\\frac{\\left|y\\{i}-y\\right|}{\\sum\\{j=1}^{N}\\left|y\\{j}-y\\right|} \\\\\\\\\n\\text{with } y & = \\operatorname{ModelB}(G) \\\\\\\\\n\\text{ and } yi & =  \\operatorname{ModelB}(G - \\\\{Ni\\\\}) \\\\\\\\\n\\end{align}\n$$\n\nNext, the proposed architecture -- Model A is trained using $\\alpha^{WS}$ to optimize both the MSE and KL divergence.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"500\"  src=\"/images/ModelB.png\"\npsrc: a href=\"https://github.com/bknyaz/graphattentionpool\"Boris Knyazev's slides/a/p\n/div\n\na name=\"a6\"(Answer Q-6)/a\n\nFor colors domain, authors use 2 layers of GNN. So mathematical form for model B is:\n\n$$\nY = \\operatorname{GNN}(\\underbrace{W^{1}}\\{64 \\times 1},\\operatorname{GNN}(\\underbrace{W^{0}}\\{64 \\times F^i}, \\underbrace{H}\\_{N \\times F^{i}} ) )\n$$\nwhere as mathematical form of model A is:\n\n$$\nY = \\operatorname{ChebyGIN}(\\underbrace{W^{1}}\\{64 \\times 1},\\operatorname{ChebyGIN}(\\underbrace{W^{0}}\\{64 \\times F^i}, \\alpha^{WS}X) )\n$$\n\nwhere $\\alpha^{WS}$ is as defined above, obtained from model B.\n\nAnalysis\n\na name=\"a7\"How powerful is attention over nodes in GNNs?/a\n\nContrary to what the authors mention in the paper, I feel that the experimental results show that there is not a lot of co-relation between attention and model accuracy. The example result below shows that the even though the proposed model has high co-relation with attention AUC, there are other models which do not show better performance even when the attention AUC is high. This observation is also backed by the paper, Jain et al. NAACL 2019 Attention is not Explanation.\n\ndiv align=\"center\"\nimg align=\"center\" width=\"350\"  src=\"/images/attentionAUC.png\"\npsrc:Knyazev et. al 2019 [Fig 3a]/p\n/div\n\nSo the power of attention over nodes is I think need more study.\n\na name=\"a8\"What are the factors influencing performance of GNNs with attention?/a\n\nFrom experiment results it seems following factors influence the GNNs with attention:\n\n initialization vector -- optimal initialization has better accuracy in Fig-4(c)\n quality of the attention -- Supervised attention has better results than weakly supervised attention.\n strength of GNN model used -- ChebyGIN model has better results than GIN/GCN\n\na name=\"a9\"Why is the variance of some results so high?/a\n\nVariance of some results is high because the model is very sensitive to the initialization parameters. It is only able to recover from bad initialization of hyper-parameters when the attention is good. Bad initialization of attention was not recoverable.\n\na name=\"a10\"How top-k compares to our threshold-based pooling method?/a\n\nExperiments show that threshold-based pooling has better results than top-k pooling for larger datasets (with high features).\n\na name=\"a11\"How results change with increase of attention model input dimensionality or capacity?/a\n\nWith increase in the input dimension for the attention model, the distribution of $\\alpha$ values become flat ($\\because \\sumi\\alphai = 1$). Experiments show that in such cases, deeper GNN model for attention are useful.\n\na name=\"a12\"Can we improve initialization of attention?/a\n\nAuthors observe for unsupervised attention models, normal or uniform distribution with high values is preferred for the initialization of parameters of attention model. But for supervised or weakly supervised model smaller values are preferred. There is no intuition on why one is preferred over the other, paper just states the observation based on empirical evaluations.\n\na name=\"a13\"What is the recipe for more powerful attention GNNs?/a\n\nRecipe for powerful attention is to get supervision for attention. If supervision is not possible, use the weakly-supervised method for attention.\n\na name=\"a14\"How results differ depending on to which layer we apply the attention model?/a\n\nAlthough it is desirable to use attention model closer to the input layer to reduce graph size and keep the attention weights interpretable, the experiments show that the attention on deeper layer have higher impact on the performance.\n\na name=\"a15\"Why is initialization of attention important?/a\n\nSince the final model is trained by considering the $\\alpha$ -- attention weights as final, when the attention those weights have bad initialization, the weights learnt in rest of the model are wrong and hence the model is not able to recover.\n\nHowever, I feel that the models should be able to recover from the bad initialization with more iterations. Literature of expectation-maximization and bi-level optimization indicates that this is possible.\n\n Doubts\n\nWhy use sigmoid in Top-K Pooling? Gate operation -- why is projection discrete ??\n\nQuestions\n\nWhat is the dimensionality of $Z\\_{i}$ ? Link to Answer\nHow to decide $P$ from input graph?  Link to Answer\nProvide mathematical form of ChebyGIN and show all the parameters  Link to Answer\nWhy is $KL$ selected as the loss function, but not cross entropy and squared error? Link to Answer\nRelation between Cross entropy and KL Divergence. Link to Answer\nGive mathematical forms of model A and B for Colors. Link to Answer\nSummarize: How powerful is attention over nodes in GNNs? Link to Answer\nSummarize: What are the factors influencing performance of GNNs with attention?  Link to Answer\nSummarize: Why is the variance of some results so high?  Link to Answer\nSummarize: How top-k compares to our threshold-based pooling method? Link to Answer\nSummarize: How results change with increase of attention model input dimensionality or capacity? Link to Answer\nSummarize: Can we improve initialization of attention? Link to Answer\nSummarize: What is the recipe for more powerful attention GNNs? Link to Answer\nSummarize: How results differ depending on to which layer we apply the attention model? Link to Answer\nSummarize: Why is initialization of attention important? Link to Answer\n\n Extra questions to be considered\n\nFind the source code related to Weakly supervised attention component and explain each line in the related source code\nWhy GIN moves from weighted mean to the sum?\nHow to do back-propagation with ranking?\nDoesn't attention lead to overfitting ?? Higher number of parameters mean high chance of overfitting.\n\nReferences\n\nBoris Knyazev's slides\nGao and Ji, Graph U-Nets, ICML 2018",
        "tags": [
            "GNN",
            "course"
        ]
    },
    {
        "uri": "/projects",
        "content": "---\ntitle: \"Selected Projects\"\nlinktitle: projects\nmenu:\n  main:\n    weight: -300\nlayout: blank\ndescription: \"These are selected projects of Harsha Kokel.\"\n---\n\n  {{ projects}}\n  \n{{leisure-project}}\nbr\n/br",
        "tags": []
    }
]