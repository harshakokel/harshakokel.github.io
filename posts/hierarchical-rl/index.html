<!DOCTYPE HTML>

<html>
  <head>
    <title>Hierarchical Reinforcement Learning</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hierarchical Reinforcement Learning"/>
<meta name="twitter:description" content="An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan."/>

    <meta property="og:title" content="Hierarchical Reinforcement Learning" />
<meta property="og:description" content="An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/hierarchical-rl/" />
<meta property="article:published_time" content="2019-10-28T16:40:08+02:00" />
<meta property="article:modified_time" content="2019-10-28T16:40:08+02:00" /><meta property="og:site_name" content="Harsha Kokel" />

    <meta itemprop="name" content="Hierarchical Reinforcement Learning">
<meta itemprop="description" content="An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan.">
<meta itemprop="datePublished" content="2019-10-28T16:40:08+02:00" />
<meta itemprop="dateModified" content="2019-10-28T16:40:08+02:00" />
<meta itemprop="wordCount" content="720">



<meta itemprop="keywords" content="RL,hierarchy,coursework," />

    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="font-family: 'Dawning of a New Day', cursive;font-size:2em;color: var(--main-highlight-color);font-weight:500">harsha kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters: [
  {left: '$$', right: '$$', display: true},
  {left: '$', right: '$', display: false},
  {left: '\\(', right: '\\)', display: false},
  {left: '\\[', right: '\\]', display: true}
]});"></script>




	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Oct 28, 2019
  </li> 
  <li class="tags crumb"> 
      <a href="https://harshakokel.com/tags/rl/" class="tag__link">rl</a>  <a href="https://harshakokel.com/tags/hierarchy/" class="tag__link">hierarchy</a>  <a href="https://harshakokel.com/tags/coursework/" class="tag__link">coursework</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Hierarchical Reinforcement Learning</h2>
  </header>

  
  
  
 
  
  
  
    <blockquote class="preview">An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan. </blockquote>
  
  
  <p><p>Standard RL planning suffers from the curse of dimensionality when the action space is too large and/or state space is infeasible to enumerate. Humans simplify the problem of planning in such complex conditions by abstracting away details which are not relevant at a given time and decomposing actions into hierarchies. Several researchers have proposed to model the <strong>temporal-abstraction</strong> in RL by composing some form of hierarchy over actions space (<a href="#Dietterich-1998%22">Dietterich 1998</a>, <a href="#sutton-1998%22">Sutton et al 1998</a>, <a href="#parr-1998%22">Parr and Russell 1998</a>). By modeling actions as hierarchies, researchers extended the primitive action space by adding abstract actions. Options framework (<a href="#sutton-1998%22">Sutton et al 1998</a>), refer the abstract actions as <em>options</em>, MAXQ (<a href="#Dietterich-1998%22">Dietterich 1998</a>) refer to them as <em>tasks</em> and Hierarchical Abstract Machines (HAM) (<a href="#parr-1998%22">Parr and Russell 1998</a>) refers to them as <em>choices</em>.</p>
<p>Common theme among these papers is to extend the Markov Decision Process (MDP) to <em>Semi-Markov Decision Process</em> (SMDP), where actions can take multiple time steps. As compared to MDP, which only allow actions of a discrete time-steps, SMDP allows modeling temporally abstract actions of varying length over a continuous time. As represented in first two trajectories of figure below. By constraining/extending the action space of the MDP over primitive and abstract actions, hierarchical RL approaches superimpose MDPs and SMDPs as shown in last trajectory.</p>
<div align="center">
<img align="center" width="400"  src="/images/HRL-SMDP.png">
<p>Semi-Markov Decision Process</p>
</div>
<!-- % Here, we mainly focus on the three hierarchical RL papers:
% \begin{enumerate}
%     \item Options framework {[Sutton et al 1998](#sutton-1998")}
%     \item MAXQ {Dietterich 1998}
%     \item Hierarchical Abstract Machines (HAM) {[Parr and Russell 1998](#parr-1998")}
% \end{enumerate} -->
<p>HRL is appealing because the abstraction of actions facilitate <strong>accelerated-learning</strong> and <strong>generalization</strong> while exploiting the structure of the domain.</p>
<p>Faster learning is possible because of the <strong>compact-representation</strong>. Original MDP is broken into sub-MDP with less states (abstracted states hide irrelevant details and hence reduce the number of states) and less actions. For example, in the Taxi Domain introduced in (<a href="#Dietterich-1998%22">Dietterich 1998</a>), if the agent is learning to navigate to a location it does not matter if the passenger is being picked or dropped. Details about location of passenger are irrelevant and hence the state space is reduced.</p>
<p>Better generalization is possible because of the <strong>abstracted actions</strong>. In the taxi domain, because we define an abstract action called $Navigation$, agent learns a policy to navigate the taxi to a location. Once that policy is learned for navigation to pick up a passenger, the same policy can be leveraged when then agent is navigating to drop the passenger.</p>
<p>Two important promises of HRL are <strong>prior-knowledge</strong> and <strong>transfer-learning</strong>. A complex task in HRL is decomposed into hierarchy (usually by humans). Hence, it is easier for humans to provide some prior on actions from their domain knowledge. Different levels of hierarchy encompass different knowledge and hence ideally it would be easier to transfer that knowledge across different problems.</p>
<p>One minor limitation of HRL is that all the hierarchical methods converge to <em>hierarchically optimal policy</em>, which can be a sub-optimal policy. For example in the taxi domain, if the hierarchy decomposition states first navigate to the passenger location and then navigate to the fuel location, the HRL agent will find an optimal policy to do that in exactly that order. This policy might be sub-optimal given an initial state which is closer to the fuel location. This limitation is an artifact of restricting the action space while solving sub-MDPs. If full action space is available in all the MDPs, the exponential increase in computational overhead makes the learning infeasible.</p>
<p>Max-Q framework has a clear hierarchical decomposition of tasks, while the options-framework do not have clear hierarchy. Options framework achieves temporal abstraction of actions, Max-Q framework additionally also achieves <strong>state abstractions</strong>. While there has been an attempt on discovering and transferring the Max-Q hierarchies (<a href="#mehta-2008">Mehta et al. 2008</a>), learning Max-Q hierarchies directly from the trajectories is still an open problem. For large and complex problem it might be a challenge to provide the task hierarchy or options and their termination conditions.</p>
<h3 id="references">References</h3>
<ul>
<li><a name="Dietterich-1998">[Dietterich 1998]</a>  Dietterich, T. G.  1998.  The maxq methodfor hierarchical reinforcement learning. In <em>ICML</em>.</li>
<li><a name="sutton-1998">[Sutton, Precup, and Singh 1998]</a>  Sutton, R. S.; Precup, D.;and Singh, S. P.  1998.  Intra-option learning about tempo-rally abstract actions. In <em>ICML</em>.</li>
<li><a name="parr-1998">[Parr and Russell 1998]</a>   Parr,  R.,  and  Russell,  S.  J. 1998. Reinforcement  learning  with  hierarchies  of  machines.   In <em>NeurIPS</em></li>
<li><a name="mehta-2008">[Mehta et al. 2008]</a>  Mehta, N.; Ray, S.; Tadepalli, P.; and Di-etterich, T. 2008. Automatic discovery and transfer of maxq hierarchies. In <em>ICML</em>.</li>
<li>The Promise of Hierarchical Reinforcement Learning by Yannis Flet-Berlia in <a href="https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/" target="_blank">The Gradient</a></li>
<li>Hierarchical Reinforcement Learning lecture by Doina Precup on <a href="https://www.youtube.com/watch?v=e8b0yC6COJ8" target="_blank">YouTube</a></li>
</ul>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=6
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/active-advice-seeking/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Active Advice Seeking for Inverse Reinforcement Learning</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/batch-rl/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Fitted Q and Batch Reinforcement Learning</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
</section>




<script type="text/javascript" src="/js/lunr.js?1617569109"></script>
<script type="text/javascript" src="/js/auto-complete.js?1617569109"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1617569109"></script>

    
<nav id="menu">
  <h2>hk</h2>
  
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>

    
    
    <section class="nav">
									<div class="mini-posts">
										<article>
                      <span class="mini-post-misc"><a href="/misc">miscellaneous</a></span>
											<a href="/misc" class="image"><img src="/images/Smile.png" alt=""></a>
										</article>
                    </div>
   </section>
  
    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
