<!DOCTYPE HTML>

<html>
  <head>
    <title>Few-Shot Learning with GNN</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Few-Shot Learning with GNN"/>
<meta name="twitter:description" content="This paper focuses on $q$-shot $K$-way classification problem &ndash; where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels."/>

    <meta property="og:title" content="Few-Shot Learning with GNN" />
<meta property="og:description" content="This paper focuses on $q$-shot $K$-way classification problem &ndash; where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/few-shot-learning-gnn/" />
<meta property="article:published_time" content="2020-04-04T16:40:08+02:00" />
<meta property="article:modified_time" content="2020-04-04T16:40:08+02:00" /><meta property="og:site_name" content="Harsha Kokel" />

    <meta itemprop="name" content="Few-Shot Learning with GNN">
<meta itemprop="description" content="This paper focuses on $q$-shot $K$-way classification problem &ndash; where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels.">
<meta itemprop="datePublished" content="2020-04-04T16:40:08+02:00" />
<meta itemprop="dateModified" content="2020-04-04T16:40:08+02:00" />
<meta itemprop="wordCount" content="1316">



<meta itemprop="keywords" content="few-shot,GNN,course," />

    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="color:inherit">Harsha Kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax '})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Apr 4, 2020
  </li> 
  <li class="tags">  Tagged with:
      <a href="https://harshakokel.com/tags/few-shot/" class="button">few-shot</a>  <a href="https://harshakokel.com/tags/gnn/" class="button">gnn</a>  <a href="https://harshakokel.com/tags/course/" class="button">course</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Few-Shot Learning with GNN</h2>
  </header>

  
  

  <p><strong></strong></p>
  
    <blockquote>My notes on <strong>Victor Garcia, Joan Bruna, ICLR 2018</strong></blockquote>
  
  <p><p>This paper focuses on $q$-shot $K$-way classification problem &ndash; where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels.</p>
<p>Given dataset $\{\left(\mathcal{T}_{i}, y_{i}\right)\}_{i\leq L}$ containing Task $\mathcal{T}_i$ and true label $y_{i}  \in\{1, K\}$ for a single test image $\bar{x}$.</p>
<p>$$
\mathcal{T}=\left\{ \underbrace{ \left\{\left(x_{1}, l_{1}\right), \ldots\left(x_{s}, l_{s}\right)\right\}}_{\text{supervised samples}}, \underbrace{\left\{\bar{x}\right\}}_{\text{test samples}} ; \right\}
$$</p>
<p>Embeddings$-\phi(.)$ are obtained from a convolutional neural network (CNN) for all the images ($\{x_i\}_1^s \cup \bar{x}$),  as highlighted in the image below.</p>
<div align="center">
<img align="center" width="500"  src="/images/kshotGNN-GNN_full.png">
</div>
<p>These embeddings are then concatenated with one-hot encoding of the labels $(h(l))$. Together they form <strong>nodes</strong> $(V)$ of the graph.</p>
<p>$\mathbf{x}_{i}^{(0)}=\left(\phi\left(x_{i}\right), h\left(l_{i}\right)\right)$</p>
<blockquote>
<p><a name="a4"><i class="fa fa-bolt"></i></a> For $k$ labels, $h(l)$ is a binary vector of size k. One-hot encoding is obtained for training image $x_i$ with the label $l_i=3$, by setting all the values in $h(l_i)$ to $0$ except for $3^{rd}$ element which is $1$ i.e. $[0,0,1,0,0&hellip;]$. For test image, since the label is not known in $\mathcal{T}$, uniform distribution is used instead of one hot encoding i.e. $h(l) = k^{-1}\mathbf{1}_k$, for $\bar{x}$.</p>
</blockquote>
<blockquote>
<p>superscript $^{(p)}$ indicate the $(p+1)^{th}$ layer input. So, $x^{(0)}$ indicates the node embeddings for first layer in GNN.</p>
</blockquote>
<p><strong>Edges</strong> $(E)$ of the graph are learnt as adjacency matrix $\tilde{A}$ using a MLP.</p>
<p>$$
\tilde{A}_{i,j} = \varphi_{\tilde{\theta}}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)= \operatorname{MLP}_{\tilde{\theta}}\bigg( \operatorname{abs}\Big( \phi(x_i) - \phi(x_j) \Big) \bigg)
$$</p>
<p>Then Graph Convolution ($Gc(.)$) is performed on the nodes $V$ and adjacency matrix $A$. <a name="a10"><i class="fa fa-bolt"></i></a>.</p>
<p>$$
\mathbf{x}_{l}^{(k+1)}=\operatorname{Gc}\left(\mathbf{x}^{(k)}\right)=\rho\left(\sum_{B \in \mathcal{A}} B \mathbf{x}^{(k)} \theta_{B, l}^{(k)}\right) \<br>
\mathcal{A} = \{\tilde{A}^{(k)}, \mathbf{1}\}
$$</p>
<div align="center">
<img align="center" width="600"  src="/images/kshotGNN-blocks.png">
<p align="center">Proposed block of GNN containing the Adjacency matrix and Graph Convolution.</p>
</div>
<p>The block containing learning of adjacency matrix and graph convolution is repeated multiple times in the experiments and output of the last layer is passed through a sigmoid activation function to obtain class probabilities  <a name="a8"><i class="fa fa-bolt"></i></a>.</p>
<div align="center">
<img align="center" width="600"  src="/images/kshotGNN-GNN.png">
</div>
<p>The complete network, including the adjacency matrix is trained end-to-end with following objective <a name="a3"><i class="fa fa-bolt"></i></a>.</p>
<p>$$
\min _{\Theta} \frac{1}{L} \sum_{i \leq L} \underbrace{\ell\left(\Phi\left(\mathcal{T}_{i} ; \Theta\right), Y_{i}\right)}_{\text{loss w.r.t. test samples}}+\underbrace{\mathcal{R}(\Theta)}_{\text{regularizer}} \<br>
\ell(\Phi(\mathcal{T} ; \Theta), Y)= \underbrace{-\sum_{k} y_{k} \log P\left(Y=y_{k} | \mathcal{T}\right)}_{\text{cross entropy loss}}
$$</p>
<blockquote>
<p>Note that the labels/predictions of the training data are not used to compute the loss. Hence the GNN will not overfit to the $h(l)$ part of the embedding.</p>
</blockquote>
<p>The over all idea of <strong>finding image embeddings and then using distance between the image embeddings to find the class label</strong> is explored in various other papers. Authors show equivalence of their approach to three other papers.</p>
<h3 id="1-convolutional-siamese-neural-network">1. <strong>Convolutional Siamese Neural Network</strong></h3>
<p>Koch et al. 2015 proposed to use Convolutional Siamese Neural Network (CSNN) to learn similarity between two images, i.e probability that both images were drawn from same label set.</p>
<p>They used two parallel twin CNN to obtain embeddings to two input images $f_{\theta}(x)$, computed absolute distance between these embeddings (d), and then computed probability by passing the distance from linear feedforward layer and sigmoid function.</p>
<div align="center">
<img align="center" width="400"  src="/images/kshotGNN-siamese.png">
</div>
<p>CSNN are trained to reduce the following loss function for all pairs of images.</p>
<p>$$
\mathcal{L}(B)=\sum_{\left(\mathbf{x}_{i}, \mathbf{x}_{j}, y_{i}, y_{j}\right) \in B} \mathbf{1}_{y_{i}=y_{j}} \log p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)+\left(1-\mathbf{1}_{y_{i}=y_{j}}\right) \log \left(1-p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)\right)
$$</p>
<p>While testing, the test image ($\mathbf{x}$) is compared with all the training images ($S$) and 1 Nearest-Neighbour approach is used to assign the class label.</p>
<p>$$
\hat{c}_{S}(\mathbf{x})=c\left(\arg \max _{\mathbf{x}_{i} \in S} P\left(\mathbf{x}, \mathbf{x}_{i}\right)\right)
$$</p>
<p><strong>Equivalence to GNN</strong></p>
<p>GNN approach is similar to CSNN if we make following changes:</p>
<ol>
<li>Restrict node embeddings:</li>
</ol>
<p>$$\operatorname{GNN}:  \mathbf{x}_{i}^{(0)}=\left(\phi\left(x_{i}\right), h\left(l_{i}\right)\right)$$
$$\operatorname{CSNN}: \mathbf{x}_{i}^{(0)}=\phi\left(x_{i}\right) = f_{\theta}(x_i)$$</p>
<ol start="2">
<li>Fix Adjacency matrix:</li>
</ol>
<p>$$\operatorname{GNN}:  \tilde{A}_{i,j} = \operatorname{MLP}_{\tilde{\theta}}\bigg( \operatorname{abs}\Big( \phi(x_i) - \phi(x_j) \Big) \bigg)$$
$$\operatorname{CSNN}: \tilde{A}_{i,j} = \operatorname{softmax}\bigg( - || \phi(x_i) - \phi(x_j) || \bigg)$$</p>
<ol start="3">
<li>Reduce the convolution block:</li>
</ol>
<p>$$\operatorname{GNN}: y = \sigma \Big( \sum_{B \in A} B \mathbf{x}^{(k)} \theta_{B,l}^{(k)} \Big)$$
$$\operatorname{CSNN}: Y_{\ast} = \sum_{j} \tilde{A}_{\ast, j}^{(0)}\left\langle\mathbf{x}_{j}^{(0)},  u\right\rangle$$</p>
<p>where, $\langle .,. \rangle$ indicates elementwise multiplication and  $u$ is a binary vector for selection of labels. So, it is 1 only for elements which correspond to labels in $\mathbf{x}$ and 0 otherwise. $\{\ast\}$ indicates the test example. <a name="a5"><i class="fa fa-bolt"></i></a>.</p>
<h3 id="2-matching-network">2. <strong>Matching Network</strong></h3>
<p>Vinyals et al., 2016 extended the SCNN approach of $2$-way comparison to $k$-way comparison by using an attention function $a(.,.)$ to compute cosine similarity of test image embedding $f_{\theta}(\mathbf{x})$ with  input image embeddings $g_{\theta}(\mathbf{x_i})$ and use it as a weighted sum to compute the class probability of the test image.</p>
<div align="center">
<img align="center" width="400"  src="/images/kshotGNN-matchingNetwork.png">
</div>
<p>$$
\begin{array}{l}c_{S}(\mathbf{x})=P(y | \mathbf{x}, S)=\sum_{i=1}^{k} a\left(\mathbf{x}, \mathbf{x}_{i}\right) y_{i}, \text { where } S=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}_{i=1}^{k} \ a\left(\mathbf{x}, \mathbf{x}_{i}\right)=\frac{\exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{i}\right)\right)\right.}{\sum_{j=1}^{k} \exp \left(\operatorname{cosine}\left(f(\mathbf{x}), g\left(\mathbf{x}_{j}\right)\right)\right.} \<br>
\end{array}
$$</p>
<blockquote>
<p>Note here $k$ is the number of examples not the number of class labels.</p>
</blockquote>
<p>Parameters $\theta$ are trained to maximize the loglikelihood of the test images.</p>
<p>$$
\theta=\arg \max _{\theta} E_{L \sim T}\left[E_{S \sim L, B \sim L}\left[\sum_{(x, y) \in B} \log P_{\theta}(y | x, S)\right]\right]
$$</p>
<p><strong>Equivalence to GNN</strong></p>
<p>GNN approach can be considered equivalent to Matching Network if $f = g$ and $\mathbf{x}_{i}^{(0)}=\phi\left(x_{i}\right) = f_{\theta}(x_i)$ and $A_{i,j} = a(\mathbf{x_i, x_j})$.</p>
<h3 id="2-prototypical-network">2. <strong>Prototypical Network</strong></h3>
<p>Snell et al. 2017 reduced the number of comparisons in Matching Network by computing a class representative for each class and instead of comparing with all available images, the test image is only compared with the class representatives.</p>
<div align="center">
<img align="center" width="300"  src="/images/kshotGNN-prototypical.png">
</div>
<p>This paper first computes embeddings of all images $f_{\theta}(x_i)$ and then computes a class representative/prototype $\mathbf{v}_c$ for each class as mean of those embeddings.</p>
<p>$$
\begin{array}{c}\mathbf{v}_{c}=\frac{1}{\left|S_{c}\right|} \sum_{\left(\mathbf{x}_{i}, y_{i}\right) \in S_{c}} f_{\theta}\left(\mathbf{x}_{i}\right) \<br>
\end{array}
$$</p>
<p>The class probability distribution of test example is then computed by taking softmax of the distance between the embedding of test image and the class prototype. The distance measure used here is squared euclidean distance.</p>
<p>$$
P(y=c | \mathbf{x})=\operatorname{softmax}\left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)=\frac{\exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c}\right)\right)}{\sum_{c^{\prime} \in \mathcal{C}} \exp \left(-d_{\varphi}\left(f_{\theta}(\mathbf{x}), \mathbf{v}_{c^{\prime}}\right)\right)}
$$</p>
<p>The network is trained to reduce the negative loglikelihood.</p>
<p>$$\mathcal{L}(\theta)=-\log P_{\theta}(y=c | \mathbf{x})$$</p>
<p><strong>Equivalence to GNN</strong></p>
<p>GNN approach can be reduced to Prototypical Network if we make following changes.</p>
<ol>
<li>
<p>Restrict the node embeddings to $\mathbf{x}_{i}^{(0)}=\phi\left(x_{i}\right) = f_{\theta}(x_i)$</p>
</li>
<li>
<p>Fix the Adjacency Metric:</p>
</li>
</ol>
<p>$$
\tilde{A}_{i, j}^{(0)}=\left\{\begin{array}{cc}q^{-1} &amp; \text {if } l_{i}=l_{j} \ 0 &amp; \text { otherwise }\end{array}\right.
$$</p>
<blockquote>
<p>q is the number of examples in each class.</p>
</blockquote>
<ol start="3">
<li>Replace the convolution block by:</li>
</ol>
<p>$$\hat{Y}_{\ast}=\sum_{j} \tilde{A}_{\ast, j}^{(0)}\left\langle\mathbf{x}_{j}^{(0)}, u\right\rangle$$</p>
<h2 id="extentions">Extentions</h2>
<p>Paper also proposed a way to extend the few-shot learning setting to semi-supervised learning and active learning setting. Although straight forward I do not see the intuition clear enough or experiments strong enough to suggest that those setting have any added benefit on reducing uncertainty of the model.</p>
<h2 id="critique">Critique</h2>
<p>Although the paper is a fascinating read since it connect various approaches of few-shot / one-shot learning with graph-convolutional network. I find the experiments very weak. Especially since authors only use 1 test example per task. Lack of intuition for the attention mechanism for active learning setting is also discouraging.</p>
<h2 id="questions">Questions</h2>
<ol>
<li>How to decide $l$? <a href="#a1">Link to Answer</a></li>
</ol>
<blockquote>
<p>I believe, the label $l$ were randomly sampled from the dataset for each task.</p>
</blockquote>
<ol start="2">
<li>Explain how are parameters $\tilde{\theta}$ trained? <a href="#a3">Link to Answer</a></li>
<li>How to compute the adjacency matrix for each layer of GNN? <a href="#a3">Link to Answer</a></li>
</ol>
<blockquote>
<p>Question 2 and 3 are equivalent, since $A$ is a function of $\tilde{\theta}$.</p>
</blockquote>
<ol start="4">
<li>How to generate one hot encoding in Figure 1. <a href="#a4">Link to Answer</a></li>
<li>What is $u$? How to generate $u$? <a href="#a5">Link to Answer</a></li>
<li>What is the meaning of $\ast$? <a href="#a5">Link to Answer</a></li>
<li>Explain how prototypers are generated in prototypical networks? Explain how they are equivalent?</li>
<li>How to calculate probability $P(Y|\mathcal{T})$? <a href="#a8">Link to Answer</a></li>
<li>How is attention vector generated?</li>
</ol>
<blockquote>
<p>Attention vector is also trained.</p>
</blockquote>
<ol start="10">
<li>What is $Gc(.)$? <a href="#a10">Link to Answer</a></li>
</ol>
<h2 id="references">References</h2>
<ol>
<li><a href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html">Tutorial on Meta-Learning by Dr. Lilian Weng</a></li>
<li>Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. <em>Siamese neural networks for one-shot image recognition.</em> ICML deep learning workshop. 2015.</li>
<li>Vinyals, Oriol, et al. <em>Matching networks for one shot learning.</em> NeurIPS. 2016.</li>
<li>Snell, Jake, Kevin Swersky, and Richard Zemel. <em>Prototypical networks for few-shot learning.</em> NeurIPS. 2017.</li>
</ol>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=15
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/relational-inductive-bias/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Relational Inductive Bias</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/rrl/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Relational Reinforcement Learning</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
									</form>
</section>




<script type="text/javascript" src="/js/lunr.js?1609558768"></script>
<script type="text/javascript" src="/js/auto-complete.js?1609558768"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1609558768"></script>

    
<nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>


    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
