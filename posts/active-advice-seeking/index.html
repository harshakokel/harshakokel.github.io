<!DOCTYPE HTML>

<html>
  <head>
    <title>Active Advice Seeking for Inverse Reinforcement Learning</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Active Advice Seeking for Inverse Reinforcement Learning"/>
<meta name="twitter:description" content="My notes on Phillip Odom and Sriraam Natarajan, AAMAS 2016."/>

    <meta property="og:title" content="Active Advice Seeking for Inverse Reinforcement Learning" />
<meta property="og:description" content="My notes on Phillip Odom and Sriraam Natarajan, AAMAS 2016." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/active-advice-seeking/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-10-05T16:40:08&#43;02:00" />
<meta property="article:modified_time" content="2019-10-05T16:40:08&#43;02:00" /><meta property="og:site_name" content="Harsha Kokel" />


    <meta itemprop="name" content="Active Advice Seeking for Inverse Reinforcement Learning">
<meta itemprop="description" content="My notes on Phillip Odom and Sriraam Natarajan, AAMAS 2016."><meta itemprop="datePublished" content="2019-10-05T16:40:08&#43;02:00" />
<meta itemprop="dateModified" content="2019-10-05T16:40:08&#43;02:00" />
<meta itemprop="wordCount" content="655">
<meta itemprop="keywords" content="RL,advice,starling," />
    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="font-family: 'Dawning of a New Day', cursive;font-size:2em;color: var(--main-highlight-color);font-weight:500">harsha kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters: [
  {left: '$$', right: '$$', display: true},
  {left: '$', right: '$', display: false},
  {left: '\\(', right: '\\)', display: false},
  {left: '\\[', right: '\\]', display: true}
]});"></script>




	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Oct 5, 2019
  </li> 
  <li class="tags crumb"> 
      <a href="https://harshakokel.com/tags/rl/" class="tag__link">rl</a>  <a href="https://harshakokel.com/tags/advice/" class="tag__link">advice</a>  <a href="https://harshakokel.com/tags/starling/" class="tag__link">starling</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Active Advice Seeking for Inverse Reinforcement Learning</h2>
  </header>

  
  
  
 
  
  
  
    <blockquote class="preview">My notes on Phillip Odom and Sriraam Natarajan, <a href="https://starling.utdallas.edu/assets/pdfs/aasirl_AAMAS.pdf" target="_blank">AAMAS 2016</a>. </blockquote>
  
  
  <p><p>In Kunapali et al 2013, authors present a way of incorporating advice in Inverse Reinforcement Learning (IRL) by extending IRL formulation to include constraints based on the expert&rsquo;s advice of preferred and avoided actions, state and reward. Odom et al 2017 expands on  Kunapali et al (2013)&rsquo;s formulation of preferred and avoided actions by seeking the advice in active learning setting. The paper proposes a <em>active advice-seeking</em> framework, where instead of seeking mere-label from the expert for selected example as done in <em>active learning</em>, they seek advice (set of preferred and avoided labels) over set of examples. Two clear advantages of active advice-seeking framework over the traditional active learning setting is that expert can provide multiple advice and that advice is generalized over set of states.</p>
<img src="/images/workflow_activeadvice.png" width="400" alt="">
<p>The workflow of the paper is shown in the figure above. First the states are clustered together, heuristic proposed is to cluster the states which has similar action distribution, and authors use the action distribution of the demonstrations as a proxy. Then, a policy is learnt only from the demonstrations. Next, query is generated by selecting the state cluster with the maximum uncertainty measure (equation below). This uncertainty measure is an artifact of the entropy of the distribution of actions in <strong>demonstration</strong> and the entropy of the <strong>policy</strong></p>
<p>\[
U_{s}\left(s_{i}\right)=\underbrace{w_{p} F\left(s_{i}\right)}_{\text{demonstration}}+\underbrace{\left(1-w_{p}\right) G\left(s_{i}\right)}_{\text{policy}}
\]</p>
<p>Advice, set of preferred and avoid actions, is received on the query and a new policy is learnt based on the advice and the demonstrations. Query generation, advice seeking and policy learning steps are repeated again and again till the query budget surpasses.</p>
<!-- % Eperiments -->
<p>Active-advise seeking is compared against learning only from trajectories (standard IRL), advice over single state (active learning) and advice over random clusters. This paper show empirically (on 4 datasets) that the active-advice seeking framework proposed is superior to the other approaches, albeit sensitive to the quality of the cluster.</p>
<h2 id="critique">Critique</h2>
<p>The paper introduces first of its kind framework to actively solicit advice from the human experts and is a very important contribution towards showing the need for Human-In-The-Loop approaches. Abstract MDPs and Relational RL have a natural way to reference a set of states by abstract state and a horn clause respectively. This paper shows that in case of propositional RL, this can be achieved by clustering states using K-Means. This could be a valuable direction as it provides best of both worlds (Propositional &amp; Relational RL). The clustering heuristic does not have a theoretical justification but empirical evaluations show that it performs well for IRL. This is reasonable considering that we are learning a policy from the demonstration and hence the states with similar actions are grouped together. However, this might not be the case always. So, a more matured clustering technique might be useful.</p>
<p>The clustering is done only once, based on the demonstrations which might be sub-optimal. An iterative clustering might have been more challenging but one wonders if that would have helped achieve optimality especially in the drone-flying domain where there is lot of scope to improve.</p>
<p>In this framework, the advice is generalized over the  clustered states, but in real-world I imagine an advice can be generalized beyond clusters and hence a way to generalize advice across states might be an interesting future direction.</p>
<p>Also, I would like to appreciate the complexity of the drone flying domain. Here, as the drone is supposed to visit the corners in a specific order, the preferred action at any location will be different based on the next corner being targeted. Paper lacks the details of how the state encoding was done, it would have been interesting to see some details. The expert advice will also have to be carefully tailored to achieve the corner sub-goal.</p>
<h4 id="references">References</h4>
<ol>
<li>Phillip Odom and Sriraam Natarajan, <em>Active Advice Seeking for Inverse Reinforcement Learning</em>, <a href="https://starling.utdallas.edu/assets/pdfs/aasirl_AAMAS.pdf" target="_blank">AAMAS 2016</a></li>
<li>Gautam Kunapuli, Phillip Odom, Jude W Shavlik, and Sriraam Natarajan, <em>Guiding autonomous agents to better behaviors through human advice</em>, <a href="https://starling.utdallas.edu/assets/pdfs/kbirl_ICDM13.pdf" target="_blank">ICDM 2013</a></li>
</ol>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=5
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/minimal-sufficient-explanation/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Minimal Sufficient Explanation</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/hierarchical-rl/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Hierarchical Reinforcement Learning</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
</section>




<script type="text/javascript" src="/js/lunr.js?1669741475"></script>
<script type="text/javascript" src="/js/auto-complete.js?1669741475"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1669741475"></script>

    
<nav id="menu">
  <h2>hk</h2>
  
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>

    
    
    <section class="nav">
									<div class="mini-posts">
										<article>
                      <span class="mini-post-misc"><a href="/misc">miscellaneous</a></span>
											<a href="/misc" class="image"><img src="/images/Smile.png" alt=""></a>
										</article>
                    </div>
   </section>
  
    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
