<!DOCTYPE HTML>

<html>
  <head>
    <title>Graph Attention Networks</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Graph Attention Networks"/>
<meta name="twitter:description" content="My notes on Peter Veličković et al. ICLR 2018.  Written as part of the Complex  Networks course by Prof. Feng Chen."/>

    <meta property="og:title" content="Graph Attention Networks" />
<meta property="og:description" content="My notes on Peter Veličković et al. ICLR 2018.  Written as part of the Complex  Networks course by Prof. Feng Chen." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/gat/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-17T16:40:08&#43;02:00" />
<meta property="article:modified_time" content="2020-02-17T16:40:08&#43;02:00" /><meta property="og:site_name" content="Harsha Kokel" />


    <meta itemprop="name" content="Graph Attention Networks">
<meta itemprop="description" content="My notes on Peter Veličković et al. ICLR 2018.  Written as part of the Complex  Networks course by Prof. Feng Chen."><meta itemprop="datePublished" content="2020-02-17T16:40:08&#43;02:00" />
<meta itemprop="dateModified" content="2020-02-17T16:40:08&#43;02:00" />
<meta itemprop="wordCount" content="1300">
<meta itemprop="keywords" content="GNN,coursework," />
    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="font-family: 'Dawning of a New Day', cursive;font-size:2em;color: var(--main-highlight-color);font-weight:500">harsha kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters: [
  {left: '$$', right: '$$', display: true},
  {left: '$', right: '$', display: false},
  {left: '\\(', right: '\\)', display: false},
  {left: '\\[', right: '\\]', display: true}
]});"></script>




	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Feb 17, 2020
  </li> 
  <li class="tags crumb"> 
      <a href="https://harshakokel.com/tags/gnn/" class="tag__link">gnn</a>  <a href="https://harshakokel.com/tags/coursework/" class="tag__link">coursework</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Graph Attention Networks</h2>
  </header>

  
  
  
 
  
  
  
    <blockquote class="preview">My notes on Peter Veličković et al. <a href="https://openreview.net/pdf?id=rJXMpikCZ" target="_blank">ICLR 2018</a>.  Written as part of the Complex  Networks <a href="https://personal.utdallas.edu/~fxc190007/courses/20S-7301/" target="_blank">course</a> by Prof. Feng Chen. </blockquote>
  
  
  <p><h3 id="introduction">Introduction</h3>
<p>Convolutional Neural Networks (CNNs) can effectively transform grid-like structures and have been used for various image segmentation/classification tasks. Various approaches have not been proposed to extended CNNs graph structures. These approaches are broadly divided into two categories:</p>
<ol>
<li>
<p><strong>Spectral appraoch</strong> leverages the spectral representations of graph and defines convolution in the Fourier domain. However, because such convolutions require eigen-decomposition of graph laplacian, they can not be directly generalized to different graph structures. Most famous of this is Convolutional Graph Networks by Kipf and Welling, ICLR 2017. One major drawback of CNN is that is assigns equal importance to all the neighbors.</p>
</li>
<li>
<p><strong>Spatial approach</strong> on other hand perform convolution directly on the graph, leveraging the neighborhood structure. This can be challenging because of varying neighborhood and most approaches perform some form of aggregation on the neighbors. Since the neighbors in graph have no particular order, the aggregation provides equal importance to all the neighbors.</p>
</li>
</ol>
<p>Besides, attention mechanisms allow for focusing on relevant parts of input and have attained state-of-the-art for various tasks. This paper proposes the use of attention mechanism to provide a relevant weights to different neighbors.</p>
<h3 id="attention-layer">Attention layer</h3>
<p>Generally, each graph convolutional layer take input $\mathbf{h}=\left\{\vec{h}_{1}, \vec{h}_{2}, \ldots, \vec{h}_{N}\right\}, \vec{h}_{i} \in \mathbb{R}^{F}$ and generates output $\mathbf{h}^{\prime}=\left\{\vec{h}_{1}^{\prime}, \vec{h}_{2}^{\prime}, \ldots, \vec{h}_{N}^{\prime}\right\}, \vec{h}_{i}^{\prime} \in \mathbb{R}^{F^{\prime}}$ by linearly transforming the input vectors using weight matrix &ndash; $\mathbf{W} \in \mathbb{R}^{F^{\prime} \times F}$, taking weighted aggregate over the neighborhood &ndash; $\mathcal{N}$ (including the node itself), and finally passing the value from a non-linear activation function &ndash; $\sigma$.</p>
<p>$$
\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{i}\right)
$$</p>
<p>This can be equivalently written in matrix form as</p>
<p>$$
\mathbf{h}^{\prime} = \sigma \left( \tilde{A} \mathbf{\alpha} , \mathbf{h} \mathbf{W} \right) \\
\text{with}, \tilde{A} = A + I_N  \\
\text{(adjanceny matrix with self loop)}
$$</p>
<blockquote>
<p>In GCN, the convolutional layer is $\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j = 1 }^{N} \hat{A_{i j}} \mathbf{W} \vec{h}_{i}\right)$ where $\hat{A}$ is a renormalized laplacian which indicated neighbor. So, effectively $\alpha_{i j} = \frac{1}{\sqrt{d_{i}d_{j}}}$ because the</p>
</blockquote>
<p>This paper proposes to use the attention mechanism &ndash; $a$ to compute the $\alpha_{i j}$ in following way:</p>
<p>$$
\begin{aligned}
\alpha_{i j} &amp; =\operatorname{softmax}_{j}\left(e_{i j}\right) \\
&amp; =\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)} \\
\text{with}, , , e_{i j} &amp; =a\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)
\end{aligned}
$$</p>
<p>Intuitively, $e_{i j}$ is importance of node $j$ on $i$ and $\alpha_{i j}$ is normalized importance.</p>
<p>The framework proposed is agnostic to the choice of attention mechanism &ndash; $a$. However, in the paper authors use a single-layer feed-forward neural network parameterized by weight vector $\overrightarrow{\mathrm{a}} \in \mathbb{R}^{2 F^{\prime}}$ and LeakyReLU nonlinearity, which takes the input $\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{j}$, where $|$ represents concatenation. The activation mechanism is represented in the figure below. Mathematically, the activation mechanism used in the experiments is:</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(\text { LeakyReLU }\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{j}\right]\right)\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(\text { LeakyReLU }\left(\overrightarrow{\mathbf{a}} T\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{k}\right]\right)\right)}
$$</p>
<div align="center">
<img align="center" width="300"  src="../../images/attention.png">
<p>src: Veličković et al. 2018 </p>
</div>
<p>Intuitively,  $\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{j}\right]$ is  a linear combination of transformed $h_i$ and $h_j$. It can be thought of as a distance between node $i$ and $j$ or some aggregate. <a name="a1">(Answers Q-1)</a></p>
<blockquote>
<p>Since the slope of ReLU is zero for -ve values, the ability to train the model is compromised in that region. This is called <strong>dying ReLU</strong> problem. LeakyReLU activation function is used instead of ReLU with a=0.01 in the figure below to avoid that problem. LeakyReLU helps speed up learning and is more balanced. <a name="a2">(Answers Q-2)</a></p>
</blockquote>
 <div align="center"> <img align="center" width="500"  src="../../images/leakyReLU.jpg"> <p>src: <a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">https://towardsdatascience.com/activation-...</a></p> </div>
<p>Following <em>Vaswani et al. 2017</em>, authors proposes to use $K$ independent attention mechanism to employ multi-head attention. So the mathematical form of each layer is:</p>
<p>$$
\vec{h}_{i}^{\prime}=\Biggm|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$</p>
<p>and the output $\mathbf{h}^{\prime}$ consists $K F^{\prime}$ features instead of $F^{\prime}$.</p>
<p>Only, in last layer the multi-head attentions are aggregated instead of concatenation. So, the mathematical form of last layer is:</p>
<p>$$
\vec{h}_{i}^{\prime}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)
$$</p>
<div align="center">
<img align="center" width="500"  src="../../images/gat.jpg">
<p>src: Veličković et al. 2018 </p>
</div>
<h3 id="complexity">Complexity</h3>
<p>Time complexity of computing single attention head is sum of complexity of computing the $e_{i j}$ for each node and then computing the softmax. The feed forward neural network, computing $e_{i j} = \text{LeakyReLU}\left(\overrightarrow{\mathbf{a}}^{T}\left[\mathbf{W} \vec{h}_{i} | \mathbf{W} \vec{h}_{j}\right]\right)$ is effectively equivalent to complexity of matrix multiplication  $\underbrace{\mathbf{W}}_{F^{\prime} \times F} \underbrace{\vec{h}_{i}}_{F \times 1} + \underbrace{\overrightarrow{\mathbf{a}}^{T}}_{1 \times 2F^{\prime}} \underbrace{\hat{h}}_{2F^{\prime} \times 1}$ for each node, $\mathcal{O}\left(|\mathcal{V}|(FF^{\prime} + 2F^{\prime})\right) = \mathcal{O}\left(|\mathcal{V}|FF^{\prime}\right)$, where $\mathcal{V}$ is number of nodes. Complexity of computing $\alpha_{i j} =\operatorname{softmax}_{j}\left(e_{i j}\right)$ for all the edges is $\mathcal{O}(|\mathcal{E}|F^{\prime})$, where $\mathcal{E}$ is number of edges. So, total complexity of a single attention head is $\mathcal{O}(|\mathcal{V}|FF^{\prime} + |\mathcal{E}|F^{\prime})$.  <a name="a3">(Answers Q-3)</a></p>
<p>Memory complexity of GAT for sparse matrix is linear in terms of nodes and edges. <a name="a4">(Answers Q-4)</a></p>
<h3 id="experiments">Experiments</h3>
<p>Authors evaluate the GAT architecture for two type of learning tasks on graph structures:<a name="a5">(Answers Q-5)</a></p>
<ol>
<li>
<p><strong>Transductive learning task</strong> is where the algorithm sees whole graph but labels of only few nodes are available. Algorithm is trained on these nodes and whole graph and the task is to produce labels for nodes which do not have labels while training. GCN works only for transductive setting.</p>
</li>
<li>
<p><strong>Inductive learning task</strong> is when the algorithm sees only training nodes and edges between those nodes. Labels of all the training nodes are available and the task is to predict label for test nodes, which are unseen during training.</p>
</li>
</ol>
<h3 id="transductive-learning">Transductive learning</h3>
<p>For transductive learning task, 10 baseline architectures are compared (including GCN, MoNet, Chebyshev, MLP etc), for three datasets Cora, Citeseer and pubmed. Two layer GAT architecture was used as shown in figure below, with 8 attention heads in first layer and 1 attention head in second layer. Softmax was used in final layer and ELU activation is used in the first layer.</p>
<div align="center">
<img align="center" width="500"  src="../../images/Transductive.png">
</div>
<p>Mathematical form of the architecture:</p>
<p>$$
Z = \operatorname{softmax}\left( \tilde{A} \mathbf{\alpha}_{2}\mathbf{W}_2\left({\biggm|}_{k=1}^{8} \operatorname{ELU}\left(\tilde{A} \mathbf{\alpha}_{1}^{k}\mathbf{W}_1 H \right)\right)  \right)
$$</p>
<p><a name="a6">(Answers Q-6)</a></p>
<h3 id="inductive-learning">Inductive Learning</h3>
<p>For inductive learning task, 6 baseline architectures are compared (including GraphSAGE, MLP etc), for PPI dataset. Three layer architecture was used as shown in the figure below.</p>
<div align="center">
<img align="center" width="500"  src="../../images/inductive.png">
</div>
<p>Mathematical form of the architecture:</p>
<p>$$
Z = \operatorname{LogisticSigmoid}\left( \frac{1}{6} \sum_{z=1}^{6} \tilde{A} \mathbf{\alpha}_{3}^{z}\mathbf{W}_3\left({\biggm|}_{y=1}^{4} \operatorname{ELU}\left(\tilde{A} \mathbf{\alpha}_{2}^{y}\mathbf{W}_2 \left({\biggm|}_{k=1}^{4} \operatorname{ELU}\left(\tilde{A} \mathbf{\alpha}_{1}^{k}\mathbf{W}_1 H \right) \right)\right)\right)  \right)
$$</p>
<p><a name="a7">(Answers Q-7)</a></p>
<h3 id="extensions">Extensions</h3>
<p>The GAT network can be extended to use for Graph classification by simply appending a pooling layer at the end. Figure below represents one such architecture.</p>
<div align="center">
<img align="center" width="500"  src="../../images/GraphClassification.png">
</div>
<p><a name="a9">(Answers Q-9)</a></p>
<p>GAT can also be used for embedding the nodes to two-dimensional space. One such architecture is presented below, where instead of Softmax, the last layer outputs 2-D vector for each node.</p>
<div align="center">
<img align="center" width="500"  src="../../images/2D.png">
</div>
<p>Mathematical form of this 2D embedding GAT network is</p>
<p>$$
Z = \operatorname{ELU}\left( \tilde{A} \mathbf{\alpha}_{2}\mathbf{W}_2\left({\biggm|}_{k=1}^{8} \operatorname{ELU}\left(\tilde{A} \mathbf{\alpha}_{1}^{k}\mathbf{W}_1 H \right)\right)  \right)
$$</p>
<p><a name="a8">(Answers Q-8)</a></p>
<h3 id="questions">Questions</h3>
<ol>
<li>What attention mechanism is used in the experiments?  <a href="#a1">Link to Answer</a></li>
<li>Why LeakyReLU but not the standard ReLU ?  <a href="#a2">Link to Answer</a></li>
<li>Proof complexity of GAT layer is $O\left(|V| F F^{\prime}+|E| F^{\prime}\right)$. <a href="#a3">Link to Answer</a></li>
<li>What is the memory complexity of GAT layer?  <a href="#a4">Link to Answer</a></li>
<li>Explain difference between transductive and inductive learning.  <a href="#a5">Link to Answer</a></li>
<li>Draw architecture of two layer GAT model for transductive learning. What is the Mathematical formulation? <a href="#a6">Link to Answer</a></li>
<li>Draw architecture of three layer GAT model for inductive learning. What is the Mathematical formulation?  <a href="#a7">Link to Answer</a></li>
<li>Design a GAT model that embed the nodes of the cora network in a two-dimensional space. Draw the architecture and give the mathematic form. <a href="#a8">Link to Answer</a></li>
<li>Design a GAT model for graph classification. Draw the architecture and
give the mathematic form.  <a href="#a9">Link to Answer</a></li>
</ol>
<h3 id="references">References</h3>
<ol>
<li>Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio, <em><a href="https://openreview.net/pdf?id=rJXMpikCZ" target="_blank">Graph Attention Networks </a></em>, ICLR 2018</li>
<li><a href="https://petar-v.com/GAT/" target="_blank">Graph Attention Networks overview</a> by Peter Veličković</li>
<li>ML Paper explained - AISC by Karim Khayrat: <a href="https://www.youtube.com/watch?v=zMIs20GUK_w" target="_blank">Graph Attention Networks Explained</a></li>
<li>A Tutorial on <a href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343" target="_blank">Attention in Deep Learning</a>, by Alex Smola and Aston Zhang, ICML 2019</li>
<li><a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a</a></li>
<li><a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6">https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6</a></li>
</ol>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=9
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/gcn/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Graph Convolutional Networks</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/understanding-node-attention/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Understanding Attention and Generalization in Graph Neural Networks</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
</section>




<script type="text/javascript" src="/js/lunr.js?1653953464"></script>
<script type="text/javascript" src="/js/auto-complete.js?1653953464"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1653953464"></script>

    
<nav id="menu">
  <h2>hk</h2>
  
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>

    
    
    <section class="nav">
									<div class="mini-posts">
										<article>
                      <span class="mini-post-misc"><a href="/misc">miscellaneous</a></span>
											<a href="/misc" class="image"><img src="/images/Smile.png" alt=""></a>
										</article>
                    </div>
   </section>
  
    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
