<!DOCTYPE HTML>

<html>
  <head>
    <title>Relational Inductive Bias</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Relational Inductive Bias"/>
<meta name="twitter:description" content="There is a recent surge in papers which are using Relational Inductive Bias with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic."/>

    <meta property="og:title" content="Relational Inductive Bias" />
<meta property="og:description" content="There is a recent surge in papers which are using Relational Inductive Bias with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/relational-inductive-bias/" />
<meta property="article:published_time" content="2020-03-30T16:40:08+02:00" />
<meta property="article:modified_time" content="2020-03-30T16:40:08+02:00" /><meta property="og:site_name" content="Harsha Kokel" />

    <meta itemprop="name" content="Relational Inductive Bias">
<meta itemprop="description" content="There is a recent surge in papers which are using Relational Inductive Bias with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic.">
<meta itemprop="datePublished" content="2020-03-30T16:40:08+02:00" />
<meta itemprop="dateModified" content="2020-03-30T16:40:08+02:00" />
<meta itemprop="wordCount" content="1010">



<meta itemprop="keywords" content="GNN,ILP," />

    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="font-family: 'Dawning of a New Day', cursive;font-size:2em;color: var(--main-highlight-color);font-weight:500">harsha kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters: [
  {left: '$$', right: '$$', display: true},
  {left: '$', right: '$', display: false},
  {left: '\\(', right: '\\)', display: false},
  {left: '\\[', right: '\\]', display: true}
]});"></script>




	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Mar 30, 2020
  </li> 
  <li class="tags crumb"> 
      <a href="https://harshakokel.com/tags/gnn/" class="tag__link">gnn</a>  <a href="https://harshakokel.com/tags/ilp/" class="tag__link">ilp</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Relational Inductive Bias</h2>
  </header>

  
  
  
 
  
  
  
    <blockquote class="preview">There is a recent surge in papers which are using <strong>Relational Inductive Bias</strong> with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic. </blockquote>
  
  
  <p><p>(<strong>Inductive</strong>) Bias refers to any basis for choosing one generalization over another, other than strict consistency with the observed training instances [Mitchell 80]. <strong>Relational inductive bias</strong> refers to biases which impose constraints on relationships and interactions among entities. Logic Programming extensively deals with the entities and relationships. So, it seems to me there should be some equivalence between the biases used in ILP with the Relational Inductive biases.</p>
<h2 id="bias-in-ilp">Bias in ILP</h2>
<blockquote>
<p>&hellip; bias is anything which influences how the concept learner draws inductive inferences based on the evidence. There are two fundamentally different forms of bias: <strong>declarative bias</strong>, which defines the space of hypotheses to be considered by the learner, i.e., what to search, and <strong>preference bias</strong>, which determines how to search that space, which hypotheses to focus on, and which ones to prune, etc.<br>
&hellip;<br>
ILP systems distinguish two kinds of declarative bias: syntactic bias (sometimes also called
language bias) and semantic bias. <strong>Syntactic bias</strong> imposes restrictions on the form (syntax)
of clauses allowed in hypothesis. &hellip; <strong>Semantic bias</strong>
imposes restrictions on the meaning, or the behavior of hypotheses.<br>
&ndash; <strong>Muggleton and De Raedt, 1994</strong></p>
</blockquote>
<p>Another typology of biases include following categorizations</p>
<ol>
<li><strong>Language Bias</strong> - specifies set of syntactically acceptable clauses.</li>
<li><strong>Search Bias</strong> - specifies which region of hypothesis set should be searched.</li>
<li><strong>Validation Bias</strong> - specifies when the search should stop.</li>
</ol>
<h3 id="language-bias">Language Bias</h3>
<p>Some straight forward syntactic constraints can be maximum length of clause, number of new variables that can be introduced in the clause, number of function operators used in horn clauses, etc. All of these are form of language-bias.</p>
<p>More evolved language biases include</p>
<ol>
<li>
<p><strong>specify the set of clauses allowed in hypotheses</strong><br>
each clause in this set can further contain set of literals which are allowed <br>
$\quad$ e.g, $\operatorname{father}(X, Y) \leftarrow { \operatorname{male}(X), \operatorname{female}(X) }, \operatorname{parent}(X, Y);$</p>
</li>
<li>
<p><strong>second-order schemata</strong><br>
Second-order schemata defines a language bias as the set of all clauses that can be obtained by instantiating a <em>second-order schema</em> with a <em>second-order substitution</em>.<br>
A <strong>second-order schema</strong> is a clause, where some of the predicate names are (existentially quantified) predicate variables.<br>
$\quad$ e.g.,
$S = \exists p, q, r : p(X, Y) \leftarrow q(X, XW), q(YW, Y), r(XW, YW);$<br>
A <strong>second-order substitution</strong> is a substitution that replaces predicate-variables by predicate names.<br>
$\quad$ e.g., $\theta = { p /\operatorname{connected}, q/\operatorname{part-oj}, r/\operatorname{touches}}$<br>
gives<br>
$\quad$ $S\theta = \operatorname{connected(X, Y)} \leftarrow \operatorname{part-of}(X,XW), \operatorname{part-ox}(YW, Y), \operatorname{touches}(XW, YW)$</p>
</li>
</ol>
<h3 id="search-bias">Search Bias</h3>
<p>Search bias can be a restriction or preference. With restriction, some hypothesis space is ignored. While with preference, some space is prioritized while searching. Search bias is given by $types$ and $modes$ in ILP. This can also be <em>semantic bias</em></p>
<p>One important <em>search/semantic bias</em> used in most ILP systems is notion of <strong>determinate clause</strong>. <em>&ldquo;A clause is determinate if all of its literals are determinate; and a literal
is determinate if each of its variables that does not appear in preceding literals has only one
possible binding given the bindings of its variables that appear in preceding literals.&quot;</em> (from Muggleton and De Raedt, 94).</p>
<p>Search biases can be broadly categorized as:</p>
<ol>
<li><strong>Ordering</strong><br>
Specification of ordering for pre-compiled hypotheses or relations to be considered for learning.</li>
<li><strong>Example selection criteria</strong><br>
How the examples are selected to verify some candidate hypothesis</li>
<li><strong>Coverage function</strong></li>
<li><strong>Intermediate validation criteria</strong></li>
</ol>
<h2 id="bias-in-graph-networks">Bias in Graph Networks</h2>
<p>In Graph Networks, inductive biases used include <strong>non-relational inductive biases</strong> like choice of activation function, dropout, weight decay, training curricula, algorithm optimization etc. <strong>Relational inductive bias</strong>, on other hand are the biases that are constrain the interactions of entities, relationships and rules while learning the model. <strong>Entities/Nodes</strong> are elements with attributes, <strong>relations/edges</strong> are property between entities and <strong>rule</strong> is a function mapping set of entities and relations to another entity and relation.</p>
<p>Three main <strong>relational inductive bias</strong> introduced in graph networks are</p>
<ol>
<li>Locality</li>
<li>Translation invariance</li>
<li>Permutation invariance</li>
</ol>
<p>For <em>fully-connected (FC) layers</em>, all the <em>entities</em> (nodes in network) are connected to all other entities and there is no restriction on rules that can be used. So, relational-inductive bias is very weak for FC layers.</p>
<p>For <em>Convolutional layer</em>, the <em>nodes</em> are the pixels of images and <em>edges</em> are the relation between neighbors. <em>Rules</em> are defined by the weights and biases of the hidden network.
One relational-inductive bias in here is constraint of locality. Rules should use related nodes, i.e. convolution function should only use the neighbors, it can not use arbitrary pixels.
Another relational-inductive bias used is that the kernel matrix is same for all the neighborhoods, i.e. all the localities are related in same way so rule is translation invariant.</p>
<p>In ILP, the locality bias is, perhaps, induced by use of $\operatorname{mode}$. A $+ve , \operatorname{mode}$ is preferred in a literal when adding it as candidate to a hypothesis clause. In a sense, we prefer edges/connected nodes from previously selected nodes in the hypothesis clause.
The translation invariance bias is, perhaps, integral in ILP since horn clause by definition use $\forall$ operator.</p>
<p>In images, there is a natural ordering of the pixels and hence the nodes are ordered. But in most graphs, nodes do not have a natural ordering, nor does edges. One of the most important contribution of the <em>Graph Neural Networks</em> and <em>Graph Convolutional Networks</em> is to provide a way to operate on nodes in graph which is permutation invariant. So, this becomes the third and the most important relational inductive bias.</p>
<p>ILP does not assume any natural ordering of the entities or relations. Hence there is no need of an equivalent bias.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There are many more biases in ILP which are not yet accounted for in Graph Networks and hence if we find a way to include those biases for Graph Networks, we might be able to achieve sample efficient and effective learning in Graph Networks.</p>
<h2 id="references">References</h2>
<ol>
<li>Inductive Logic Programming: theory and methods, Stephen Muggleton and Luc De Raedt, J. Logic Programming 1994</li>
<li>Claire Nédellec, Céline Rouveirol, Hilde Adé, Francesco Bergadano, and Birgit Tausend. <em>Declarative Bias in ILP</em>, 1996.</li>
<li>Tom Mitchell, <em>The need for biases in learning generalizations</em>, 1980.</li>
<li>Battaglia et al. <em>Relational inductive biases, deep learning, and graph network</em>, arXiv:1806.01261, 2018</li>
</ol>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=14
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/maml/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Model-Agnostic Meta-Learning</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/few-shot-learning-gnn/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Few-Shot Learning with GNN</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
</section>




<script type="text/javascript" src="/js/lunr.js?1614619334"></script>
<script type="text/javascript" src="/js/auto-complete.js?1614619334"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1614619334"></script>

    
<nav id="menu">
  <h2>hk</h2>
  
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>

    
    
    <section class="nav">
									<div class="mini-posts">
										<article>
                      <span class="mini-post-misc"><a href="/misc">miscellaneous</a></span>
											<a href="/misc" class="image"><img src="/images/Smile.png" alt=""></a>
										</article>
                    </div>
   </section>
  
    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
