<!DOCTYPE HTML>

<html>
  <head>
    <title>Fitted Q and Batch Reinforcement Learning</title>
    
    
    <meta name="description" content="This website is the home of Harsha Kokel. A Ph.D. student working with Prof. Sriraam Natarajan at The University of Texas at Dallas" />
    
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.gstatic.com"> 
    <link href="https://fonts.googleapis.com/css2?family=Dawning+of+a+New+Day&display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
     <script src="/js/jquery-3.3.1.min.js"></script>
    <link rel="manifest" href="/site.webmanifest">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Fitted Q and Batch Reinforcement Learning"/>
<meta name="twitter:description" content="Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an RL for healthcare project, as part of Advanced RL course by Prof. Sriraam Natarajan."/>

    <meta property="og:title" content="Fitted Q and Batch Reinforcement Learning" />
<meta property="og:description" content="Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an RL for healthcare project, as part of Advanced RL course by Prof. Sriraam Natarajan." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/posts/batch-rl/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-12-06T16:40:08&#43;02:00" />
<meta property="article:modified_time" content="2019-12-06T16:40:08&#43;02:00" /><meta property="og:site_name" content="Harsha Kokel" />


    <meta itemprop="name" content="Fitted Q and Batch Reinforcement Learning">
<meta itemprop="description" content="Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an RL for healthcare project, as part of Advanced RL course by Prof. Sriraam Natarajan."><meta itemprop="datePublished" content="2019-12-06T16:40:08&#43;02:00" />
<meta itemprop="dateModified" content="2019-12-06T16:40:08&#43;02:00" />
<meta itemprop="wordCount" content="882">
<meta itemprop="keywords" content="RL,offline,summary," />
    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="/" style="font-family: 'Dawning of a New Day', cursive;font-size:2em;color: var(--main-highlight-color);font-weight:500">harsha kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body,{delimiters: [
  {left: '$$', right: '$$', display: true},
  {left: '$', right: '$', display: false},
  {left: '\\(', right: '\\)', display: false},
  {left: '\\[', right: '\\]', display: true}
]});"></script>




	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/posts/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Dec 6, 2019
  </li> 
  <li class="tags crumb"> 
      <a href="https://harshakokel.com/tags/rl/" class="tag__link">rl</a>  <a href="https://harshakokel.com/tags/offline/" class="tag__link">offline</a>  <a href="https://harshakokel.com/tags/summary/" class="tag__link">summary</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Fitted Q and Batch Reinforcement Learning</h2>
  </header>

  
  
  
 
  
  
  
    <blockquote class="preview">Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an <a href="/pdf/ECMO-RL.pdf">RL for healthcare</a> project, as part of Advanced RL course by Prof. Sriraam Natarajan. </blockquote>
  
  
  <p><div class="box">
<h4 id="terminologies">Terminologies</h4>
<h5 id="offline-planning-problem-mdp">Offline Planning Problem (MDP)</h5>
<p>We are given the full MDP model and the problem is solved using all the components of the MDP</p>
<h5 id="online-planning-problem-rl">Online Planning Problem (RL)</h5>
<p>We have limited knowledge of the MDP. We can discover it by interacting with the system</p>
<h5 id="model-based-rl">Model-based RL</h5>
<p>Approaches to solving Online planning problem (RL) by first estimating (when missing) or accessing the full MDP Model i.e. transition and reward function and then finding policy $\pi$ is called Model-based RL</p>
<h5 id="model-free-rl">Model-free RL</h5>
<p>On the contrary, approaches to solving the Online RL Problem directly, i.e. solving for $\pi$ directly with either value function $V$ or state-action function $Q$ is called Model-free RL.</p>
</div>
<h4 id="batch-rl">Batch RL</h4>
<p>The simulation environment is not present and complete set of transition samples $\langle s, a, r, s^{\prime}
\rangle$ is given and the challenge is to learn without exploring.</p>
<h5 id="background-on-q-learning">Background on Q Learning</h5>
<p>Bellman optimality equation for the action-value function ($Q$) is given as:</p></p>
<p>$$Q^{\pi}(s, a)=\sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma \sum_{a^{\prime}} \pi\left(s^{\prime}, a^{\prime}\right) Q^{\pi}\left(s^{\prime}, a^{\prime}\right)\right]
$$</p>
<p>where $T\left(s, a, s^{\prime}\right)$ is a transition probability of landing in state $s^{\prime}$ on taking action $a$ in state $s$ and $R\left(s, a, s^{\prime}\right)$ is a Reward at state $s^{\prime}$ reached on taking action $a$ in state $s$.</p>
<p>In the dynamic programming setting, the $Q$ function for optimal policy is represented as:</p>
<p>$$Q_{k+1}(s, a) \leftarrow \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right]
$$</p>
<p>Q-Learning is a model-free approach to learn the $Q$ function by exploring the environment, i.e. performing actions based on some policy. A table of Q function for each state action pair $Q(s,a)$ is maintained and the table in updated after every action using the running average formula:</p>
<p>$$Q(s, a) \leftarrow(1-\alpha) Q(s, a)+(\alpha)[
R\left(s, a, s^{\prime}\right)+\gamma \max_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)]
$$</p>
<p>With multiple episodes the Q values will eventually converge and the optimal policy might be retrieved from that.</p>
<h4 id="drawbacks-of-q-learning">Drawbacks of Q Learning.</h4>
<p>There are several draw backs of the Q-Learning. These drawbacks might be minor in typical reinforcement learning setting where we have simulators. But these drawbacks are serious limitations in the <a href="#BatchRL">Batch RL</a> setting. In Batch RL, the simulation environment is not present and complete set of transition samples ($\langle s, a, r, s^{\prime} \rangle$) is given and the challenge is to learn without exploring.</p>
<p><strong>Exploration overhead</strong></p>
<p>As we see it in the figure below, at some point in the top left cell $(1,3)$, agent explored the action of going north and because it landed in the same cell, it updated $Q(s,north)=0.11$ as per the $\max_{a} Q(s,a)$ of that cell during that episode. After that episode, even though the $\max_{a} Q(s,a)$ of that cell changes, the $Q(s,north)$ does not get updated till the agent explores going north.</p>
<img src="/images/QLearning.gif" alt="Q Learning">
<blockquote>
source: <a href="https://www.youtube.com/watch?v=IXuHxkpO5E8&amp;t=3492s">UC Berkeley CS188: Lecture of Pieter Abbeel</a>
</blockquote>
<!-- ![Q Learning](QLearning.png) -->
<!-- More detail on 'exploration overhead' is found in  Batch RL chapter. -->
<!-- This is where **experience replay** comes in. -->
<p><strong>Stability issue</strong></p>
<p>Q-Learning has &lsquo;asynchronous update&rsquo; i.e. after each observation the value is updated locally only for the state at hand and all other states are left untouched. In the above figure, we know the value at the Red tile, but the Q value for the tile below it is not updated until we explore the action of going to red tile from that tile.</p>
<p>Similar idea of asynchronous update is also applicable in function approximations where $Q$ function is estimated by a function and at every time step the function is updated using:</p>
<p>$$f(s, a) \leftarrow(1-\alpha) f(s, a)+\alpha \left( r +\gamma \max_{a^{\prime} \in A} f\left(s^{\prime}, a^{\prime}\right) \right)
$$</p>
<p><strong>Inefficient approximation</strong></p>
<p>The &lsquo;asynchronous update&rsquo; in function approximation is particularly harmful with global approximation functions. An attempt to improve $Q$ value of a single state after every time step might impair all other approximations. Specially when approximation function used is like Neural Network, where a single example can impact changes in all the weights. <a href="#Gordon1995">Gordon 1995</a> proves that using an impaired approximation in next iteration, the $f$ function may divergence from the optimal $Q$ function.</p>
<p>This is where <em>fitted</em> methods come in.</p>
<hr>
<h3 id="fitted-approaches">Fitted Approaches</h3>
<p><a href="#Gordon1995">Gordon 1995</a> provided a stable function approximation approach by separating dynamic programming step from function approximation step. Effectively, the above function update equation is now split to two steps.</p>
<p>$$f^{\prime}(s, a) \leftarrow  r +\gamma \max_{a^{\prime} \in A} f\left(s^{\prime}, a^{\prime}\right)  , , , , \forall s,a \\
f(s, a) \leftarrow(1-\alpha) f(s, a)+\alpha f^{\prime}(s, a)
$$</p>
<p><strong>Observation: Splitting the function update from one to two steps is equivalent to changing the gram-schmidt orthonormalization to modified-gram schmidt orthonormalization.</strong></p>
<p><a href="#Ernst2005">Ernst 2005</a> proposed fitted Q iteration by borrowing the splitted approach from Gordon. The approach proposes iterative approximation of Q value by reformulating the Q Learning as a supervised regression problem. Algorithm proposed for fitted Q iteration is mentioned below.</p>
<pre class="editor-colors lang-{tidy=FALSE,"><span><span class="syntax--text syntax--plain syntax--null-grammar">Given: tuples {&lt;s,a,r,s'&gt;}, stopping condition</span></span>
<span class=""><span class="syntax--text syntax--plain syntax--null-grammar"></span></span>
<span><span class="syntax--text syntax--plain syntax--null-grammar">1. Q(s, a) = 0</span></span>
<span><span class="syntax--text syntax--plain syntax--null-grammar">2. while (!stopping condition):</span></span>
<span><span class="syntax--text syntax--plain syntax--null-grammar">3.    Build a training set:</span></span>
<span><span class="syntax--text syntax--plain syntax--null-grammar"><span class="leading-whitespace">         </span>{feature; regression value} = {&lt;s,a&gt; ; r + max_a Q(s,a)}</span></span>
<span><span class="syntax--text syntax--plain syntax--null-grammar">4.    Learn a function approximating the regression values Q (s,a)</span></span></pre>
<p>This is in principle similar to equations mentioned above, with $f^{\prime}$ as regression value and  $\alpha=1$.</p>
<p>Further extensions to the fitted Q approaches have learnt $f$ function as some linear combination of the previous function and regression values.</p>
<h4 id="references">References</h4>
<ul>
<li><a name="BatchRL"></a>Lange, S., Gabel, T., &amp; Riedmiller, M. Batch Reinforcement Learning. <a href="https://link.springer.com/chapter/10.1007/978-3-642-27645-3_2">2012</a></li>
<li><a name="Gordon1995"></a>Gordon, G. J. Stable Function Approximation in Dynamic Programming, <a href="https://www.sciencedirect.com/science/article/pii/B9781558603776500402">ICML 1995</a>.</li>
<li><a name="Ernst2005"></a>Ernst, D., Geurts, P., &amp; Wehenkel, L. Tree-Based Batch Mode Reinforcement Learning, In <a href="https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf">JMLR 2005</a>.</li>
<li><a href="http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf"><a href="http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf">http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf</a></a></li>
</ul>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=7
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item prev" href="https://harshakokel.com/posts/hierarchical-rl/">
        <span class="postpagination__label"><i class="fa solid fa-backward"></i> Previous post</span>
        <span class="postpagination__title">Hierarchical Reinforcement Learning</span>
    </a>
  

  
    <a class="postpagination__item next" href="https://harshakokel.com/posts/gcn/">
      <span class="postpagination__label">Next post <i class="fa solid fa-forward"></i></span>
      <span class="postpagination__title" >Graph Convolutional Networks</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    <section id="search" class="alt">
										<input type="text" name="search-by" id="search-by" type="search" placeholder="Search">
</section>




<script type="text/javascript" src="/js/lunr.js?1649354668"></script>
<script type="text/javascript" src="/js/auto-complete.js?1649354668"></script>
<script type="text/javascript">
    
        var baseurl = "https:\/\/harshakokel.com";
    
</script>
<script type="text/javascript" src="/js/search.js?1649354668"></script>

    
<nav id="menu">
  <h2>hk</h2>
  
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>

    
    
    <section class="nav">
									<div class="mini-posts">
										<article>
                      <span class="mini-post-misc"><a href="/misc">miscellaneous</a></span>
											<a href="/misc" class="image"><img src="/images/Smile.png" alt=""></a>
										</article>
                    </div>
   </section>
  
    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
