<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on Home</title>
    <link>https://harshakokel.com/tags/rl/</link>
    <description>Recent content in RL on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Original-Theme is licensed with the Creative Commons Attribution 3.0 Unported License</copyright>
    <lastBuildDate>Wed, 22 Apr 2020 16:40:08 +0200</lastBuildDate><atom:link href="https://harshakokel.com/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From Skills to Symbols:Learning Symbolic Representations for Abstract High-Level Planning</title>
      <link>https://harshakokel.com/posts/skills-to-symbols/</link>
      <pubDate>Wed, 22 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/skills-to-symbols/</guid>
      <description>George Konidaris, Leslie Pack Kaelbling, Tomas Lozano-Perez, JAIR 2018 This paper learns abstract symbolic representations from lower level trajectories for planning at a high-level. Big Idea of this paper is that given different domains of increasing difficulty at lower level but similar high level tasks, if we are able to segregate the low-level and high-level tasks, the tasks can be considered equivalent at higher level and hence can be solved in a uniform manner.</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning With Relational Inductive Biases</title>
      <link>https://harshakokel.com/posts/drrl/</link>
      <pubDate>Wed, 15 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/drrl/</guid>
      <description>Vinicius Zambaldi, David Raposo, Adam Santoro et al. ICLR 2019. Deep RL methods have been every effective but they have poor generalization capability, especially combinatorial generalization (for eg. if the number of blocks are changed in the blocks world). Recent advances in graph network literature have achieved combinatorial generalization by learning neural network that can reason about relationship of various nodes in graphs. Since this reasoning happens pairwise, the algorithms are able to scale to varying number of objects.</description>
    </item>
    
    <item>
      <title>Relational Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/rrl/</link>
      <pubDate>Sun, 12 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/rrl/</guid>
      <description>Džeroski, Sašo, Luc De Raedt, and Kurt Driessens, Machine Learning 2001 The paper came before the goal-conditioned RL, Multi-task RL or Graph Neural Network literature. Major motivation of this paper is to learn a generalizable policy. Generalization in terms of varying number of objects in the domain (for example, in blocks-world number of blocks can change) or change in the goal state (for example, stack red block on blue block instead if green on yellow).</description>
    </item>
    
    <item>
      <title>Hindsight Experience Replay</title>
      <link>https://harshakokel.com/posts/her/</link>
      <pubDate>Fri, 20 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/her/</guid>
      <description>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017 Remember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.
In standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states.</description>
    </item>
    
    <item>
      <title>Few-Shot Bayesian Imitation Learning with Logical Program Policies</title>
      <link>https://harshakokel.com/posts/logic-program-policies/</link>
      <pubDate>Wed, 18 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/logic-program-policies/</guid>
      <description>Tom Silver, Kelsey R. Allen, Alex K. Lew, Leslie Kaelbling, and Josh Tenenbaum, AAAI 2020 This paper introduces a bayesian imitation learning approach to learn policies from few demonstrations. They call these policies Logical Program Policies (LPP) which are essentially policies learnt as combination of logical and programmatic policies. Logical because these are relational and programmatic because they are features are automatically learned.
The bayesian prior used here is the prior probability distribution over the Probablistic Context Free Grammer (P-CFG).</description>
    </item>
    
    <item>
      <title>Fitted Q and Batch Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/batch-rl/</link>
      <pubDate>Fri, 06 Dec 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/batch-rl/</guid>
      <description>Terminologies Offline Planning Problem (MDP) We are given the full MDP model and the problem is solved using all the components of the MDP
Online Planning Problem (RL) We have limited knowledge of the MDP. We can discover it by interacting with the system
Model-based RL Approaches to solving Online planning problem (RL) by first estimating (when missing) or accessing the full MDP Model i.e. transition and reward function and then finding policy $\pi$ is called Model-based RL</description>
    </item>
    
    <item>
      <title>Hierarchical Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/hierarchical-rl/</link>
      <pubDate>Mon, 28 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/hierarchical-rl/</guid>
      <description>Standard RL planning suffers from the curse of dimensionality when the action space is too large and/or state space is infeasible to enumerate. Humans simplify the problem of planning in such complex conditions by abstracting away details which are not relevant at a given time and decomposing actions into hierarchies. Several researchers have proposed to model the temporal-abstraction in RL by composing some form of hierarchy over actions space (Dietterich 1998, Sutton et al 1998, Parr and Russell 1998).</description>
    </item>
    
    <item>
      <title>Active Advice Seeking for Inverse Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/active-advice-seeking/</link>
      <pubDate>Sat, 05 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/active-advice-seeking/</guid>
      <description>Phillip Odom and Sriraam Natarajan, AAMAS 2017 In Kunapali et al 2013, authors present a way of incorporating advice in Inverse Reinforcement Learning (IRL) by extending IRL formulation to include constraints based on the expert&amp;rsquo;s advice of preferred and avoided actions, state and reward. Odom et al 2017 expands on Kunapali et al (2013)&amp;rsquo;s formulation of preferred and avoided actions by seeking the advice in active learning setting. The paper proposes a active advice-seeking framework, where instead of seeking mere-label from the expert for selected example as done in active learning, they seek advice (set of preferred and avoided labels) over set of examples.</description>
    </item>
    
    <item>
      <title>Minimal Sufficient Explanations for Factored Markov Decision Processes</title>
      <link>https://harshakokel.com/posts/minimal-sufficient-explanation/</link>
      <pubDate>Tue, 03 Sep 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/minimal-sufficient-explanation/</guid>
      <description>Omar Zia Khan, Pascal Poupart, and James P Black. ICAPS 2009 Automated planning problems have long been attempted using Markov Decision Processes (MDPs). MDPs are capable of handling the probabilistic and sequential nature of planning problem. They solve the problem by providing a policy which is a mapping from states to actions. However, to use this policy in the real-world, we first need users to trust the policy. The issue of trust can be ameliorated if the policy provides an explanation for its recommeded actions.</description>
    </item>
    
  </channel>
</rss>
