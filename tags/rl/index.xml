<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL on Harsha Kokel</title>
    <link>https://harshakokel.com/tags/rl/</link>
    <description>Recent content in RL on Harsha Kokel</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Original-Theme is licensed with the Creative Commons Attribution 3.0 Unported License</copyright>
    <lastBuildDate>Wed, 22 Apr 2020 16:40:08 +0200</lastBuildDate><atom:link href="https://harshakokel.com/tags/rl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning</title>
      <link>https://harshakokel.com/posts/skills-to-symbols/</link>
      <pubDate>Wed, 22 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/skills-to-symbols/</guid>
      <description>In the pursuit of learning planner from data, I ended up reading Konidaris et al. &lt;a href=&#34;https://www.jair.org/index.php/jair/article/view/11175&#34; target=&#34;_blank&#34;&gt;(JAIR 2018)&lt;/a&gt;. Getting through this paper was an onerous task. Which I would not like to do again. So, here are my notes on the key concepts from that paper, which are relevant for learning high-level, abstract planner.</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning With Relational Inductive Biases</title>
      <link>https://harshakokel.com/posts/drrl/</link>
      <pubDate>Wed, 15 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/drrl/</guid>
      <description>Relational RL has not made a lot of splashes in real-world because it is easier to write a planner than learn a relational RL agent. This might be about to change with the current achievements of the graph based relational reasoning approaches. This article summarizes my understanding of the pioneering work of Vinicius Zambaldi et al. &lt;a href=&#34;https://openreview.net/forum?id=HkxaFoC9KQ&#34; target=&#34;_blank&#34;&gt;(ICLR 2019)&lt;/a&gt; on Deep Relational RL.</description>
    </item>
    
    <item>
      <title>Relational Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/rrl/</link>
      <pubDate>Sun, 12 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/rrl/</guid>
      <description>My notes on Džeroski, Sašo, Luc De Raedt, and Kurt Driessens, &lt;a href=&#34;https://link.springer.com/article/10.1023/A:1007694015589&#34; target=&#34;_blank&#34;&gt;Machine Learning 2001&lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Hindsight Experience Replay</title>
      <link>https://harshakokel.com/posts/her/</link>
      <pubDate>Fri, 20 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/her/</guid>
      <description>My notes on Marcin Andrychowicz et al. &lt;a href=&#34;https://arxiv.org/abs/1707.01495&#34; target=&#34;_blank&#34;&gt;NeurIPS 2017&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Few-Shot Bayesian Imitation Learning with Logical Program Policies</title>
      <link>https://harshakokel.com/posts/logic-program-policies/</link>
      <pubDate>Wed, 18 Mar 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/logic-program-policies/</guid>
      <description>My notes on Tom Silver, Kelsey R. Allen, Alex K. Lew, Leslie Kaelbling, and Josh Tenenbaum, &lt;a href=&#34;https://github.com/tomsilver/policies_logic_programs&#34; target=&#34;_blank&#34;&gt;AAAI 2020&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Fitted Q and Batch Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/batch-rl/</link>
      <pubDate>Fri, 06 Dec 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/batch-rl/</guid>
      <description>Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an &lt;a href=&#34;https://harshakokel.com/pdf/ECMO-RL.pdf&#34;&gt;RL for healthcare&lt;/a&gt; project, as part of Advanced RL course by Prof. Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>Hierarchical Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/hierarchical-rl/</link>
      <pubDate>Mon, 28 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/hierarchical-rl/</guid>
      <description>An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan.</description>
    </item>
    
    <item>
      <title>Active Advice Seeking for Inverse Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/active-advice-seeking/</link>
      <pubDate>Sat, 05 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/active-advice-seeking/</guid>
      <description>My notes on Phillip Odom and Sriraam Natarajan, &lt;a href=&#34;https://starling.utdallas.edu/assets/pdfs/aasirl_AAMAS.pdf&#34; target=&#34;_blank&#34;&gt;AAMAS 2016&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Minimal Sufficient Explanations for Factored Markov Decision Processes</title>
      <link>https://harshakokel.com/posts/minimal-sufficient-explanation/</link>
      <pubDate>Tue, 03 Sep 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/minimal-sufficient-explanation/</guid>
      <description>My notes on Omar Zia Khan, Pascal Poupart, and James P Black, &lt;a href=&#34;https://ojs.aaai.org/index.php/ICAPS/article/view/13365&#34; target=&#34;_blank&#34;&gt;ICAPS 2009&lt;/a&gt;</description>
    </item>
    
  </channel>
</rss>
