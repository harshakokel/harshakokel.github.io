<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>relational on Harsha Kokel</title>
    <link>https://harshakokel.com/tags/relational/</link>
    <description>Recent content in relational on Harsha Kokel</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Original-Theme is licensed with the Creative Commons Attribution 3.0 Unported License</copyright>
    <lastBuildDate>Mon, 13 Apr 2020 16:40:08 +0200</lastBuildDate><atom:link href="https://harshakokel.com/tags/relational/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A simple neural network module for relational reasoning</title>
      <link>https://harshakokel.com/posts/relational-network/</link>
      <pubDate>Mon, 13 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/relational-network/</guid>
      <description>Considering that most of the data is some form of graph, there has been lot of focus on improving neural networks to work with graph data. Amidst this, Santoro et al, paper focuses on neural network&amp;rsquo;s ability of doing relational-reasoning i.e. manipulating structured representations of entities and relations. What separates this paper from the other graph network papers is two things: a) the graph or relation between entities is not provided rather learned and b) the edges between entities can be of different types.</description>
    </item>
    
    <item>
      <title>Relational Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/rrl/</link>
      <pubDate>Sun, 12 Apr 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/rrl/</guid>
      <description>The paper came before the goal-conditioned RL, Multi-task RL or Graph Neural Network literature. Major motivation of this paper is to learn a generalizable policy. Generalization in terms of varying number of objects in the domain (for example, in blocks-world number of blocks can change) or change in the goal state (for example, stack red block on blue block instead if green on yellow).
Authors demonstrate that by using approaches from inductive logic programming literature, first-order policy can be learnt which naturally supports both the generalization discussed above.</description>
    </item>
    
  </channel>
</rss>
