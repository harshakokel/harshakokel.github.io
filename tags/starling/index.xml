<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>starling on Harsha Kokel</title>
    <link>https://harshakokel.com/tags/starling/</link>
    <description>Recent content in starling on Harsha Kokel</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Original-Theme is licensed with the Creative Commons Attribution 3.0 Unported License</copyright>
    <lastBuildDate>Fri, 08 May 2020 16:40:08 +0200</lastBuildDate><atom:link href="https://harshakokel.com/tags/starling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Whom Should I Perform the Lab Test on Next? An Active Feature Elicitation Approach</title>
      <link>https://harshakokel.com/posts/afe/</link>
      <pubDate>Fri, 08 May 2020 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/afe/</guid>
      <description>Motivation For the success of clinical studies, it is important to recruit people with diverse features. Not all the features are readily available when the decision about recruitment is done. Some features such as demographic details are available at no additional cost while other details like the MRI Image which are costly can be elicitated if the patient is recruited. But since these costly features are not already available during the decision of recruitment, we need a principled approach to make the decision of recruitment in the absence of costly features.</description>
    </item>
    
    <item>
      <title>Active Advice Seeking for Inverse Reinforcement Learning</title>
      <link>https://harshakokel.com/posts/active-advice-seeking/</link>
      <pubDate>Sat, 05 Oct 2019 16:40:08 +0200</pubDate>
      
      <guid>https://harshakokel.com/posts/active-advice-seeking/</guid>
      <description>In Kunapali et al 2013, authors present a way of incorporating advice in Inverse Reinforcement Learning (IRL) by extending IRL formulation to include constraints based on the expert&amp;rsquo;s advice of preferred and avoided actions, state and reward. Odom et al 2017 expands on Kunapali et al (2013)&amp;rsquo;s formulation of preferred and avoided actions by seeking the advice in active learning setting. The paper proposes a active advice-seeking framework, where instead of seeking mere-label from the expert for selected example as done in active learning, they seek advice (set of preferred and avoided labels) over set of examples.</description>
    </item>
    
  </channel>
</rss>
