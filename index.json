[
{
	"uri": "https://harshakokel.com/tags/abstractions/",
	"title": "abstractions",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/abstraction-for-planning/",
	"title": "Automatically Generating Abstractions for Planning",
	"tags": ["#planning", "#abstractions", "#hierarchy"],
	"description": "  An effective hierarchical decomposition of a problem would solve a task at lower level without violating the conditions in more abstract/higher levels of the hierarchy. Knoblock (1994) formalizises this intuition as ordered monotonicity property. This post briefly explains that property and describes how to learn the hierarchy using the sufficient condition for that property.",
	"content": "A problem space is defined as $\\Sigma = \\langle L, S, O \\rangle$, consisting of $L$ is set of first-order literals, $S$ is the set of finite states (described using literals), and $O$ is the set of operators in the domain. The authors propose to assign a $Level(l), \\forall l \\in L$, which indicates the hierarchy-level of the literal. Level 0 is the complete ground state and the i \u0026gt; 0 is an abstraction. Any plan at abstraction level $i$ can only access literals with level $i$ or higher.\nAn ordered monotonicity property says:\n For any abstract plan $\\Pi^i$ at level $i$ with operators $\\alpha$, the order of operator will remain same for plan at level $i-1$. $$\\forall \\alpha, \\alpha^{\\prime} \\in \\Pi^{i}, \\alpha \\preceq \\alpha^{\\prime} \\implies \\forall c(\\alpha), c(\\alpha^{\\prime}) \\in \\Pi^{i-1} c(\\alpha) \\preceq c(\\alpha^{\\prime})$$ Any addition of an operator at level $i-1$ is allowed only if that operator achieves some precondition $p$ for some other operator which existed at level $i$, and $Level(p)=i-1$. $$\\exists \\beta \\in \\Pi^{i-1}, \\beta \\neq c(x) , \\forall x \\in \\Pi^{i} \\implies \\exists \\alpha \\in \\Pi^{i}, \\exists p \\in P_{\\alpha}, p \\in E_{\\beta}, Level(p)=i-1$$ If any operator at $i-1$ changes a literal with level $i$, then that operator should exist at level $i$. $$\\exists \\beta \\in \\Pi^{i-1}, p \\in e_{\\beta}, Level(p) \\geq i \\implies \\exists x \\in \\Pi^{i}, \\beta = c(x) $$  This heuristic property of hierarchy is motivated from the below observation.\n An effective partitioning of a problem requires that the subproblems can be solved without violating the conditions that were already achieved in the more abstract levels of the abstraction hierarchy. In other words, a hierarchical planner ideally\u000cfinds a solution at one level and then maintains the structure of that solution while the remaining parts of a solution are\u000cfilled in.\n Authors note that following are some sufficient condition for the ordered monotonicity property:\n All the relevant effects (that are required for goal or for some preconditions) have equal or higher level than other effects. All the relevant effects have equal or higher level than the preconditions.  $$ \\begin{aligned} \\forall \\alpha \\in O, \\forall e, e^{\\prime} \\in E_{\\alpha}, \\forall p \\in P_{\\alpha}, e \\in Relevant \\implies Level(e) \\geq Level(e^{\\prime}) \\\\ \\wedge \\ Level(e) \\geq Level(p) \\\\ \\end{aligned} $$\nAlthough these conditions are not necessary, they are sufficient to ensure the ordered monotonicity property. Hence given the problem and description of the domain, the literals can be organized in an topological order. This order is an abstraction hierarchy.\n"
},
{
	"uri": "https://harshakokel.com/",
	"title": "Harsha Kokel",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/hierarchy/",
	"title": "hierarchy",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/planning/",
	"title": "planning",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/",
	"title": "Posts",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/coursework/",
	"title": "coursework",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/logical-nn/",
	"title": "Logical Neural Network",
	"tags": ["#neurosymbolic", "#coursework"],
	"description": "  Ryan Riegel, et al. (arxiv 2020) proposes to build Neural Network by adding one neuron for each logical gate and literal in a logic formulae and hence building a neural framework for logical inference. This article reviews their work. It was written jointly with Siwen Yan, as part of the course on NeuroSymbolic systems by Prof. Sriraam Natarajan.",
	"content": "Ryan Riegel, et al. (arxiv 2020) proposes Logical Neural Network (LNN), a neural framework to perform logical inference. They propose to build a neural network with 1-to-1 correspondence with logical formulae. So, every neuron in the LNN is either a logical literal or logical gate. Given set of logical formulae, a LNN is a graph with one neuron for every unique proposition occurring in any formula and one neuron for each logical operation occurring in each formula, as shown in the figure below. Each neuron outputs a lower and upper bound on the truth values of the corresponding sub-formulae or proposition. A logical neural accepts the output of their corresponding neurons and propositional neurons accepts the bound on propositions' truth value.\nsrc: Ryan Riegel, et al. (arxiv 2020)\n For purpose of demonstrating the contributions authors use proposition formulae in the paper and extend it to FOL in the appendix. Hence, the class presentation focused on propositional formulae only. It is long known that a neuron can be treated as logical gate and hence a neural network can approximate any boolean formula. Authors claim that although this fact is known, it has not been explored much. For example, KBANN1 uses the symbolic knowledge to create an initial neural network but then the parameters of the neural network are learned from examples, so the notion of neuron being the logical gate is lost. Literature from differential ILP like Tensor Log2, which also uses symbolic knowledge/clauses to build the structure of neural network, do not use neurons as logical gates. Perhaps one exception is the CIL2P3 work of Garcez.\nSince the truth values can be any value between [0,1], the choice of activation function for the logical operators must implement real-valued logic. There can be multiple ways of achieving that using importance weighting, this paper proposes weighted nonlinear logic using Ñukasiewicz-like logic. Other types of real-valued logic functions can be used and the framework is able to accommodate that. The activation function defined using the weighted generalization of the Ñukasiewicz logic still follows the logic properties and the DeMorgan\u0026rsquo;s laws. Augmenting NN with FOL paper4 also uses Łukasiewicz logic as activation function in the augmented network. The difference is that paper does not consider real-valued logic and hence does not have upper or lower bounds. It also does not use weighted generalization. This weighted generalization of logic is the main contribution of the paper.\nInference is performed using the iterative upward and downward pass of the network and learning is strictly restricted to the parameter learning since the structure is built using formulae. Hence the LNN model retains the interpretability of the original logical formulae in the network.\nLNN is empirically evaluated on 3 benchmark datasets: Smokers and friends, Lehigh University Benchmark (LUBM), and Thousands of Problems for Theorem Proving (TPTP). These experiments show that the LNN performs comparative to the LTNs (Logic Tensor Networks) and better than MLNs (Markov Logic Networks). LNN is better able to handle the contradictions and is the only neural model that was able to solve any common sense reasoning problem of TPTP. This clearly demonstrates the power of LNN as neural theorem prover.\nThe use of many-valued logic in AI or ML has been scarce. It has mainly been used in the philosophical studies and its usefulness in the field of AI is unexplored, except for the use in fuzzy logic. Perhaps this paper opens door for use of many valued logic in neuro-symbolic studies.\nStrengths  The differentiable way of using weighted nonlinear logic for activation function has huge potential. First neural theorem prover which doesn\u0026rsquo;t need vector embeddings The learnt model remains interpretable since representation is disentangled from neural parameters, as against Neural Theorem Prover5. The compositionality or modularity of the network structure can potentially enable transfer Opens door for use of many-valued logic in neural setting. It enables the open-world assumption by probabilistic bounds, yielding resilience to incomplete knowledge  Drawbacks  Needs handcrafting of all the rules upfront, no structure learning Does not support equality and functions Still needs grounding LNN should be compared with Neural Theorem Prover5, since they both are theorem provers. Paper is dense and difficult to read  Potential improvements  Enhance LNN with structure learning ability (rule induction)  References  ^G. G. Towell and J. W. Shavlik. Knowledge-based artificial neural networks. Artificial intelligence 1994. ^W. W. Cohen. Tensorlog: A differentiable deductive database, 2016. ^A. S. d. Garcez and G. Zaverucha. The connectionist inductive learning and logic programming system, Applied Intelligence 1999. ^Tao Li, Vivek Srikumar, Augmenting Neural Networks with First-order Logic, ACL 2019 ^Tim Rocktäschel and Sebastian Riedel. End-to-end differentiable proving, NeurIPS, 2017.  "
},
{
	"uri": "https://harshakokel.com/tags/neurosymbolic/",
	"title": "neurosymbolic",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/nn-with-fol/",
	"title": "Augmenting Neural Networks with First-order Logic",
	"tags": ["#neurosymbolic", "#coursework"],
	"description": "  Declarative knowledge, first-order rules are used in ILP (a lot) to reduce dependency on the data. Since deep neural network are data hungry, can we use some first-order rules and reduce their data requirement? This post reviews the work by Tao and Srikumar (ACL 2019) which attempts to answer this research question.",
	"content": "Tao and Srikumar, ACL 2019, addresses the problem of incorporating declarative knowledge into a Neural Network. They propose converting the (easily available) first-order logic representation of the knowledge into a network and provide a framework to augment this network to any neural network of choice. The main motivation to use the declarative knowledge as an inductive bias is to reduce the dependency on the data, to achieve comparative performance with less examples.\nTo convert the FOL rules to a network, each predicate in the rule is mapped to a named neuron. For example, given a rule $A_1 \\wedge A_2 \\rightarrow B_1$, the network will have 3 named neurons: $a_1, a_2,$ and $b_1$ with arrow from $a_1$ and $a_2$ to $b_1$. The Łukasiewicz T-norm and T-conorm are used as functions for the logical operators, inspired by probabilistic soft logic literature. Auxiliary variables and auxiliary named neurons are included as needed to compute logical operations. For example, $(\\lnot A \\vee B) \\wedge (C \\vee D)$ is converted to $P \\wedge Q$ with $(\\lnot A \\vee B) \\leftrightarrow P$ and $(C \\vee D) \\leftrightarrow Q$. The benefit of using Łukasiewicz functions is that they are differentiable. This network doesn\u0026rsquo;t have any parameters hence do not require any learning.\nTo ensure that the network is acyclic, the authors recommend using contrapositive statements when needed. For example, if the rule $B_1 \\rightarrow A_1$ is introducing cycle in the network, then use its contrapositive equivalent $\\lnot A_1 \\rightarrow \\lnot B_1$ instead.\nThis rule network is added as constraint to some layer $y=g(\\mathbf{Wx})$ of the original neural network. The constrained neural layer is defined as follows with hyperparameter $\\rho$ handling the importance factor.\n$$y = g(\\mathbf{Wx} + \\rho \\underbrace{d(\\mathbf{z})}_{knowledge})$$\nAuthors empirically evaluate their proposed augmented NN for three tasks: machine comprehension, natural language inference, and text chunking. In each of these tasks the augmentation is performed at different layers. In machine comprehension task, where the use BiDAF as the base neural network, the constrained augmentation is done for attention nodes. In natural language inference task, they use L-DAtt as the base method and augment attention node as well as label nodes. In the text chunking task, they augment the label layer. These experiments confirm their hypothesis that using the knowledge improve the performance, but only when the data is less. With more data, the augmented knowledge does not improve performance significantly.\nCritique  The framework of augmenting NN proposed is very general and hence can be potentially used in any task where deep neural networks are used. I haven\u0026rsquo;t quite understood the emphasis on the differentiability of the augmented network since there are no parameters to be learnt there. The hyperparameter $\\rho$ is tuned. The right hand side of the rule looks pretty limited. The rules used in the experiments are also very simple. In the text chunking task we would assume that the bidirectional LSTM would be able to learn rules $C_{1:4}$. It is not clear from experiments which rule improves the results in this task.  "
},
{
	"uri": "https://harshakokel.com/tags/summary/",
	"title": "summary",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/neurosymbolic-systems/",
	"title": "Types of Neuro-Symbolic Systems",
	"tags": ["#neurosymbolic", "#summary"],
	"description": "  I attended the AAAI 2020 conference in NY, and one of the most influencing talk in that conference (for me, of course!) was the address by Prof. Henry Kautz on The Third AI Summer. In that talk, he provided some taxonomy for the future Neural and Symbolic approaches. This article is my attempt to summarize that taxonomy.",
	"content": "Prof. Henry Kautz, in his Robert S. Engelmore Memorial Lecture at AAAI 2020^, talked about the past and present of AI and highlighted that the future of AI is in the combination of the Neural and Symbolic approaches, alluding to the famous and heated AI debate between Prof. Gary Marcus and Prof. Yoshua Benjio. In this regard, he brings forth a taxonomy of Neuro-Symbolic Systems that I aim to elaborate upon.\n1. symbolic Neuro symbolic This is the current standard operating procedure of deep learning, where any/all the symbols in the problem are converted to vector embeddings. Vectors are then processed by neural models, which spits out another vector which is then converted to the required symbol.\nsymbolic Neuro symbolic\n Most of the NLP systems fall under this category since words are converted to vectors before the neural manipulations are conducted. Personally, I would not call this a Neuro-Symbolic integration.\n2. Symbolic[Neuro] This is an over all symbolic solver which uses Neural model internally as subroutine for one or more function.\nSymbolic[Neuro]\n The example provided by Prof. Kautz includes Alpha Go system which uses a symbolic Monte-Carlo Tree search algorithm while using a neural state estimator.\n3. Neuro;Symbolic This is a more refined integration of Neural and Symbolic approaches where the Neural and Symbolic systems are leveraged for different tasks in a big pipeline. Both systems communicate with each other either to extract information or to improve the individual/collective systems performance.\nNeuro;Symbolic\n The Neuro Symbolic Concept Learner by Mao et al.^ is presented as an example of this type of Neuro-Symbolic integrations since the Neural model does object detection and the symbolic reasoner (which is a GNN) does the relational reasoning. I am not quite comfortable with categorizing the GNN as symbolic systems. So, I would not consider the Mao et al. as Type 3.\nI believe the Illanes et al. ICAPS 2020^ and other related work are a better candidate for Type 3 Neuro-Symbolic integration. Illanes et al. integrate the symbolic planner with the neural RL agent and both systems feed off of each other.\n4. Neuro:Symbolic $\\rightarrow$ Neuro This type is categorized as Neuro-Symbolic systems where the symbolic knowledge is compiled into the structure of Neural models. Kautz and Lamb et al. both provide example of Lample and Charton 2020, but I fail to understand how Lample et al. is compiling the symbolic knowledge into the neural model. However, the other examples provided by Lamb et al. are quite easy to follow. Arabshahi et al. compiles the symbolic expression tree to tree LSTMs. Other works like Hitzler et al. 2004^ and Garcez et al. 2015^ compiled the logic program or if..then.. rules to a Neural Network architecture.\nneuro.png\" Neuro:Symbolic $\\rightarrow$ Neuro\n 5. NeuroSymbolic This category was not listed during the AAAI 2020 Lecture, but added in Prof. Kautz' slides later^. This category covers all the approached where the first-order logic language is tensorized and neural methods are used to perform reasoning over this tensorized first-order logic representation. The research in this category include Logic Tensor Network [Serafini et al. 2016^], Tensor Product Representation [Smolensky et al. 2016^], Neural Tensor Network [Socher et al. 2013^], relational embeddings etc.\nNeuroSymbolic  The difference between this and type 1 is that in type 1 only the objects are represented as vector embeddings. Here complete first-order language is represented by tensor embedding, including object, relation, clauses/rules, functions, predicates.\n6. Neuro[Symbolic] Finally, the last one is the transpose of Type 2. Here, the overall Neural model performs symbolic reasoning by either learning the relations between the symbols or paying attention to selected symbols at certain point. This is exactly what the Graph Neural Network do. So I think GNN is synonymous to Type 6 neuro-symbolic integration. This is also the argument made by Lamb et al. 2020^.\nNeuro[Symbolic]  I prefer calling the GNN as type 6 Neuro-Symbolic integration then calling GNN as symbolic reasoners. I believe symbolic reasoners, in general, have more reasoning capability than GNN. So, calling GNN as symbolic reasoner in my opinion is a stretch.\nReferences  Robert S. Engelmore Memorial Lecture at AAAI 2020 by Henry Kautz. The Third AI Summer Lamb et al. Graph Neural Networks Meet Neural-Symbolic Computing: A Survey and Perspective. IJCAI, 2020 Arabshahi et al. Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs.\tICLR, 2018 Hitzler et al. Logic programs and connectionist networks. Journal of Applied Logic, 2004 Garcez et al. Neural-symbolic learning and reasoning: contributions and challenges. AAAI Spring Symposium Series. 2015 Smolensky et al. Basic Reasoning with Tensor Product Representations. arXiv:1601.02745, 2016 Serafini et al. Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge. arXiv:1606.04422, 2016 Socher et al. Reasoning With Neural Tensor Networks For Knowledge Base Completion. NeurIPS, 2013 Illanes et al. Symbolic Plans as High-Level Instructions for Reinforcement Learning. ICAPS 2020 Mao et al. The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. ICLR 2019  "
},
{
	"uri": "https://harshakokel.com/tags/causal/",
	"title": "causal",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/causal-tools/",
	"title": "Tools for Causal Inference",
	"tags": ["#causal", "#summary"],
	"description": "  I read the Book of Why last year and recently in the reading group at Starling lab we read the 7 Tools of causal inference by Prof. Pearl. I was a little taken by how much I have forgotten about causal inference. So I found the need to jolt down my understanding, so I can refer to it later. This article summarized my current understanding of the tools presented in the paper, based on the paper and the book.",
	"content": "There is enough motivation now showing the necessity of learning models that have a correct causal structure. The famous \u0026ldquo;Correlation is not causation\u0026rdquo; quote should ring a bell. Although the research on learning causal structure from observed data has not yet shown its potential. There are certain tasks from causal literature which can still be solved by just using observed data. Specifically, interventional and counterfactual queries can still be answered if we know the causal structure. And, to do that we need the following tools of causal inference popularized by Prof. Pearl.\nTool 1. Encoding causal assumptions: Transparency and testability. Tool 1 emphasize the importance of graphical representation of causality using a Bayesian Network. By representing the causal influences using BN, we can leverage the three rules of conditional independences for inference.\n Although very powerful, the BN representation is limited. Only with graphical representation we will not be able to differentiate between $P(y|X=3)$ and $P(y|do(X=3))$. Ferenc Huszár gives a beautiful explanation of the difference between the two in his post on causal inference 1. Given different causal BN and data generated from different distributions as shown in the figure below, we can see that the joint distribution of $x$ any $y$ is identical. Even the conditional probability $P(y|X=3)$ is identical.\nsrc: Causal Inference by Ferenc Husz\u0026aacute;r\n But, what if we force the variable $x$ to take a fixed value $3$ in each of these cases, i.e. intervene on variable $x$? Should we still expect the distribution of y to be same? No. As expected the impact of intervening on variable $x$ has different impact on values of $y$. This is evident in the two distribution plotted below.\nsrc: Causal Inference by Ferenc Husz\u0026aacute;r\n Above values of $p(y|do(X=3))$ is computed by generating the data after forcing $x=3$. In machine learning, we want to learn the impact of assigning value $x=3$ without re-generating the data. So, how do we compute $p(y|do(X=3))$ without generating the data? We need more advanced tools for that.\nTool 2. Do-calculus and the control of confounding. In the above example, intervening on variable $x$ and fixing it to $3$ is like making $x$ independent of all its ancestor. So, we remove all the incoming edges to $X$. Now, Computing $P(y|X=3)$ of the resulting graph will give us $P(y|do(X=3))$ of the original graph.\nsrc: Causal Inference by Ferenc Husz\u0026aacute;r\n This is what do-calculus is. The process of mapping the do-expressions to the standard conditional probability equations using the three rules. The probability equations we get at the end are called estimand and the the value obtained after computing the estimand on the observed data is called estimate.\n One of the most common adjustment in the causal inference is de-confounding i.e. adjusting for all the common causes (confounders) of two variables. This is the famous backdoor criterion. Back-door path for $X \\rightarrow Y$ is any path that ends in $Y$ and has arrow pointing into $X$. To de-confound $X$ and $Y$, we have to block all the back-door path by adjusting for (i.e. conditioning on) variables which either follow chain-rule or common ancestor rule. We have to be very careful to not condition on any common descendent in the back-door path.\n For graphs where we are unable to block the back-door path (say, because of unobservable variable), do-calculus gives us a work-around. This is commonly called the front-door criterion.\n For graph given above, the front-door formula is derived as follows:\n$$ \\begin{aligned} P(Y |do(X= \\tilde{x})) \u0026amp; = \\sum_z P(Y|do(X = \\tilde{x}), Z=z) P(Z=z|do(X = \\tilde{x})) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\rhd \\text{probability axiom} \\\\ \u0026amp; = \\sum_z P(Y|do(X= \\tilde{x}),do(Z=z)) P(Z=z|do(X= \\tilde{x})) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\rhd \\text{ back door-path between X and Y is blocked by X,} \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{so we apply exchange do-rule } \\\\ \u0026amp; = \\sum_z P(Y|do(X= \\tilde{x}),do(Z=z)) P(Z=z|X= \\tilde{x}) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\rhd \\text{exchange do-rule, back door-path between X and Z } \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{ is blocked by common descendent Y} \\\\ \u0026amp; = \\sum_z P(Y|do(Z=z)) P(Z=z|X= \\tilde{x}) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\rhd \\text{action rule, No forward path from X to Y } \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{ as do(Z) blocks it.} \\\\ \u0026amp; = \\sum_z \\left( \\sum_x P(Y|X=x, Z=z) P(X=x) \\right) P(Z=z|X= \\tilde{x}) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\rhd \\text{using back-door adjustment } \\\\ \u0026amp; = \\sum_z P(Z=z|X= \\tilde{x}) \\sum_x P(Y|X=x, Z=z) P(X=x) \\\\ \u0026amp; \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\rhd \\text{same as front-door criterion } \\\\ \\end{aligned} $$\nIts amazing, to compute the $P(Y | do(X= \\tilde{x}))$, we have to sum over all the possible assignments of $x$. With only Tool 1, I would have mostly computed probability by only looking at data points where $X = \\tilde{x}$.\n $$ P(Y|do(X)) = \\sum_w \\sum_z P(y |do(X),W=w,Z=z)P(Z=z|do(X), W=w) P(W=w |do(X)) $$ Since, W and Z blocks backdoor between X \u0026 Y Since, W blocks the backdoor between X \u0026 Z, using exchange do-rule we see $$ P(Z=z|do(X),W=w) = P (Z=z|X,W=w) $$ Since, back-door for $X \\rightarrow W$ us blocked by Z, exchange do-rule gives $$ P(W=w|do(X)) = \\sum_{z^{\\prime}} P(W=w|X, Z=z^{\\prime}) P(Z=z^{\\prime}) $$ -- Tool 3. The algorithmitization of counterfactuals. Thumb rule of counterfactual counterfactuals talks about a specific individual/event/scenario.  \nCounterfactual queries are questions about a parallel universe where one(or few) feature(s) is(are) different for a specific example, not on the whole population. It is a question always asked in retrospection for a specific instance. For example, An interventional question to analyze importance of wearing mask during current pandemic can be asked as \u0026ldquo;What would be the total number of COVID-19 positive patients, if we banned mask in public?\u0026rdquo; i.e $P($ COVID_Pos $| , do($ wear_mask $= 0) )$. But a counterfactual question would be \u0026ldquo;Given that I had worn mask when I went to buy grocery last week and I do not have COVID-19 today, what would have happened if I had not worn mask?\u0026rdquo; This question is about me, and about a specific instance of not wearing mask. This counterfactual question has to account for every variable which is not a descendent of wear_mask staying exactly the same. So, the fact that I drove to the TomThumb or the cart which I used should stay the same, because in my assumed causal graph, they are not descendants of wear_mask variable. But on the other hand the fact that I washed my hands and face for exactly 12 secs after coming home, and that I entered the spice aisle might change. Because in my causal assumption these are descendent of wearing mask. Hey! I would have washed hands and face for 20 secs if I was not wearing mask and I would have definitely not entered the spice aisle which had like 3 people in there. I would have maintained six feet distance.\nSo, mathematically the counterfactual query is $P(h_c^*=1|h_m^\\ast=0,h_c=0, h_m=1,..)$, where, $h_c$ is Harsha has COVID-19, $h_m$ is Harsha wore mask, and \\ast indicates counterfactual universe. So, all the variable that are not descendants of $h_m$ will stay same i.e $h_i = h_i^\\ast, \\forall i \\notin Des(h_m)$. To compute the probability of the counterfactual query, we need more than the do-calculus from Tool-2. We need structural equation models (SEMs) and an algorithm.\n I strongly recommend Ferenc Huszár\u0026amp;rsquo;s post on counterfactuals for more clarity on how the interventional probability $P(Y| do(X))$ is the computation on population, but counterfactuals is for individuals.\n Let\u0026rsquo;s see how we use tool 3 to answer the counterfactual query, \u0026ldquo;What would Alice’s salary be if she had a college degree?\u0026rdquo;. This query is from the fictitious employee data reproduced below from The Book of Why. Here, the experience is mentioned in number of years and three levels of education are : 0 = high school degree, 1 = college degree, 2 = graduate degree. The assumed causal graph is also shown below.\nsrc: The Book of Why\n The SEM for this graph will be:\n$$ f_0(Ed) = , U_{0} \\\\ f_1(Ex) = b_1 , + , a_1 Ed , + , U_{1} \\\\ f_2(S) = b_2 , + , a_2 Ed , + a_2 Ex , + , U_{2} $$\nHere, U are unobserved idiosyncratic variables which capture the peculiarity of the individual in question. So, these values are different for each user. For instance, We can think of $U_2$ accounting for difference in salary as some artifact of individuals unobserved characteristics which is independent of level of education and experience (say, time management). In terms of graphical model, we can think of these variables as parents of the variable on the left side of the equation. Note that these idiosyncratic variables do not have incoming arrows. So, given the counterfactual explanation above, these values should remain unchanged in the parallel universe. If we were to do linear regression we would have an $\\varepsilon$ instead of $U$ to adjust for the noise. However, the noise is then a random parameter which is not necessarily kept constant across universe. This is where the SEM differ from linear regression.\nThe SEM coefficients can be estimated from the observed data, much like linear regression.\n$$ f_1(Ex) = 10 – 4 Ed + U_1 \\\\ f_2(S) = 65,000 + 5,000 Ed + 2,500 Ex + U_2 $$\nNow, the algorithm to compute the counterfactual query is these three steps:\n Following these steps for Alice here, in step 1 we find that $U_1(Alice) = , –4$ and $U_2(Alice) = $1,000$. We do not actually care for $U_0(Alice)$ because in step 2 we set $f_0(Ed(Alice)) = 1$. 1 for college degree, and remove all incoming arrows to $Ed$ and perform do-calculus adjustments. Here, since there are no causal parents of $Ed$, our SEM model equations $f_1$ and $f_2$ do not change. Then, in step 3, we estimate the counterfactual parameters, $Ex^\\ast(Alice) = 2$ and $Ed^\\ast(Alice) = 76000$.\nRest of the tools\u0026hellip;. to be added later.\nCritique I certainly believe that the advent of ML and AI necessitates the task of learning the causal structure from observed data. The three tools mentioned above do not do that. They assume the causal structure is provided. These tools provide us the correct way of utilizing the causal structure in machine learning. I believe most of the machine learning models build for classification/regression tasks assume some form of causal structure already. What is not clear is, if those models correctly leverage the causal structure that they are assuming. These tools help us answer that. The question about whether or not the causal structure assumed is correct or not can be subjective and task specific. The three tools mentioned above certainly do not answer that.\nReferences  The Book of Why: The New Science of Cause and Effect by Judea Pearl and Dana Mackenzie The Seven Tools of Causal Inference, with Reflections on Machine Learning by Judea Pearl 1https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/ Causality: Models, Reasoning and Inference by Judea Pearl  "
},
{
	"uri": "https://harshakokel.com/posts/afe/",
	"title": "Active Feature Elicitation",
	"tags": ["#active-learning", "#starling", "#healthcare"],
	"description": "  Natarjan et al., IJCAI 2018 is one of the most ebullient papers from the Starling Lab (in my opinion, of course!). It formalizes a unique problem setting called Active Feature Elicitation. The task here is to select the best set of examples on whom the missing features can be queried actively. This blog post summarizes my understanding of that paper.",
	"content": "Motivation For the success of clinical studies, it is important to recruit people with diverse features. Not all the features are readily available when the decision about recruitment is done. Some features such as demographic details are available at no additional cost while other details like the MRI Image which are costly can be elicitated if the patient is recruited. But since these costly features are not already available during the decision of recruitment, we need a principled approach to make the decision of recruitment in the absence of costly features.\nObjective The objective of this paper is to define the active feature elicitation problem and provide an approach to this problem.\n Active feature elicitation is the problem setting where the goal is to select the best set of examples on whom the missing features can be queried on to best improve the classifier performance.\nThe top part shows the part of the data that is fully observed ($\\mathbf{E}_{o}$). The bottom left quadrant shows the observed features($\\mathbf{X}_{o}$) of the potential cohorts and the right quadrant is the data that needs to be collected ($\\mathbf{X}_{u}$)for the most useful potential recruits. Given, the labels of the potential recruits ($y$), the goal is to identify the most informative cohorts that would aid the study($\\mathbf{E}_{a} \\subset \\mathbf{E}_{u}$).\n Summary First, let us look at some baseline approaches.\nRandom ($RND$): The most common approach to recruit cohort from potential candidates is to randomly pick the patients.\nUncertainty Sampling:\na) observed feature only ($USobs$):\nAn informed approach is to look at all the observed features \u0026ndash; $\\mathbf{X}_{o}$ for all the examples $\\mathbf{E} = \\mathbf{E}_{o} \\cup \\mathbf{E}_{u}$ and learn a classifier that predicts the label ($y$) given only observed features. Now, the potient recruitment subset $\\mathbf{E}_{a}$ can be obtained by picking $n$ most uncertain examples from this classifier.\nb) all features ($USAll$): More informed approach will be to impute the unobserved features \u0026ndash; $\\mathbf{X}_{u}$ using mean, mode or some strategy, learn a classifier and sample the subset $\\mathbf{E}_{a}$ which has maximum prediction uncertainty.\nSampling based on the maximum prediction uncertainty means we recruit the candidates who have the highest entropy i.e. probability distribution [0.5,0.5] for binary classification.\nAlthough this makes sense in an active learning setting, where we want to obtain a label for example which has maximum uncertainty. In the current setting of active feature elicitation, where we want to recruit the most diverse cohort, the entropy does not make sense. The authors argue that we should look for the most diverse prediction distribution and propose the following approach.\nFirst, build two classifiers:\n  $M_o = P(y | \\mathbf{X}_{o})$ which predicts class label based on only the observed features . Train it on all the examples $\\mathbf{E} = \\mathbf{E}_{o} \\cup \\mathbf{E}_{u}$.\n  $M_u = P(y | \\mathbf{X}_{o}, \\mathbf{X}_{u})$ which predicts class label based on all the features. Train it only on example set $\\mathbf{E}_{o}$\n  Since both the models are trained to achieve same prediction for the set $\\mathbf{E}_{o}$, we assume that the probability distribution of both the model is similar of set $\\mathbf{E}_{o}$.\n$$ P\\left(y^{j} | \\mathbf{X}_{o}^{j}, \\mathbf{X}_{u}^{j}\\right) \\approx P\\left(y^{j} | \\mathbf{X}_{o}^{j}\\right) \\quad \\forall i \\in \\mathbf{E}_{o} $$\nNow, we want to recruit candidates from set $\\mathbf{E}_{u}$ which are different than the existing candidate set $\\mathbf{E}_{o}$. If we use the probability distribution $P(y_j | \\mathbf{X}_{o}^j, \\mathbf{X}_{u}^j)$ as a representative of each example in the set $\\mathbf{E}_{o}$, we want to find example $i$ which is at maximum distance from $\\mathbf{E}_{o}$. Since, we do not have $P(y_i | \\mathbf{X}_{o}^i, \\mathbf{X}_{u}^i)$, authors propose to use $P(y_i | \\mathbf{X}_{o}^i)$ to represent examples from set $\\mathbf{E}_{u}$\nNow, we can use any divergence/distance measure to compute the farthest example. For example, if we use the KL divergence,\n$$ D_{i j}=\\mathrm{KL}\\left(\\mathrm{P}\\left(y^{i} | \\mathbf{X}_{o}^{i}\\right) | \\mathrm{P}\\left(y^{j} | \\mathbf{X}_{o}^{j}, \\mathbf{X}_{u}^{j}\\right)\\right) $$\nwe can compute the mean distance for each example $i \\in \\mathbf{E}_{u}$:\n$$ \\mathrm{MD}_{i}=\\frac{1}{\\left|\\mathbf{E}_{o}\\right|} \\sum_{j=1}^{\\left|\\mathbf{E}_{o}\\right|} D_{i j} $$\nand recruit the $n$ examples with maximum $MD_i$\nThe proposed approach is agnostic of any classifier and distance measure. Experiments show that the recruitment done using this approach was more informative.\n"
},
{
	"uri": "https://harshakokel.com/tags/active-learning/",
	"title": "active-learning",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/healthcare/",
	"title": "healthcare",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/starling/",
	"title": "starling",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/adversary/",
	"title": "adversary",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/meta-attack-gnn/",
	"title": "Attacking GNN with Meta Learning",
	"tags": ["#GNN", "#adversary", "#coursework"],
	"description": "  This article reviews a very exciting ICLR 2019 paper: Adversarial Attacks on Graph Neural Networks via Meta Learning. This was originally written as part of a class assignment at UT dallas.",
	"content": "Daniel Zügner and Stephan Günnemann, ICLR 2019 highlights the weakness of Graph Neural Networks. Since the iid assumption does not hold in the graph data, any perturbation at a single node might have major impacts. This weakness can be leveraged by adversaries to attack GNN. The paper is set in a transductive learning setting where a graph is given along with label to some of the nodes and the task is to predict labels of the remaining nodes. The objective of the adversarial attack is to reduce the overall performance of the model by modifying the training data. So the objective can be formulated as a max-min problem where the attacker wants modify the graph to maximize the loss while the training agent will learn parameters to minimize the loss.\n$$ \\underset{{G}}{\\arg\\max } \\ \\ \\underset{\\theta}{\\arg\\min} \\ \\mathcal{L}_{\\text{train}} \\left(f_{\\theta}({G})\\right) , $$\nThis can be reformulated as following optimization function:\n$$ \\underset{\\hat{G} \\in \\Phi(G)}{\\arg \\min} \\ {\\mathcal{L}_{\\mathrm{atk}}}\\left(f_{\\theta^{\\ast}}(\\hat{G})\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg \\min } \\quad \\mathcal{L}_{\\operatorname{train}}\\left(f_{\\theta}(\\hat{G})\\right) $$\nIdeally $\\mathcal{L}_{\\mathrm{atk}} = - \\mathcal{L}_{\\mathrm{train}}$ but authors also use another option $\\mathcal{L}_{\\mathrm{atk}} = - \\mathcal{L}_{\\mathrm{self}}$ where they learn a model to predict the labels of the unlabeled dataset and then try to maximize the prediction error for those nodes. Since the above mentioned objective is a bilevel optimization problem it is a difficult to solve and hence authors propose a meta-learning approach.\nMeta-Learning Recall that in MAML we saw a similar objective where we wanted to optimize parameter $\\theta$ such that the loss on individual tasks $\\mathcal{T}_i$ is also minimum when adapted from parameter $\\theta$:\n$$ \\underset{\\theta}{\\arg \\min} \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) \\quad \\text { s.t. } \\theta_{i}^{\\prime} = \\underset{\\theta_{i}^{\\prime} }{\\arg \\min } \\quad \\mathcal{L}_{\\operatorname{\\mathcal{T}_{i}}}\\left(f_{\\theta}\\right) $$\nBelow is a quick review of MAML equations. Map the colors with the image visualize to the equations.\n simulated adaptation / learning / training $(\\nabla_{\\theta}\\mathcal{L}_{T_{i}})$ update performed by the meta-learner is as follows:\n$$\\theta_{i}^{\\prime}=\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{T_{i}}\\left(f_{\\theta}\\right)$$\n meta objective is $\\theta_{i}^{\\prime}$ should be close to $\\theta_i^{\\ast}$\n$$\\underset{\\theta}{\\arg \\min} \\ \\sum_{T_{i}} \\mathcal{L}_{T_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right)$$\n $\\therefore$ meta update $\\left(\\nabla \\mathcal{L}_{\\text {meta }}\\right)$ is:\n$$ \\begin{aligned} \\theta \u0026amp; = \\theta-\\beta \\sum_{T_{i}} \\nabla_{\\theta} \\mathcal{L}_{T_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) \\\\ \u0026amp; = \\theta- \\beta\\sum_{T_{i}} \\nabla_{\\theta_{i}^{\\prime}} \\mathcal{L}_{T_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) \\cdot \\nabla_{\\theta} \\theta_{i}^{\\prime} \\end{aligned} $$\n first order approximation assumes $\\nabla_{\\theta} \\theta_{i}^{\\prime}=1$\n$$=\\theta-\\beta \\sum_{T_{i}} \\mathbin{\\color{#EA6B2D}{\\nabla_{\\theta_{i}^{\\prime}}\\mathcal{L}_{T_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right)}}$$\n Adversarial Meta-Learning In MAML both the meta objective and the adaptation/learning phase optimized over the same parameters. In this paper, the adaptation/learning phase optimizes model parameters but the meta-objective optimizes the graph $G$. So the equations look quite similar with meta objective optimizing $G$ instead of $\\theta$.\nAdversarial objective is\n$$\\underset{\\hat{G}}{\\arg\\min } \\ \\mathcal{L}_{\\text{atk}}\\left(f_{\\theta^{\\ast}}(\\hat{G})\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg\\min} \\ \\mathcal{L}_{\\text {train }}\\left(f_{\\theta}(\\hat{G})\\right)$$\nsimulated adaptation / learning / training update by the attacker is\n$$ \\theta =\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\text{train}} f_{\\theta}(G) $$\n meta update:\n$$ \\begin{aligned} G \u0026amp; = G -\\beta \\nabla_{G} \\mathcal{L}_{\\text{atk}}\\left(f_{\\theta}(G)\\right) \\\\ \u0026amp; =G-\\beta \\nabla_{f} \\mathcal{L}_{\\text{atk}}\\left(f_{\\theta}(G)\\right) \\bigg[ \\nabla_{G} f_{\\theta}(G)+\\nabla_{\\theta} f_{\\theta}(G) \\nabla_{G} \\theta \\bigg] \\end{aligned} $$\n first order approximation $\\theta \\approx \\tilde{\\theta}$ where $\\left(\\nabla_{G} \\tilde{\\theta}=0\\right)$\n$$ =G-\\beta \\nabla_{f} \\mathcal{L}_{\\text{atk}}\\left(f_{\\tilde{\\theta}}(G)\\right) \\nabla_{G} f_{\\tilde{\\theta}}(G) $$\n Another major difference between MAML and this paper is the assumption in the first order approximation of the meta-gradients. In MAML, the first order approximations assumes $\\nabla_{\\theta}\\theta_i^{\\prime} = 1$ i.e. it assumes the parameters $\\theta$ and $\\theta^{\\prime}$ are essentially the same. Here, in the first order approximation, authors assume that the $\\nabla_{\\tilde{\\theta}}G= 0$ i.e. the graph $G$ is constant (i.e. independent of $\\tilde{\\theta}$).\nIn the exact meta-attack version, the graph $G$ used in the simulated learning/adaptation/training phase is constantly optimized by the attacker w.r.t parameters and hence the $\\nabla_{{\\theta}}G \\neq 0$. But for the approximate version, the graph $G$ is optimized only after $t$ steps of simulated adaptation/training/learning iterations. So, the for most of the $\\tilde{\\theta}$ updates the graph $G$ is gonna be constant. Hence, the parameter $\\nabla_{\\tilde{\\theta}}G = 0$.\nSince, the graph modifications are limited to edge manipulations, the optimization objective replaces graph $G$ with adjacency matrix $A$. And since the gradient direction is the direction of maximizing the function and we want to maximize the $L_\\text{train/self}$, we use +ve sign for gradient update.\n$$ A = A - \\beta \\ \\nabla_{A}\\mathcal{L}_\\text{atk}(f_\\theta(A)) \\\\ = A + \\beta \\ \\nabla_{A}\\mathcal{L}_\\text{train/self}(f_\\theta(A)) \\\\ $$\nGraph admissibility: $\\Phi(G)$ Since the inherent objective of an attack is be unnoticeable there are certain constraints on modifications that the attacker can do on the graph $\\hat{G} \\in \\Phi(G)$\n There is a budget on the number of perturbations allowed. So, $||A−\\hat{A}||_0 \\leq \\Delta$, where $\\Delta$ is a budget and $A$ and $\\hat{A}$ are adjacency matrix of original and modified graph $G$ Nodes which was initially connected should remain connected, so no singleton node created as a result of perturbations Degree distribution of the graph remains the same.  Extension to graph attributes To extend the meta learning formulation to modify the graph attributes we can treat the node feature matrix of the graph $X$ as hyper-parameter and reformulate the attack objective as below.\n$$ \\underset{{X}}{\\arg \\min} \\ {\\mathcal{L}_{\\mathrm{atk}}}\\left(f_{\\theta^{\\ast}}({X}, A)\\right) \\quad \\text { s.t. } \\quad \\theta^{\\ast}=\\underset{\\theta}{\\arg \\min } \\quad \\mathcal{L}_{\\operatorname{train}}\\left(f_{\\theta}({X}, A)\\right) $$\nSo, the meta gradient equation will be as follows:\n$$ X = X + \\beta \\ \\nabla_{X}\\mathcal{L}_\\text{train/self}(f_\\theta(X, A)) $$\nEmpirical evaluations Experiments show that the proposed method is indeed able to reduce the classification accuracy of the model from around 82% to 60% by making change in 15% of the edges (Fig 1). Interesting insight is in table 3 is that if parameters $W$ trained with a clean graph $A$ are used on the modified graph $\\hat{A}$, it is still able to achieve 83% accuracy on the perturbed graph. But, if the parameters $\\hat{W}$ are trained on the perturbed graph $\\hat{A}$ the accuracy on the clean as well as perturbed graph is reduced significantly.\n The analysis of perturbed graphs reveals that the majority of the perturbations in the graph are edge insertions (table 5). Yet, the mean shortest path of the adversarial graph is higher than the original graph. This might mean that the edges which are removed in the perturbations were some of the key connections.\n Critique The paper brings forth a novel application of meta-learning in the bilevel optimization problems and demonstrates a successfully use case of adversarial attacks. They show both the exact and approximate formulations and their results. The approach was successful in reducing the classification accuracy.\nThe attacker here is making an assumption about the learning algorithm that will be used for classification, which might not be true in general. In meta-learning since the parameters are shared between the meta-learning and the actual classification model, the assumption on the learning algorithm is valid. However, here the attacker is modifying the training data before the classifier is learned and a different entity is gonna learn classifier. I feel the assumption of the attacker is not justified.\nIt would be interesting to see the classification accuracy of different GNN models on the $A$ and $\\hat{A}$, other than the ones which were used while attacking.\nQuestions Q1. How is min max problem $\\operatorname{max}_x\\operatorname{min}_\\theta f(x, \\theta)$ solved by this approach? The problem $\\operatorname{max}_x\\operatorname{min}_\\theta f(x, \\theta)$ is solved using meta-gradients by replacing the $\\operatorname{min}_\\theta$ with meta optimization step, $\\theta^\\ast$.\nQ2. List types of attacks and summarize it one line and discuss differences  Targeted Attacks: Attacks that are aimed to change prediction of a single example Global Attack: Attacks which aim at changing the overall performance of the model Evasion Attacks: Exploratory attacks done during test time for e.g. to understand the thresholds of a classifier. Poisoning Attacks: Causative attacks done during training to mess up the learning process.  Q3. How is the linear surrogate model obtained from two layer GCN? Replacing the $\\sigma$ activation function with an Identity function $I$, the non-linearity is removed and we can achieve a linear function.\n$$ \\begin{aligned} f_{\\theta}(A, X) \u0026amp; =\\sigma\\left(\\hat{A} \\sigma \\left(\\hat{A} X W_1 \\right) W_2 \\right) \\\\ \u0026amp; = \\mathbf{I}\\left(\\hat{A} \\mathbf{I} \\left(\\hat{A} X W_1 \\right) W_2 \\right) \\\\ \u0026amp; = \\left(\\hat{A}^2 X W \\right) \\\\ \\text{where, } W \u0026amp; = W_1W_2 \\\\ \\end{aligned} $$\nQ4. How is the following equation derived from the MAML paper? $$\\nabla_{A}^{\\text {meta }}= \\nabla_{f} \\mathcal{L}_{\\text {atk }}\\left(f_{\\tilde{\\theta}_{T}}(A)\\right) \\cdot \\nabla_{A} f_{\\tilde{\\theta}_{T}}(A)$$\nThis equation is derived using the first order approximations discussed above.\nQ5. What is the difference between $\\theta_t$ and $\\tilde{\\theta}_t$? The difference between $\\theta_t$ and $\\tilde{\\theta}_t$ is the notion of constant $G$ vs dynamic $G$ as explained above.\nQ6. How can meta-gradient problem formulation be modified to attack node features. Mentioned above in extension to graph attributes section\nQ7/8. Summarize the impact and analysis of the attack Mentioned above in Emprical evaluations section\n"
},
{
	"uri": "https://harshakokel.com/tags/gnn/",
	"title": "GNN",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/skills-to-symbols/",
	"title": "Learning Symbolic Representations for planning",
	"tags": ["#planning", "#abstractions", "#RL"],
	"description": "  In the pursuit of learning planner from data, I ended up reading Konidaris et al. (JAIR 2018). Getting through this paper was an onerous task. Which I would not like to do again. So, here are my notes on the key concepts from that paper, which are relevant for learning high-level, abstract planner.",
	"content": "This paper learns abstract symbolic representations from lower level trajectories for planning at a high-level. Big Idea of this paper is that given different domains of increasing difficulty at lower level but similar high level tasks, if we are able to segregate the low-level and high-level tasks, the tasks can be considered equivalent at higher level and hence can be solved in a uniform manner.\n For example, given three tasks as shown above:\n A 5 x 5 grid world were agent has to take the key and open the lock or Same task for 10 x 10 grid or Robot running around the room trying to unlock a cupboard in continuous state and action space.  If we are given low level skills in each of the domain like navigating to a location and picking up the key or opening a lock, then high level plan in all these domains is equivalent. So, how to learn the abstractions which can help us plan at higher level? And ensure that the plan generated from such abstractions are executable with certainty?\nTo build a high level abstract planner, they use the options framework at the low-level. Operators in planning is equivalent to the options, precondition of an operator can be seen as initiation set and the effects of the operator can be seen as termination condition.\n  Since operators in planning only impact subset of variables and leave others unchange, this can be seen as factored MDP.\n A plan is feasible if it can be executed and it is satisfiable if it is feasible and reaches goal state. To ensure plan is feasible and satisfiable, we need to learn precondition, effect and remainder for each abstract operator. Precondition indicates the necessary conditions for taking action, effect indicates the changes in the state because of taking action, and remainder indicates variables which are unaffected by the action.\n I appreciate the details authors provide to prove the above intuition. May be I will add the proof here later.\n To infer these three things, authors prove that learning following classifier for each abstract action is sufficient:\n $precondition(X,o)$ \u0026ndash; a classifier that indicates if option $o$ can be performed in the state $X$. This is equivalent to initiation set $I_o$. $effects(X,o)$ \u0026ndash; a classifier that indicates if state $X$ belongs to effect set of the option $o$ $mask(o)$ \u0026ndash; a set of variables that are modified by option $o$  From the above three functions, we can infer the remainder and image set of option $o$ as follows:\n Image $Im(X,o)$ – set of state obtained after executing option $o$ in state set $X$\n$Remainder(X,o)$ – possible states with same values as $X$ for all variables except $mask(o)$\n $$ Remainder(X,o) = Project(X, mask(o))$$ $$Im(X,o) = Effects(X,o) \\cap Remainder(X,o)$$\nThus we can learn PDDL operators directly from the trajectories and leverage the rich planning literature to perform high level planning in RL tasks.\nReferences George Konidaris, Leslie Pack Kaelbling, Tomas Lozano-Perez, From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning, JAIR 2018\n"
},
{
	"uri": "https://harshakokel.com/tags/rl/",
	"title": "RL",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/drrl/",
	"title": "Deep Relational RL",
	"tags": ["#RL", "#GNN", "#neurosymbolic", "#relational"],
	"description": "  Relational RL has not made a lot of splashes in real-world because it is easier to write a planner than learn a relational RL agent. This might be about to change with the current achievements of the graph based relational reasoning approaches. This article summarizes my understanding of the pioneering work of Vinicius Zambaldi et al. (ICLR 2019) on Deep Relational RL.",
	"content": "Deep RL methods have been every effective but they have poor generalization capability, especially combinatorial generalization (for eg. if the number of blocks are changed in the blocks world). Recent advances in graph network literature have achieved combinatorial generalization by learning neural network that can reason about relationship of various nodes in graphs. Since this reasoning happens pairwise, the algorithms are able to scale to varying number of objects.\nIn this paper, authors introduce how multi-headed dot product attention can be used to perform relational reasoning in model-free deep RL and hence achieve combinatorial generalization.\nMulti-Head Dot Product Attention (MHDPA) This is the self attention mechanism proposed in the paper Vaswani et al. NeurIPS 2017, Attention is all you need. In that paper, the MHDPA was used on an input of word embeddings but in general it can be any form of entities. Check out the neat explanation of MHDPA by Jay Alammar here\nOn a very high level, attention mechanism\n converts these entities ($X$) to Queries ($Q$), Keys ($K$) and Values ($V$) 1, computes the similarity score between each query and key $QK^{T}$, scales and normalizes it to a distribution: $\\operatorname{Softmax}(\\frac{Q.K^{T}}{\\sqrt{d}})$. outputs the weighted values based on this distribution: $Z = \\operatorname{Softmax}(\\frac{Q.K^{T}}{\\sqrt{d}})\\cdot V$  Multi headed version of attention does two additional steps\n Concatenates all the attention outputs $(\\mathbin\\Vert_i Z_i)$ Transform it original $X$ dimension by multiplying it with weight matrix $W$  src: The Illustrated Transformer by Jay Alammar  Summary Zambaldi et al. proposes to use the MHDPA (with image embeddings as entities) to perform relational-reasoning while training a network for distributed A2C model. First the images from the box-world domain are processed through a convolutional neural network in the \u0026ldquo;input module\u0026rdquo;. The spatial representation learnt from the CNN is then used as embedding after concatenating $x$ and $y$ co-ordinate as additional features. MHDPA is used to perform manipulations between this entities a.k.a. relational-reasoning. Finally the multiple attention heads are aggregated by another multi-layered-perceptron $g_\\theta$ (instead of the weight matrix $W$ used in Vaswani et al. 2017). Then in output module, max-pooling is performed and a FC layer converts it to actor policy $\\pi$ and critic\u0026rsquo;s state-value (or advantage value) $B$.\n  Authors mention that the use of a $g_\\theta$, a non-linear MLP, in the final stage is aligned with the use of MLP in relational-network paper where a MLP is used to manipulate the relation embeddings.\n Qualitative Analysis of the attention heads show that they infact learn lock-key relationship and also a relationship between agent and entities.\n References  Illustrated Transformer 1What exactly are keys, queries, and values in attention mechanisms?  The idea behind Actor-Critics and how A2C and A3C improve them  "
},
{
	"uri": "https://harshakokel.com/tags/relational/",
	"title": "relational",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/relational-network/",
	"title": "Relational Network",
	"tags": ["#VQA", "#relational", "#GNN"],
	"description": "  Overview of Adam Santoro et al. (NeurIPS 2017).",
	"content": "Considering that most of the data is some form of graph, there has been lot of focus on improving neural networks to work with graph data. Amidst this, Santoro et al, paper focuses on neural network\u0026rsquo;s ability of doing relational-reasoning i.e. manipulating structured representations of entities and relations. What separates this paper from the other graph network papers is two things: a) the graph or relation between entities is not provided rather learned and b) the edges between entities can be of different types. Most graph network papers that learn edges focus on approximating a distance metric between entities. This paper instead focuses on learning relations between entities.\nSummary The paper proposes a computational block which they call relational network (RN), which takes a set of object ($O$) as input and outputs a vector. The main computational unit in RN are functions: $f_\\phi$ and $g_\\theta$, which are both Multi Layered Perceptrons. $g_\\theta$ approximates the relation between each pair of object and $f_\\phi$ performs the reasoning over these entities.\n$$ \\operatorname{RN}(O)=f_{\\phi}\\left(\\sum_{i, j} g_{\\theta}\\left(o_{i}, o_{j}\\right)\\right) $$\nTo achieve combinatorial generalization, i.e. be able to use $f_\\phi$ over varying number of objects, authors use the sum of $g_\\theta$ as input to $f_\\phi$. So, the the input dimension of $f_\\phi$ is equal to the output dimension of the $g_\\theta$, which are both constant and independent of order or number of objects in the input.\n RN can also be thought of as a MLP with parameter typing for first few hidden layers, which is equivalent to $g_\\theta$.\n Making the input and output dimensions independent of the number of objects has a big advantage in terms of data efficiency. In standard MLP, when the number of objects increase, the input dimension may increase and hence the number of parameters would also increase.\nThe whole RN network is end-to-end differentiable and hence trainable by back propagation.\nRN for VQA Authors show the utility of RN on Visual Question Answering task of CLEVR dataset. In CLEVR, a model needs to reason about relations between different objects in the image and then answer the question.\n  In the figure above, even though the question \u0026lsquo;\u0026lsquo;What is the size of the brown sphere?\u0026rsquo;\u0026rsquo; is shown as non-relational, if the answer of this question is going to be \u0026lsquo;small\u0026rsquo;, \u0026lsquo;medium\u0026rsquo; or \u0026lsquo;large\u0026rsquo;, I would consider it as a relational question. Because the size is relative. On other hand, if the answer is \u0026lsquo;2 cm in diameter\u0026rsquo; it is non-relational. I strongly believe the dataset is aiming for the former answer.\n Input image is processed through a CNN to obtain object embedding. Input questions are processed though LSTM to obtain question embedding. The proposed $g_\\theta$ function is then modified to predict the relationship between objects in context of the question asked: $g_\\theta(o_i, o_j, q)$. $Here, q_\\theta$ generates a fixed length vector which are aggregated and forwarded to $f_\\phi$ which outputs softmax over all the possible answers.\n RN\u0026rsquo;s success in Sort-of-CLEVR dataset show that it is able to do better relational reasoning then MLP.\nCritique In my opinion since the embedding of the relation i.e. output of $g_\\theta$ are not evaluated here, the claim of RN being able to do relational reasoning is not accurate. Experiments clearly show the benefit of the CNN+RN over CNN+MLP but that just means RN is better at fitting the curve.\nI do not quite understand what entails relational reasoning and how can one purely test that ability.\nReferences  Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap, A simple neural network module for relational reasoning, NeurIPS 2017 https://rasmusbergpalm.github.io/recurrent-relational-networks/ https://medium.com/apache-mxnet/relation-networks-for-visual-question-answering-using-mxnet-gluon-f029fde8f863 https://medium.com/intuitionmachine/intuitive-relational-reasoning-for-deep-learning-3ae164f9f5cd  "
},
{
	"uri": "https://harshakokel.com/tags/vqa/",
	"title": "VQA",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/rrl/",
	"title": "Relational Reinforcement Learning",
	"tags": ["#RL", "#relational", "#SRL"],
	"description": "  My notes on Džeroski, Sašo, Luc De Raedt, and Kurt Driessens, Machine Learning 2001",
	"content": "The paper came before the goal-conditioned RL, Multi-task RL or Graph Neural Network literature. Major motivation of this paper is to learn a generalizable policy. Generalization in terms of varying number of objects in the domain (for example, in blocks-world number of blocks can change) or change in the goal state (for example, stack red block on blue block instead if green on yellow).\nAuthors demonstrate that by using approaches from inductive logic programming literature, first-order policy can be learnt which naturally supports both the generalization discussed above.\nIn particular, author propose to learn Q-Tree i.e. TILDE-RT (Top-down Induction of Logical decision trees for regression) as Q-Function which take the state and action pair and predict q values. The policy function, P-Tree, can then be induced from Q-Tree.\nFollowing Q-RRL algorithm is proposed to learn Q-Tree, which updates the TILDE-RT after every episode. Data set used to learn the TILDE-RT is generated by exploring the environment and P-RRL algorithm is proposed for inducing P-Tree from Q-Tree\n  Critique  Proposed solution does not seem to scale well specifically because the Logical programs do not scale with higher number of data points or high dimensional data. The relational trees might be able to generalize to various number of blocks but I think it will not generalize to different goals. For e.g. if all the training examples had goal(on(.,.)) and if the test examples have goal(clear(.)), I do not think the TILDE-RT will be able to achieve that goal. With Graph Neural Network and goal-conditioned RL, both the generalizations targeted by Q-RRL are achieved in a scalable manner. So, the only additional benefit RRL really has is the use of domain knowledge, which comes from ILP.  "
},
{
	"uri": "https://harshakokel.com/tags/srl/",
	"title": "SRL",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/few-shot/",
	"title": "few-shot",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/few-shot-learning-gnn/",
	"title": "Few-Shot Learning with GNN",
	"tags": ["#few-shot", "#GNN", "#coursework"],
	"description": "  My notes on Victor Garcia, Joan Bruna, ICLR 2018. Written as part of the Complex Networks course by Prof. Feng Chen.",
	"content": "This paper focuses on $q$-shot $K$-way classification problem \u0026ndash; where we have $K$ class labels and for each class label we have $q$ example images, so totally we have $s=qK$ training images. Authors propose to leverage the progress in Graph Convolutional Networks by formulating this problem as a node classification problem in graph $(G=(V,E))$, where nodes are images and an edge between two nodes indicate those two images are similar and may have same labels.\nGiven dataset $\\{\\left(\\mathcal{T}_{i}, y_{i}\\right)\\}_{i\\leq L}$ containing Task $\\mathcal{T}_i$ and true label $y_{i} \\in\\{1, K\\}$ for a single test image $\\bar{x}$.\n$$ \\mathcal{T}=\\left\\{ \\underbrace{ \\left\\{\\left(x_{1}, l_{1}\\right), \\ldots\\left(x_{s}, l_{s}\\right)\\right\\}}_{\\text{supervised samples}}, \\underbrace{\\left\\{\\bar{x}\\right\\}}_{\\text{test samples}} ; \\right\\} $$\nEmbeddings$-\\phi(.)$ are obtained from a convolutional neural network (CNN) for all the images ($\\{x_i\\}_1^s \\cup \\bar{x}$), as highlighted in the image below.\n These embeddings are then concatenated with one-hot encoding of the labels $(h(l))$. Together they form nodes $(V)$ of the graph.\n$\\mathbf{x}_{i}^{(0)}=\\left(\\phi\\left(x_{i}\\right), h\\left(l_{i}\\right)\\right)$\n  For $k$ labels, $h(l)$ is a binary vector of size k. One-hot encoding is obtained for training image $x_i$ with the label $l_i=3$, by setting all the values in $h(l_i)$ to $0$ except for $3^{rd}$ element which is $1$ i.e. $[0,0,1,0,0\u0026hellip;]$. For test image, since the label is not known in $\\mathcal{T}$, uniform distribution is used instead of one hot encoding i.e. $h(l) = k^{-1}\\mathbf{1}_k$, for $\\bar{x}$.\n  superscript $^{(p)}$ indicate the $(p+1)^{th}$ layer input. So, $x^{(0)}$ indicates the node embeddings for first layer in GNN.\n Edges $(E)$ of the graph are learnt as adjacency matrix $\\tilde{A}$ using a MLP.\n$$ \\tilde{A}_{i,j} = \\varphi_{\\tilde{\\theta}}\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)= \\operatorname{MLP}_{\\tilde{\\theta}}\\bigg( \\operatorname{abs}\\Big( \\phi(x_i) - \\phi(x_j) \\Big) \\bigg) $$\nThen Graph Convolution ($Gc(.)$) is performed on the nodes $V$ and adjacency matrix $A$. .\n$$ \\mathbf{x}_{l}^{(k+1)}=\\operatorname{Gc}\\left(\\mathbf{x}^{(k)}\\right)=\\rho\\left(\\sum_{B \\in \\mathcal{A}} B \\mathbf{x}^{(k)} \\theta_{B, l}^{(k)}\\right) \\\\ \\mathcal{A} = \\{\\tilde{A}^{(k)}, \\mathbf{1}\\} $$\nProposed block of GNN containing the Adjacency matrix and Graph Convolution.\n The block containing learning of adjacency matrix and graph convolution is repeated multiple times in the experiments and output of the last layer is passed through a sigmoid activation function to obtain class probabilities .\n The complete network, including the adjacency matrix is trained end-to-end with following objective .\n$$ \\min _{\\Theta} \\frac{1}{L} \\sum_{i \\leq L} \\underbrace{\\ell\\left(\\Phi\\left(\\mathcal{T}_{i} ; \\Theta\\right), Y_{i}\\right)}_{\\text{loss w.r.t. test samples}}+\\underbrace{\\mathcal{R}(\\Theta)}_{\\text{regularizer}} \\\\ \\ell(\\Phi(\\mathcal{T} ; \\Theta), Y)= \\underbrace{-\\sum_{k} y_{k} \\log P\\left(Y=y_{k} | \\mathcal{T}\\right)}_{\\text{cross entropy loss}} $$\n Note that the labels/predictions of the training data are not used to compute the loss. Hence the GNN will not overfit to the $h(l)$ part of the embedding.\n The over all idea of finding image embeddings and then using distance between the image embeddings to find the class label is explored in various other papers. Authors show equivalence of their approach to three other papers.\n1. Convolutional Siamese Neural Network Koch et al. 2015 proposed to use Convolutional Siamese Neural Network (CSNN) to learn similarity between two images, i.e probability that both images were drawn from same label set.\nThey used two parallel twin CNN to obtain embeddings to two input images $f_{\\theta}(x)$, computed absolute distance between these embeddings (d), and then computed probability by passing the distance from linear feedforward layer and sigmoid function.\n CSNN are trained to reduce the following loss function for all pairs of images.\n$$ \\mathcal{L}(B)=\\sum_{\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}, y_{i}, y_{j}\\right) \\in B} \\mathbf{1}_{y_{i}=y_{j}} \\log p\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)+\\left(1-\\mathbf{1}_{y_{i}=y_{j}}\\right) \\log \\left(1-p\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)\\right) $$\nWhile testing, the test image ($\\mathbf{x}$) is compared with all the training images ($S$) and 1 Nearest-Neighbour approach is used to assign the class label.\n$$ \\hat{c}_{S}(\\mathbf{x})=c\\left(\\arg \\max _{\\mathbf{x}_{i} \\in S} P\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right)\\right) $$\nEquivalence to GNN\nGNN approach is similar to CSNN if we make following changes:\n Restrict node embeddings:  $$\\operatorname{GNN}: \\mathbf{x}_{i}^{(0)}=\\left(\\phi\\left(x_{i}\\right), h\\left(l_{i}\\right)\\right)$$ $$\\operatorname{CSNN}: \\mathbf{x}_{i}^{(0)}=\\phi\\left(x_{i}\\right) = f_{\\theta}(x_i)$$\nFix Adjacency matrix:  $$\\operatorname{GNN}: \\tilde{A}_{i,j} = \\operatorname{MLP}_{\\tilde{\\theta}}\\bigg( \\operatorname{abs}\\Big( \\phi(x_i) - \\phi(x_j) \\Big) \\bigg)$$ $$\\operatorname{CSNN}: \\tilde{A}_{i,j} = \\operatorname{softmax}\\bigg( - || \\phi(x_i) - \\phi(x_j) || \\bigg)$$\nReduce the convolution block:  $$\\operatorname{GNN}: y = \\sigma \\Big( \\sum_{B \\in A} B \\mathbf{x}^{(k)} \\theta_{B,l}^{(k)} \\Big)$$ $$\\operatorname{CSNN}: Y_{\\ast} = \\sum_{j} \\tilde{A}_{\\ast, j}^{(0)}\\left\\langle\\mathbf{x}_{j}^{(0)}, u\\right\\rangle$$\nwhere, $\\langle .,. \\rangle$ indicates elementwise multiplication and $u$ is a binary vector for selection of labels. So, it is 1 only for elements which correspond to labels in $\\mathbf{x}$ and 0 otherwise. $\\{\\ast\\}$ indicates the test example. .\n2. Matching Network Vinyals et al., 2016 extended the SCNN approach of $2$-way comparison to $k$-way comparison by using an attention function $a(.,.)$ to compute cosine similarity of test image embedding $f_{\\theta}(\\mathbf{x})$ with input image embeddings $g_{\\theta}(\\mathbf{x_i})$ and use it as a weighted sum to compute the class probability of the test image.\n $$ c_{S}(\\mathbf{x})= P(y | \\mathbf{x}, S)=\\sum_{i=1}^{k} a\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right) y_{i}, \\text { where } S=\\left\\{\\left(\\mathbf{x}_{i}, y_{i}\\right)\\right\\}_{i=1}^{k} a\\left(\\mathbf{x}, \\mathbf{x}_{i}\\right)= \\frac{\\exp \\left(\\operatorname{cosine}\\left(f(\\mathbf{x}), g\\left(\\mathbf{x}_{i}\\right)\\right)\\right.}{\\sum_{j=1}^{k} \\exp \\left(\\operatorname{cosine}\\left(f(\\mathbf{x}), g\\left(\\mathbf{x}_{j}\\right)\\right)\\right.} $$\n Note here $k$ is the number of examples not the number of class labels.\n Parameters $\\theta$ are trained to maximize the loglikelihood of the test images.\n$$ \\theta=\\arg \\max _{\\theta} E_{L \\sim T}\\left[E_{S \\sim L, B \\sim L}\\left[\\sum_{(x, y) \\in B} \\log P_{\\theta}(y | x, S)\\right]\\right] $$\nEquivalence to GNN\nGNN approach can be considered equivalent to Matching Network if $f = g$ and $\\mathbf{x}_{i}^{(0)}=\\phi\\left(x_{i}\\right) = f_{\\theta}(x_i)$ and $A_{i,j} = a(\\mathbf{x_i, x_j})$.\n2. Prototypical Network Snell et al. 2017 reduced the number of comparisons in Matching Network by computing a class representative for each class and instead of comparing with all available images, the test image is only compared with the class representatives.\n This paper first computes embeddings of all images $f_{\\theta}(x_i)$ and then computes a class representative/prototype $\\mathbf{v}_c$ for each class as mean of those embeddings.\n$$ \\mathbf{v}_{c}=\\frac{1}{\\left|S_{c}\\right|} \\sum_{\\left(\\mathbf{x}_{i}, y_{i}\\right) \\in S_{c}} f_{\\theta}\\left(\\mathbf{x}_{i}\\right) $$\nThe class probability distribution of test example is then computed by taking softmax of the distance between the embedding of test image and the class prototype. The distance measure used here is squared euclidean distance.\n$$ P(y=c | \\mathbf{x})=\\operatorname{softmax}\\left(-d_{\\varphi}\\left(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_{c}\\right)\\right)=\\frac{\\exp \\left(-d_{\\varphi}\\left(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_{c}\\right)\\right)}{\\sum_{c^{\\prime} \\in \\mathcal{C}} \\exp \\left(-d_{\\varphi}\\left(f_{\\theta}(\\mathbf{x}), \\mathbf{v}_{c^{\\prime}}\\right)\\right)} $$\nThe network is trained to reduce the negative loglikelihood.\n$$\\mathcal{L}(\\theta)=-\\log P_{\\theta}(y=c | \\mathbf{x})$$\nEquivalence to GNN\nGNN approach can be reduced to Prototypical Network if we make following changes.\n  Restrict the node embeddings to $\\mathbf{x}_{i}^{(0)}=\\phi\\left(x_{i}\\right) = f_{\\theta}(x_i)$\n  Fix the Adjacency Metric:\n  $$ \\tilde{A}_{i, j}^{(0)}=\\left\\{\\begin{array}{cc} q^{-1} \u0026amp; \\text {if } l_{i}=l_{j} \\\\ 0 \u0026amp; \\text{ otherwise } \\end{array}\\right. $$\n q is the number of examples in each class.\n Replace the convolution block by:  $$\\hat{Y}_{\\ast}=\\sum_{j} \\tilde{A}_{\\ast, j}^{(0)}\\left\\langle\\mathbf{x}_{j}^{(0)}, u\\right\\rangle$$\nExtentions Paper also proposed a way to extend the few-shot learning setting to semi-supervised learning and active learning setting. Although straight forward I do not see the intuition clear enough or experiments strong enough to suggest that those setting have any added benefit on reducing uncertainty of the model.\nCritique Although the paper is a fascinating read since it connect various approaches of few-shot / one-shot learning with graph-convolutional network. I find the experiments very weak. Especially since authors only use 1 test example per task. Lack of intuition for the attention mechanism for active learning setting is also discouraging.\nQuestions  How to decide $l$? Link to Answer   I believe, the label $l$ were randomly sampled from the dataset for each task.\n Explain how are parameters $\\tilde{\\theta}$ trained? Link to Answer How to compute the adjacency matrix for each layer of GNN? Link to Answer   Question 2 and 3 are equivalent, since $A$ is a function of $\\tilde{\\theta}$.\n How to generate one hot encoding in Figure 1. Link to Answer What is $u$? How to generate $u$? Link to Answer What is the meaning of $\\ast$? Link to Answer Explain how prototypers are generated in prototypical networks? Explain how they are equivalent? How to calculate probability $P(Y|\\mathcal{T})$? Link to Answer How is attention vector generated?   Attention vector is also trained.\n What is $Gc(.)$? Link to Answer  References  Tutorial on Meta-Learning by Dr. Lilian Weng Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. ICML deep learning workshop. 2015. Vinyals, Oriol, et al. Matching networks for one shot learning. NeurIPS. 2016. Snell, Jake, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. NeurIPS. 2017. My slides that I presented in class, here  "
},
{
	"uri": "https://harshakokel.com/tags/ilp/",
	"title": "ILP",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/relational-inductive-bias/",
	"title": "Relational Inductive Bias",
	"tags": ["#GNN", "#ILP"],
	"description": "  There is a recent surge in papers which are using Relational Inductive Bias with Deep Reinforcement Learning. So here is my investigation on what is it and how is this connected to the inductive biases used in Logic.",
	"content": "(Inductive) Bias refers to any basis for choosing one generalization over another, other than strict consistency with the observed training instances [Mitchell 80]. Relational inductive bias refers to biases which impose constraints on relationships and interactions among entities. Logic Programming extensively deals with the entities and relationships. So, it seems to me there should be some equivalence between the biases used in ILP with the Relational Inductive biases.\nBias in ILP  \u0026hellip; bias is anything which influences how the concept learner draws inductive inferences based on the evidence. There are two fundamentally different forms of bias: declarative bias, which defines the space of hypotheses to be considered by the learner, i.e., what to search, and preference bias, which determines how to search that space, which hypotheses to focus on, and which ones to prune, etc.\n\u0026hellip;\nILP systems distinguish two kinds of declarative bias: syntactic bias (sometimes also called language bias) and semantic bias. Syntactic bias imposes restrictions on the form (syntax) of clauses allowed in hypothesis. \u0026hellip; Semantic bias imposes restrictions on the meaning, or the behavior of hypotheses.\n\u0026ndash; Muggleton and De Raedt, 1994\n Another typology of biases include following categorizations\n Language Bias - specifies set of syntactically acceptable clauses. Search Bias - specifies which region of hypothesis set should be searched. Validation Bias - specifies when the search should stop.  Language Bias Some straight forward syntactic constraints can be maximum length of clause, number of new variables that can be introduced in the clause, number of function operators used in horn clauses, etc. All of these are form of language-bias.\nMore evolved language biases include\n  specify the set of clauses allowed in hypotheses\neach clause in this set can further contain set of literals which are allowed $\\quad$ e.g, $\\operatorname{father}(X, Y) \\leftarrow { \\operatorname{male}(X), \\operatorname{female}(X) }, \\operatorname{parent}(X, Y);$\n  second-order schemata\nSecond-order schemata defines a language bias as the set of all clauses that can be obtained by instantiating a second-order schema with a second-order substitution.\nA second-order schema is a clause, where some of the predicate names are (existentially quantified) predicate variables.\n$\\quad$ e.g., $S = \\exists p, q, r : p(X, Y) \\leftarrow q(X, XW), q(YW, Y), r(XW, YW);$\nA second-order substitution is a substitution that replaces predicate-variables by predicate names.\n$\\quad$ e.g., $\\theta = { p /\\operatorname{connected}, q/\\operatorname{part-oj}, r/\\operatorname{touches}}$\ngives\n$\\quad$ $S\\theta = \\operatorname{connected(X, Y)} \\leftarrow \\operatorname{part-of}(X,XW), \\operatorname{part-ox}(YW, Y), \\operatorname{touches}(XW, YW)$\n  Search Bias Search bias can be a restriction or preference. With restriction, some hypothesis space is ignored. While with preference, some space is prioritized while searching. Search bias is given by $types$ and $modes$ in ILP. This can also be semantic bias\nOne important search/semantic bias used in most ILP systems is notion of determinate clause. \u0026ldquo;A clause is determinate if all of its literals are determinate; and a literal is determinate if each of its variables that does not appear in preceding literals has only one possible binding given the bindings of its variables that appear in preceding literals.\u0026quot; (from Muggleton and De Raedt, 94).\nSearch biases can be broadly categorized as:\n Ordering\nSpecification of ordering for pre-compiled hypotheses or relations to be considered for learning. Example selection criteria\nHow the examples are selected to verify some candidate hypothesis Coverage function Intermediate validation criteria  Bias in Graph Networks In Graph Networks, inductive biases used include non-relational inductive biases like choice of activation function, dropout, weight decay, training curricula, algorithm optimization etc. Relational inductive bias, on other hand are the biases that are constrain the interactions of entities, relationships and rules while learning the model. Entities/Nodes are elements with attributes, relations/edges are property between entities and rule is a function mapping set of entities and relations to another entity and relation.\nThree main relational inductive bias introduced in graph networks are\n Locality Translation invariance Permutation invariance  For fully-connected (FC) layers, all the entities (nodes in network) are connected to all other entities and there is no restriction on rules that can be used. So, relational-inductive bias is very weak for FC layers.\nFor Convolutional layer, the nodes are the pixels of images and edges are the relation between neighbors. Rules are defined by the weights and biases of the hidden network. One relational-inductive bias in here is constraint of locality. Rules should use related nodes, i.e. convolution function should only use the neighbors, it can not use arbitrary pixels. Another relational-inductive bias used is that the kernel matrix is same for all the neighborhoods, i.e. all the localities are related in same way so rule is translation invariant.\nIn ILP, the locality bias is, perhaps, induced by use of $\\operatorname{mode}$. A $+ve , \\operatorname{mode}$ is preferred in a literal when adding it as candidate to a hypothesis clause. In a sense, we prefer edges/connected nodes from previously selected nodes in the hypothesis clause. The translation invariance bias is, perhaps, integral in ILP since horn clause by definition use $\\forall$ operator.\nIn images, there is a natural ordering of the pixels and hence the nodes are ordered. But in most graphs, nodes do not have a natural ordering, nor does edges. One of the most important contribution of the Graph Neural Networks and Graph Convolutional Networks is to provide a way to operate on nodes in graph which is permutation invariant. So, this becomes the third and the most important relational inductive bias.\nILP does not assume any natural ordering of the entities or relations. Hence there is no need of an equivalent bias.\nConclusion There are many more biases in ILP which are not yet accounted for in Graph Networks and hence if we find a way to include those biases for Graph Networks, we might be able to achieve sample efficient and effective learning in Graph Networks.\nReferences  Inductive Logic Programming: theory and methods, Stephen Muggleton and Luc De Raedt, J. Logic Programming 1994 Claire Nédellec, Céline Rouveirol, Hilde Adé, Francesco Bergadano, and Birgit Tausend. Declarative Bias in ILP, 1996. Tom Mitchell, The need for biases in learning generalizations, 1980. Battaglia et al. Relational inductive biases, deep learning, and graph network, arXiv:1806.01261, 2018  "
},
{
	"uri": "https://harshakokel.com/tags/metalearning/",
	"title": "metalearning",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/maml/",
	"title": "Model-Agnostic Meta-Learning",
	"tags": ["#metalearning", "#coursework"],
	"description": "  My notes on Chelsea Finn, Pieter Abbeel, Sergey Levine, ICML 2017. Written as part of the Complex Networks course by Prof. Feng Chen.",
	"content": "Meta-Learning a.k.a the \u0026lsquo;\u0026lsquo;Learning to Learn\u0026rsquo;\u0026rsquo; problem, is the field of study where the researchers are trying to learn the parts of model which in standard machine learning setting are decided by researchers/humans/users. To elaborate, consider for example a standard gradient based machine learning problem. Given a training data and test data, to solve a problem the researches first decide what loss function to optimize and based on existing literature or their expertise they figure out various meta-information of the model. In the figure below, for a standard gradient based machine learning model meta-information like network structure, initialization parameters ($\\theta^0$), update method etc are all decided manually.\nMeta-learning research aims to learn a model which can help decide such meta-information (all or subset) for any new task.\nsrc: Prof. Hung-yi Lee's slides\n  One of the use-case of meta-learning is in a field called few-shot learning. In few-shot learning, machine learning algorithm is supposed to learn a model for a task from few supervised examples. Meta-Learning can help in few-shot learning by providing a better initialization parameters. Few-shot learning is the problem of learning a model from few examples, meta learning is the problem of learning a model that can easily adapt to the new task from few examples.\nThis is also the premise of the Model-Agnostic Meta-Learning (MAML) paper by Finn et al 2017.\n Transfer-Learning is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem1. For Deep Neural Network models, one of the the popular approaches to transfer-learning is by using a pre-trained model. Pre-trained model essentially transfers the knowledge of network parameters between different tasks. This is essentially equivalent to providing initialization parameters for the new task. Transfer-learning via pre-trained model as well as meta-learning use the network parameters of one model as initialization parameters for another model. The difference is in the optimization of the network parameters. While pre-trained model are optimize for some predefined task, meta-learning model are optimized so that they can adapt to new tasks quickly.\nThe key idea of Model-Agnostic Meta-Learning (MAML) algorithm is to optimize model which can adapt to new task quickly. Consider (pretty similar) tasks ${\\mathcal{T}_i , | , i \\in { 1,2,3,4}}$ with optimal parameters ${ \\theta^{\\star}_i , | , i \\in {1,2,3,4} }$. Say, for $\\mathcal{T}_4$ we have only $k$ supervised examples but we have large number of supervised examples for rest of the tasks i.e. $\\mathcal{T}_1, \\mathcal{T}_2$ and $\\mathcal{T}_3$.\nA transfer learning approach will train $3$ different models (with parameters $\\theta_1, \\theta_2$ and $\\theta_3$). Try all three as pretrained model for $\\mathcal{T}_4$, compare the performance and pick one that works the best i.e. closest to $\\theta_4^{\\star}$.\nMAML on the other hand uses tasks $\\mathcal{T}_1, \\mathcal{T}_2$ and $\\mathcal{T}_3$ for meta-training and treats them same as task $\\mathcal{T}_4$ i.e. only uses $k$ example from each task. MAML learns a single model with parameter $\\theta$ in meta-training such that for each task $\\mathcal{T}_i$ the gradient step using $k$ examples from that parameter $\\theta$ in the direction of $\\theta_i^{\\star}$ reaches a $\\theta^{\\prime}_i$. The meta-training objective to bring $\\theta^{\\prime}_i$ close to $\\theta_i^{\\star}$. So, the next update of the parameter $\\theta$ is a gradient step in a direction calculated as a linear combination of gradient step from $\\theta_i^{\\prime}$ to $\\theta_i^{*}$. This is represented in the figure below, albeit not visibly.\nFinn et al. 2017 ICLR, Figure 1.\n Meta-learning (bold line: —) is performing a search in parameter space such that a gradient step (gray line: →) for any of the training tasks $\\mathcal{T}_i, i\\in {1,2,3}$ is close to optimal parameters $\\theta_i^{\\star}$. The parameter $\\theta$ is then used as initialization value and fine-tuned for a specific task, this is called learning or adaptation (broken line: - - -).\nDuring meta-training, MAML adapts the parameter $\\theta$ for training tasks $\\mathcal{T}_i, i\\in { 1,2,3}$ to compute the $\\theta$ update. In meta-testing, MAML adapts the parameter $\\theta$ for test task $\\mathcal{T}_4$. We obtain $\\theta_4^{\\prime}$ by taking gradient step using the $k$ examples.\nParameter $\\theta_i^{\\prime}$ is computed for any task $\\mathcal{T}_i$ using following fine-tuning/learning/adaptation equation.\n$$ \\theta_{i}^{\\prime}=\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta}\\right) $$\nMeta-learning aims to reduce the distance between $\\theta_i^{\\prime}$ and $\\theta_i^{\\star}$. Since, $\\theta_i^{\\star}$ is unknown it instead tries to minimize $\\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right)$ for all the tasks. So, meta-objective is:\n$$ \\min _{\\theta} \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right)=\\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta}\\right)}\\right) $$\n Note that we restrict our model to minimize the objective of tasks from a distribution $p(\\mathcal{T})$.\n Meta-optimization is hence done with following update equation:\n$$ \\theta \\leftarrow \\theta-\\beta \\nabla_{\\theta} \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) $$\nNotice that the update equation above depends on the gradient of loss function $\\mathcal{L}_{\\mathcal{T}_i}\\left(f_{\\theta_i^{\\prime}}\\right)$, but $\\theta_i^{\\prime}$ depends on the gradient of loss function $\\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta}\\right)$. So evidently MAML involves second level gradients.\nFull algorithm of MAML is quite easy to follow from the above three equations.\nFinn et al. 2017 ICLR, Algorithm 1.\n Instead of doing the search of $\\theta$ for all the tasks in training, like we did in the example above. MAML samples tasks from distribution $p(\\mathcal{T})$.  In theory, this might just be a distribution of task based on available sample size of each task or distribution based on the similarity to the test task. In practice, they randomly sampled label set from images corpus and then sampled few examples for training and few for testing. To update the parameters $\\theta$ in line 8, code computes the second gradient using tensor flow optimizers.\nBelow figure explains the MAML update equation used in practice. The first arrow for each task is the gradient from fine-tuning equation and the second arrow is from the meta-optimization equation.\nAdapted from slides of Prof. Hung-yi Lee.\n MAML vs Pretrained Finn et al, ICLR 2017 Figure 4.\n The above image highlights the difference between MAML and pretrained models for the MAML-RL in 2D Navigation task. While the MAML model can adapt to the new task quickly, the pre-trained models take longer.\nFirst-order MAML Since the MAML involves second-order derivative, it can be computationally expensive. Authors propose a first-order approximation for such scenarios, by omitting the second order derivatives.\nSince,\n$$ \\nabla_{\\theta} \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) = \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) \\\\ = \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\nabla_{\\theta_i^{\\prime}} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}}\\right) \\cdot \\nabla_{\\theta} (\\theta_i^{\\prime}) $$\nIn first-order MAML, authors use $\\nabla_{\\theta} (\\theta_i^{\\prime}) \\approx 1$\nReptile Reptile further simplifies the gradient computation of MAML by proposing following algorithm.\nNichol et al. 2018, Algorithm 1.\n  Notice that the initial parameter $\\theta$ used in MAML is equivalent to $\\phi$ in the reptile algorithm.\n  Instead of computing the $\\theta^{\\prime}_i$ for each task with one step gradient, Reptile computes parameter $W$ by running stochastic gradient descent for $k$ steps. Then instead of computing the gradient w.r.t the task for updating the initial parameter $\\theta$ (as done in line 8 of MAML), Reptile recommend to just shift the initial parameter in the direction of $W$ by using $(W - \\phi)$ as gradient. Below figure explains the Reptile update process.\nAdapted from slides of Prof. Hung-yi Lee.\n  The task in 2D Navigation domain is to direct the agent in the 2D space to a particular goal point. These goal points are randomly sampled for each task. So, for each task $\\mathcal{T}_i$ in 2D domain, the MDP $$ is defined by $S \\in \\mathbb{R}^2$, $A \\in \\mathbb{R}^2$ is velocity in range $[-0.1, 0.1]$, $R$ is the negative squared distance to the goal, and $T$ is deterministic movement between states.  MDP is a set of $$, States, Actions, Transition function, Reward function, discount factor resp. It can also have initial distribution of state. -- References  Definition of transfer learning from Wikipedia ICML 2019 Meta leraning tutorial Prof. Hung-yi Lee\u0026rsquo;s slides on meta learning Reptile paper: Alex Nichol, Joshua Achiam, John Schulman On First-Order Meta-Learning Algorithms, 2018 Reptile Blog Code: Deep Metalearning using “MAML” and “Reptile”  Further resources  Tutorial on Meta-Learning by Dr. Lilian Weng Awesome meta learning lists  "
},
{
	"uri": "https://harshakokel.com/posts/her/",
	"title": "Hindsight Experience Replay",
	"tags": ["#RL"],
	"description": "  My notes on Marcin Andrychowicz et al. NeurIPS 2017.",
	"content": "Remember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.\nIn standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states. Popular solution for this problem is to use reward shaping functions but even they have some unforeseen consequences.\nThis paper highlights that even though the current trajectory $T_i = \u0026lt;s_0, a_0, s_1, a_1, \u0026hellip; s_{t_i}, a_{t_i}\u0026gt;$ did not reach the achieved goal state $g_i$, it reached the state $s_{t_i}$ and hence if the goal state was $s_{t_i}$ this would have been a useful trajectory. With the advent of goal-conditioned policy learning, policies $\\pi$ are no longer learnt for a single goal, rather goal state $g$ is taken as input along with state and action. i.e instead of $\\pi:S \\times A \\rightarrow [0,1]$, goal-conditioned policies are $\\pi:S \\times A \\times S_G \\rightarrow [0,1]$. So, we can engineer different goal for trajectories which do not reach their pre-determined goal and add it to the *replay buffer* with engineered goal state as well as the pre-determined goal. This increases the buffer size, providing more transition samples to learn.\nCritique Although the idea of the using the existing sampled trajectory by engineering the goal seems useful, it is not clear if there exists a principled approach to do this and how is it better than engineering reward-shaping functions?\nReferences  Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017 https://openai.com/blog/ingredients-for-robotics-research/  "
},
{
	"uri": "https://harshakokel.com/tags/generalization/",
	"title": "generalization",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/tags/imitation/",
	"title": "imitation",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/logic-program-policies/",
	"title": "Logical Program Policies",
	"tags": ["#RL", "#imitation", "#generalization"],
	"description": "  My notes on Tom Silver, Kelsey R. Allen, Alex K. Lew, Leslie Kaelbling, and Josh Tenenbaum, AAAI 2020.",
	"content": "This paper introduces a bayesian imitation learning approach to learn policies from few demonstrations. They call these policies Logical Program Policies (LPP) which are essentially policies learnt as combination of logical and programmatic policies. Logical because these are relational and programmatic because they are features are automatically learned.\nThe bayesian prior used here is the prior probability distribution over the Probablistic Context Free Grammer (P-CFG). Paper proposes to generate a dataset ($\\mathcal{D}$) where each state action pair $(s,a)$ is an example. Feature set of each example is obtained by initializing all the P-CFGs for each example. The target variable $y$ for each example is $1$ if $(s,a) \\in \\mathcal{D}$, $0$ otherwise.\nNext, all the features are arranged in decreasing order of their prior probabilities and iteratively decision-trees ($DT$s) are learnt with incremental feature size. So, at iteration $i$, features used are $f_o, f_1 , \u0026hellip; f_i$. The DT learned are converted to logical representation (i.e. disjunction of conjunctions of the P-CFG feature) and each DT is evaluated on the dataset $\\mathcal{D}$ and finally the top-$K$ DTs are used as weighted mixture model for testing.\nCritique The paper is a difficult read and doesn\u0026rsquo;t convey the actual procedure followed in the code. Algorithm in the paper suggest that the posterior $q$ is iteratively refined however digging into the code suggests that the posterior $q$ is independently computed for each tree and there is no carry forward from one iteration to another.\nThe paper introduced a good set of 2D grid domains with generalizable domain-specific language. Code is very neat and easy to read.\nThe baselines used in the paper CNN and FCN are not meant for few-shot learning so it is not a surprise that they did not work. Some comparision to meta-learning approaches would have been useful.\nReferences  https://github.com/tomsilver/policies_logic_programs  "
},
{
	"uri": "https://harshakokel.com/posts/understanding-node-attention/",
	"title": "Understanding Attention and Generalization in Graph Neural Networks",
	"tags": ["#GNN", "#coursework"],
	"description": "  My notes on Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer, NeurIPS 2019. Written as part of the Complex Networks course by Prof. Feng Chen.",
	"content": "Attention in CNNs (Answer Q-1)\nAttention in CNNs is reweighting the feature map $X \\in \\mathbb{R}^{N \\times C}$, to provide attention to some nodes.\n$$ \\begin{aligned} Z \u0026amp; =\\alpha \\odot X \\quad (Z_{i}=\\alpha_{i} X_{i}) \\\\ \\text{such that,} \\quad \\quad \u0026amp; \\sum_{i}^{N} \\alpha_{i} = 1 \\\\ \\odot \u0026amp; \\text{ is element-wise multiplication} \\end{aligned} $$\n Note: $\\alpha_i$ is a scalar and $X_i$ is vector of size C. So, $Z_i$ is also a vector of size C. $Z \\in \\mathbb{R}^{N \\times C}$\n Pooling in CNNs Pooling in CNNs divide the grid into local regions uniformly (not neighbors) and aggregate them to reduce the dimension.\nsrc: stackoverflow\n So, there is no parallelism between attention and pooling in the CCNs.\nBut in GNN, pooling also use the neighborhoods.\nTop K Pooling  Top K pooling was proposed by Gao and Ji, Graph U-Nets, ICML 2018, it is supposed to be a equivalent of k-max pooling (generalization of max-pooling) in the CNN where each feature map is reduced to size k by picking units with highest values. Since, in GNN the k-highest values can possibly come from different node for each feature-map the straight forward extention of k-max pooling does not work. So, Gao and Ji propose to project all the nodes to 1D and then select top K from that.\n Given feature matrix $X^{\\ell} \\in \\mathbb{R}^{N \\times C}$ and adjacency matrix $A^{\\ell} \\in \\mathbb{R}^{N \\times N}$, first project the feature matrix to 1D using projection vector $\\mathbf{p}$ and normalize it,\n$$ \\mathbf{y}=\\frac{X^{\\ell} \\mathbf{p}^{\\ell}}{\\left|\\mathbf{p}^{\\ell}\\right|} $$\nFrom this normalized 1D representation of each node ($\\mathbf{y}$), filter top K nodes and use indexes ( $\\mathrm{idx}=\\operatorname{rank}(\\mathbf{y}, k)$) to retrieve relevant feature matrix and adjacency matrix.\n$$ \\begin{aligned} \\tilde{X}^{\\ell} \u0026amp;=X^{\\ell}(\\text { idx}, :) \\\\A^{\\ell+1} \u0026amp;=A^{\\ell}(\\mathrm{idx}, \\mathrm{idx}) \\end{aligned} $$\nHowever, since the $\\mathbf{y}$ is discrete valued, authors use gate operation ($\\operatorname{sigmoid}$) to convert $\\mathbf{y}$ to real value and make it eligible for back-propagation $(\\tilde{\\mathbf{y}} = \\operatorname{sigmoid}(\\mathbf{y}(\\mathrm{idx}))$. The final feature matrix for the next layer is obtained by element wise multiplication of feature vectors of selected nodes and $\\tilde{\\mathbf{y}}$,\n(Answer Q-2)\nOver all,\n$$ \\begin{aligned} X^{\\ell+1} \u0026amp; =\\tilde{X}^{\\ell} \\odot\\left( \\tilde{\\mathbf{y}}\\mathbf{1}_{C}^{T}\\right) \\\\ \\text{with} \\quad \\tilde{\\mathbf{y}} \u0026amp; = \\operatorname{sigmoid}(\\frac{X^{\\ell} \\mathbf{p}^{\\ell}}{\\left|\\mathbf{p}^{\\ell}\\right|}(\\mathrm{idx})) \\end{aligned} $$\nIn GNNs, there is a parallelism between pooling and attention. Node attention $\\alpha$ can be thought of as $\\mathbf{\\tilde{y}}$.\n$$ Z_{i}=\\left\\{\\begin{array}{ll}{\\alpha_{i} X_{i},} \u0026amp; {\\forall i \\in P} \\\\{\\emptyset,} \u0026amp; {\\text { otherwise }}\\end{array}\\right. $$\nwhere, $P = \\{\\text{idx}\\}$ and $|P| = k$. $P$ is obtained by finding the indices of top-k values of $\\mathbf{y}$, which is computed by learning projection vector $\\mathbf{p}$ using back-propagation on input graph.\nThis paper proposes to combine the attention and pooling to a single computational block, which does not have a fixed $k$. Instead, set $P$ is determined by threshold $\\tilde{\\alpha}$:\n$$ Z_{i}=\\left\\{\\begin{array}{ll}\\alpha_{i} X_{i}, \u0026amp; \\forall i: \\alpha_{i}\u0026gt;\\tilde{\\alpha} \\\\\\emptyset, \u0026amp; \\text { otherwise }\\end{array}\\right. $$\nFurther, they also propose a combination of GIN and ChebyNet called ChebyGIN to be used for convolution after pooling.\nChebyGIN Graph Convolutional Network (GCN), Graph Isomorphism Network (GIN), ChebyNet have similar formulation with minor changes. The proposed ChebyGIN formulation is an extention of these changes. This section highlights the equivalence and differences in the mathematical forms of these networks. We compare the Convolution layer of these networks, each take input $H^{(\\ell)}$ (equivalently $h_i^{(\\ell)}, \\forall i \\in V$) and outputs $H^{{\\ell+1}}$ (equivalently $h_i^{(\\ell+1)}$). And, $\\mathcal{N}(i)$ indicates neighborhood of the node $i$.\n GCN:\n$$ \\begin{aligned} H^{(\\ell+1)} \u0026amp;= \\sigma\\left(\\hat{A} H^{(\\ell)}\\mathbf{W}^{(\\ell)}\\right) \\\\ \\mathbf{h}_{i}^{(\\ell+1)} \u0026amp; = \\sigma\\left( \\frac{1}{\\mathcal{c}_{ii}} \\mathbf{h}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} + \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\mathcal{c}_{ij}} \\mathbf{h}_{j}^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right) \\\\ \\text{with, } c_{ij} = \\sqrt{d_i, d_j} \u0026amp; \\text{ and } d_i = |\\mathcal{N}(i)| \\end{aligned} $$\nGIN:\nReplaces $\\sigma$ with multi-layer perceptron (MLP) and since, the MLP has weights and does rescaling from $f^i$ to $f^o$, we do not need $\\mathbf{W}^{(\\ell)}$ and the normalization $(\\frac{1}{\\mathcal{c}_{ij}})$.\n$$ \\mathbf{h}_{i}^{(\\ell+1)} = \\operatorname{MLP}\\left( \\mathbf{h}_{i}^{(\\ell)}(1 + \\varepsilon^{(\\ell)}) + \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{h}_{j}^{(\\ell)}\\right) $$\nHere, when $\\varepsilon \u0026gt; 0$ the current node is given more importance, when $\\varepsilon = 0$ the current node has same importance.\nChebyNet:\nGeneralization of GCN to $k^{th}$ order approximation of Chebyshev polynomial.\n$$ \\mathbf{h}_{i}^{(\\ell+1)} = \\sigma\\left( \\sum_{k =1}^{K-1} \\left( \\left( \\frac{1}{\\mathcal{c}_{ii}^{k}} \\mathbf{h}_{i}^{(\\ell)} \\mathbf{W}^{(\\ell)} \\right) + \\theta_k \\sum_{j \\in \\mathcal{N}_{k}(i)} \\frac{1}{\\mathcal{c}_{ij}^{k}} \\mathbf{h}_{j}^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right) \\right) $$\nChebyGIN:\n(Answer Q-3)\nReplaces $\\sigma$, $\\mathbf{W}$ and $\\frac{1}{\\mathcal{c}_{ij}^{k}}$ of ChebyNet, same as GCN.\n$$ \\mathbf{h}_{i}^{(\\ell+1)} = \\underbrace{\\operatorname{MLP}}_{\\text{FC Layer}}\\left( \\underbrace{ \\sum_{k =1}^{K-1} \\mathbf{h}_{i}^{(\\ell)}d_i(1 + \\varepsilon^{(\\ell)}) + \\theta_k \\sum_{j \\in \\mathcal{N}_{k}(i)} \\mathbf{h}_{j}^{(\\ell)} d_j}_{\\text{GNN Layer}} \\right) $$\n$\\theta_k$ is still multiplied at the neighborhood level to obtain different weights for each neighborhood. All the node feature vectors ($h_i$) are multiplied by node degree ($d_i$) for first layer.\nProposed architecture src: Boris Knyazev's slides\n The proposed architecture is as follows: first layer is a attention/pooling of input graph, second layer is GNN which aggregates features from local neighborhoods, and third layer is a fully connected (FC) layer, which can also do global pooling and finally an output layer which will be used for training. A separate fully connected MLP called attention network is trained to obtain attention values on each node.\nAttention Network For supervised learning of the attention network, the ground truth of attention values for each node ($\\alpha_i^{GT} \\in [0,1]$) in the graph is obtained by heuristic.\nFor example, in experimental dataset for graph color count, attention on each node is defined as follows:\n$$ \\alpha_{i}^{GT}=\\left\\{\\begin{array}{ll}{\\frac{1}{|N_{green}|},} \u0026amp; {\\forall i \\in N_{green}} \\\\{0,} \u0026amp; {\\text { otherwise }}\\end{array}\\right. $$\nIn experimental dataset of graph triangle count, following heuristic is used;\n$$ \\alpha_{i}^{GT}=\\left\\{\\begin{array}{ll}{\\frac{T_{i} }{\\sum_{i} T_{i}},} \u0026amp; {\\forall i \\in \\text{Triangle } } \\\\ {0,} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\\\ \\text{with, } T_i = \\text{number of triangles that include node } i $$\nFor MNIST-$75SP$ dataset where each node is a superpixel and edges are formed based on spatial distance between superpixel centers, following heuristic was used:\n$$ \\alpha_{i}^{GT}=\\left\\{\\begin{array}{ll}{\\frac{1}{N_{nonzero}},} \u0026amp; {\\forall i \\in \\text{Nonzero intensity superpixel} }\\\\ {0,} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\\\ \\text{with, } N_{nonzero} = \\text{number of such pixels } i $$\nTraining These networks are trained using back-propagation to minimize the Mean-Squared Error (MSE) loss or the Cross-Entropy loss (CE) of the over all prediction and minimize the Kullback-Leibler (KL) divergence loss between ground truth attention $\\alpha^{GT}$ and predicted coefficients\u000b$\\alpha$. The KL term is weighted by scale $\\beta$ and number of nodes $N$.\n$$ \\mathcal{L}=\\mathcal{L}_{M S E / C E}+\\frac{\\beta}{N} \\sum_{i} \\alpha_{i}^{G T} \\log \\left(\\frac{\\alpha_{i}^{G T}}{\\alpha_{i}}\\right) $$\n(Answer Q-4)\nSince $\\sum_i \\alpha_i = 0$, $\\alpha$ can be thought of as a probability distribution of attention over all the nodes and so, minimizing the KL-divergence is an obvious first choice. Below equation shows relationship between cross-entropy, entropy and KL Divergence. (Answer Q-5)\n$$ H(p, q)=H(p)+D_{\\mathrm{KL}}(p | q) $$\nWeakly supervised model For domains where the ground truth of attention is hard to obtain for each node, authors propose a weakly supervised learning setting as follows. Train an attention network (model B), which has same structure as the proposed architecture (model A) except for attention/pooling layer. Model B is trained to reduce the $\\mathcal{L}_{MSE}$ for $y$ prediction. Then, the $\\alpha^{WS}$ is calculated using the trained model and input graph $G$.\n$$ \\begin{aligned} {\\alpha}_{i}^{W S} \u0026amp; =\\frac{\\left|y_{i}-y\\right|}{\\sum_{j=1}^{N}\\left|y_{j}-y\\right|} \\\\ \\text{with } y \u0026amp; = \\operatorname{ModelB}(G) \\\\ \\text{ and } y_i \u0026amp; = \\operatorname{ModelB}(G - \\{N_i\\}) \\\\ \\end{aligned} $$\nNext, the proposed architecture \u0026ndash; Model A is trained using $\\alpha^{WS}$ to optimize both the MSE and KL divergence.\nsrc: Boris Knyazev's slides\n (Answer Q-6)\nFor colors domain, authors use 2 layers of GNN. So mathematical form for model B is:\n$$ Y = \\operatorname{GNN}(\\underbrace{W^{1}}_{64 \\times 1},\\operatorname{GNN}(\\underbrace{W^{0}}_{64 \\times F^i}, \\underbrace{H}_{N \\times F^{i}} ) ) $$ where as mathematical form of model A is:\n$$ Y = \\operatorname{ChebyGIN}(\\underbrace{W^{1}}_{64 \\times 1},\\operatorname{ChebyGIN}(\\underbrace{W^{0}}_{64 \\times F^i}, \\alpha^{WS}X) ) $$\nwhere $\\alpha^{WS}$ is as defined above, obtained from model B.\nAnalysis How powerful is attention over nodes in GNNs?\nContrary to what the authors mention in the paper, I feel that the experimental results show that there is not a lot of co-relation between attention and model accuracy. The example result below shows that the even though the proposed model has high co-relation with attention AUC, there are other models which do not show better performance even when the attention AUC is high. This observation is also backed by the paper, Jain et al. NAACL 2019 Attention is not Explanation.\nsrc:Knyazev et. al 2019 [Fig 3a]\n So the power of attention over nodes is I think need more study.\nWhat are the factors influencing performance of GNNs with attention?\nFrom experiment results it seems following factors influence the GNNs with attention:\n initialization vector \u0026ndash; optimal initialization has better accuracy in Fig-4(c) quality of the attention \u0026ndash; Supervised attention has better results than weakly supervised attention. strength of GNN model used \u0026ndash; ChebyGIN model has better results than GIN/GCN  Why is the variance of some results so high?\nVariance of some results is high because the model is very sensitive to the initialization parameters. It is only able to recover from bad initialization of hyper-parameters when the attention is good. Bad initialization of attention was not recoverable.\nHow top-k compares to our threshold-based pooling method?\nExperiments show that threshold-based pooling has better results than top-k pooling for larger datasets (with high features).\nHow results change with increase of attention model input dimensionality or capacity?\nWith increase in the input dimension for the attention model, the distribution of $\\alpha$ values become flat ($\\because \\sum_i\\alpha_i = 1$). Experiments show that in such cases, deeper GNN model for attention are useful.\nCan we improve initialization of attention?\nAuthors observe for unsupervised attention models, normal or uniform distribution with high values is preferred for the initialization of parameters of attention model. But for supervised or weakly supervised model smaller values are preferred. There is no intuition on why one is preferred over the other, paper just states the observation based on empirical evaluations.\nWhat is the recipe for more powerful attention GNNs?\nRecipe for powerful attention is to get supervision for attention. If supervision is not possible, use the weakly-supervised method for attention.\nHow results differ depending on to which layer we apply the attention model?\nAlthough it is desirable to use attention model closer to the input layer to reduce graph size and keep the attention weights interpretable, the experiments show that the attention on deeper layer have higher impact on the performance.\nWhy is initialization of attention important?\nSince the final model is trained by considering the $\\alpha$ \u0026ndash; attention weights as final, when the attention those weights have bad initialization, the weights learnt in rest of the model are wrong and hence the model is not able to recover.\nHowever, I feel that the models should be able to recover from the bad initialization with more iterations. Literature of expectation-maximization and bi-level optimization indicates that this is possible.\nDoubts  Why use sigmoid in Top-K Pooling? Gate operation \u0026ndash; why is projection discrete ??  Questions  What is the dimensionality of $Z_{i}$ ? Link to Answer How to decide $P$ from input graph? Link to Answer Provide mathematical form of ChebyGIN and show all the parameters Link to Answer Why is $KL$ selected as the loss function, but not cross entropy and squared error? Link to Answer Relation between Cross entropy and KL Divergence. Link to Answer Give mathematical forms of model A and B for Colors. Link to Answer Summarize: How powerful is attention over nodes in GNNs? Link to Answer Summarize: What are the factors influencing performance of GNNs with attention? Link to Answer Summarize: Why is the variance of some results so high? Link to Answer Summarize: How top-k compares to our threshold-based pooling method? Link to Answer Summarize: How results change with increase of attention model input dimensionality or capacity? Link to Answer Summarize: Can we improve initialization of attention? Link to Answer Summarize: What is the recipe for more powerful attention GNNs? Link to Answer Summarize: How results differ depending on to which layer we apply the attention model? Link to Answer Summarize: Why is initialization of attention important? Link to Answer  Extra questions to be considered  Find the source code related to Weakly supervised attention component and explain each line in the related source code Why GIN moves from weighted mean to the sum? How to do back-propagation with ranking? Doesn\u0026rsquo;t attention lead to overfitting ?? Higher number of parameters mean high chance of overfitting.  References  Boris Knyazev, Graham W. Taylor, and Mohamed R. Amer, NeurIPS 2019] Boris Knyazev\u0026amp;rsquo;s slides Gao and Ji, Graph U-Nets, ICML 2018  "
},
{
	"uri": "https://harshakokel.com/posts/gat/",
	"title": "Graph Attention Networks",
	"tags": ["#GNN", "#coursework"],
	"description": "  My notes on Peter Veličković et al. ICLR 2018. Written as part of the Complex Networks course by Prof. Feng Chen.",
	"content": "Introduction Convolutional Neural Networks (CNNs) can effectively transform grid-like structures and have been used for various image segmentation/classification tasks. Various approaches have not been proposed to extended CNNs graph structures. These approaches are broadly divided into two categories:\n  Spectral appraoch leverages the spectral representations of graph and defines convolution in the Fourier domain. However, because such convolutions require eigen-decomposition of graph laplacian, they can not be directly generalized to different graph structures. Most famous of this is Convolutional Graph Networks by Kipf and Welling, ICLR 2017. One major drawback of CNN is that is assigns equal importance to all the neighbors.\n  Spatial approach on other hand perform convolution directly on the graph, leveraging the neighborhood structure. This can be challenging because of varying neighborhood and most approaches perform some form of aggregation on the neighbors. Since the neighbors in graph have no particular order, the aggregation provides equal importance to all the neighbors.\n  Besides, attention mechanisms allow for focusing on relevant parts of input and have attained state-of-the-art for various tasks. This paper proposes the use of attention mechanism to provide a relevant weights to different neighbors.\nAttention layer Generally, each graph convolutional layer take input $\\mathbf{h}=\\left\\{\\vec{h}_{1}, \\vec{h}_{2}, \\ldots, \\vec{h}_{N}\\right\\}, \\vec{h}_{i} \\in \\mathbb{R}^{F}$ and generates output $\\mathbf{h}^{\\prime}=\\left\\{\\vec{h}_{1}^{\\prime}, \\vec{h}_{2}^{\\prime}, \\ldots, \\vec{h}_{N}^{\\prime}\\right\\}, \\vec{h}_{i}^{\\prime} \\in \\mathbb{R}^{F^{\\prime}}$ by linearly transforming the input vectors using weight matrix \u0026ndash; $\\mathbf{W} \\in \\mathbb{R}^{F^{\\prime} \\times F}$, taking weighted aggregate over the neighborhood \u0026ndash; $\\mathcal{N}$ (including the node itself), and finally passing the value from a non-linear activation function \u0026ndash; $\\sigma$.\n$$ \\vec{h}_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} \\mathbf{W} \\vec{h}_{i}\\right) $$\nThis can be equivalently written in matrix form as\n$$ \\mathbf{h}^{\\prime} = \\sigma \\left( \\tilde{A} \\mathbf{\\alpha} , \\mathbf{h} \\mathbf{W} \\right) \\\\ \\text{with}, \\tilde{A} = A + I_N \\\\ \\text{(adjanceny matrix with self loop)} $$\n In GCN, the convolutional layer is $\\vec{h}_{i}^{\\prime}=\\sigma\\left(\\sum_{j = 1 }^{N} \\hat{A_{i j}} \\mathbf{W} \\vec{h}_{i}\\right)$ where $\\hat{A}$ is a renormalized laplacian which indicated neighbor. So, effectively $\\alpha_{i j} = \\frac{1}{\\sqrt{d_{i}d_{j}}}$ because the\n This paper proposes to use the attention mechanism \u0026ndash; $a$ to compute the $\\alpha_{i j}$ in following way:\n$$ \\begin{aligned} \\alpha_{i j} \u0026amp; =\\operatorname{softmax}_{j}\\left(e_{i j}\\right) \\\\ \u0026amp; =\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(e_{i k}\\right)} \\\\ \\text{with}, , , e_{i j} \u0026amp; =a\\left(\\mathbf{W} \\vec{h}_{i}, \\mathbf{W} \\vec{h}_{j}\\right) \\end{aligned} $$\nIntuitively, $e_{i j}$ is importance of node $j$ on $i$ and $\\alpha_{i j}$ is normalized importance.\nThe framework proposed is agnostic to the choice of attention mechanism \u0026ndash; $a$. However, in the paper authors use a single-layer feed-forward neural network parameterized by weight vector $\\overrightarrow{\\mathrm{a}} \\in \\mathbb{R}^{2 F^{\\prime}}$ and LeakyReLU nonlinearity, which takes the input $\\mathbf{W} \\vec{h}_{i} | \\mathbf{W} \\vec{h}_{j}$, where $|$ represents concatenation. The activation mechanism is represented in the figure below. Mathematically, the activation mechanism used in the experiments is:\n$$ \\alpha_{i j}=\\frac{\\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} | \\mathbf{W} \\vec{h}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(\\text { LeakyReLU }\\left(\\overrightarrow{\\mathbf{a}} T\\left[\\mathbf{W} \\vec{h}_{i} | \\mathbf{W} \\vec{h}_{k}\\right]\\right)\\right)} $$\nsrc: Veličković et al. 2018  Intuitively, $\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} | \\mathbf{W} \\vec{h}_{j}\\right]$ is a linear combination of transformed $h_i$ and $h_j$. It can be thought of as a distance between node $i$ and $j$ or some aggregate. (Answers Q-1)\n Since the slope of ReLU is zero for -ve values, the ability to train the model is compromised in that region. This is called dying ReLU problem. LeakyReLU activation function is used instead of ReLU with a=0.01 in the figure below to avoid that problem. LeakyReLU helps speed up learning and is more balanced. (Answers Q-2)\n src: https://towardsdatascience.com/activation-...\n Following Vaswani et al. 2017, authors proposes to use $K$ independent attention mechanism to employ multi-head attention. So the mathematical form of each layer is:\n$$ \\vec{h}_{i}^{\\prime}=\\Biggm|_{k=1}^{K} \\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right) $$\nand the output $\\mathbf{h}^{\\prime}$ consists $K F^{\\prime}$ features instead of $F^{\\prime}$.\nOnly, in last layer the multi-head attentions are aggregated instead of concatenation. So, the mathematical form of last layer is:\n$$ \\vec{h}_{i}^{\\prime}=\\sigma\\left(\\frac{1}{K} \\sum_{k=1}^{K} \\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j}^{k} \\mathbf{W}^{k} \\vec{h}_{j}\\right) $$\nsrc: Veličković et al. 2018  Complexity Time complexity of computing single attention head is sum of complexity of computing the $e_{i j}$ for each node and then computing the softmax. The feed forward neural network, computing $e_{i j} = \\text{LeakyReLU}\\left(\\overrightarrow{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} | \\mathbf{W} \\vec{h}_{j}\\right]\\right)$ is effectively equivalent to complexity of matrix multiplication $\\underbrace{\\mathbf{W}}_{F^{\\prime} \\times F} \\underbrace{\\vec{h}_{i}}_{F \\times 1} + \\underbrace{\\overrightarrow{\\mathbf{a}}^{T}}_{1 \\times 2F^{\\prime}} \\underbrace{\\hat{h}}_{2F^{\\prime} \\times 1}$ for each node, $\\mathcal{O}\\left(|\\mathcal{V}|(FF^{\\prime} + 2F^{\\prime})\\right) = \\mathcal{O}\\left(|\\mathcal{V}|FF^{\\prime}\\right)$, where $\\mathcal{V}$ is number of nodes. Complexity of computing $\\alpha_{i j} =\\operatorname{softmax}_{j}\\left(e_{i j}\\right)$ for all the edges is $\\mathcal{O}(|\\mathcal{E}|F^{\\prime})$, where $\\mathcal{E}$ is number of edges. So, total complexity of a single attention head is $\\mathcal{O}(|\\mathcal{V}|FF^{\\prime} + |\\mathcal{E}|F^{\\prime})$. (Answers Q-3)\nMemory complexity of GAT for sparse matrix is linear in terms of nodes and edges. (Answers Q-4)\nExperiments Authors evaluate the GAT architecture for two type of learning tasks on graph structures:(Answers Q-5)\n  Transductive learning task is where the algorithm sees whole graph but labels of only few nodes are available. Algorithm is trained on these nodes and whole graph and the task is to produce labels for nodes which do not have labels while training. GCN works only for transductive setting.\n  Inductive learning task is when the algorithm sees only training nodes and edges between those nodes. Labels of all the training nodes are available and the task is to predict label for test nodes, which are unseen during training.\n  Transductive learning For transductive learning task, 10 baseline architectures are compared (including GCN, MoNet, Chebyshev, MLP etc), for three datasets Cora, Citeseer and pubmed. Two layer GAT architecture was used as shown in figure below, with 8 attention heads in first layer and 1 attention head in second layer. Softmax was used in final layer and ELU activation is used in the first layer.\n Mathematical form of the architecture:\n$$ Z = \\operatorname{softmax}\\left( \\tilde{A} \\mathbf{\\alpha}_{2}\\mathbf{W}_2\\left({\\biggm|}_{k=1}^{8} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}_{1}^{k}\\mathbf{W}_1 H \\right)\\right) \\right) $$\n(Answers Q-6)\nInductive Learning For inductive learning task, 6 baseline architectures are compared (including GraphSAGE, MLP etc), for PPI dataset. Three layer architecture was used as shown in the figure below.\n Mathematical form of the architecture:\n$$ Z = \\operatorname{LogisticSigmoid}\\left( \\frac{1}{6} \\sum_{z=1}^{6} \\tilde{A} \\mathbf{\\alpha}_{3}^{z}\\mathbf{W}_3\\left({\\biggm|}_{y=1}^{4} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}_{2}^{y}\\mathbf{W}_2 \\left({\\biggm|}_{k=1}^{4} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}_{1}^{k}\\mathbf{W}_1 H \\right) \\right)\\right)\\right) \\right) $$\n(Answers Q-7)\nExtensions The GAT network can be extended to use for Graph classification by simply appending a pooling layer at the end. Figure below represents one such architecture.\n (Answers Q-9)\nGAT can also be used for embedding the nodes to two-dimensional space. One such architecture is presented below, where instead of Softmax, the last layer outputs 2-D vector for each node.\n Mathematical form of this 2D embedding GAT network is\n$$ Z = \\operatorname{ELU}\\left( \\tilde{A} \\mathbf{\\alpha}_{2}\\mathbf{W}_2\\left({\\biggm|}_{k=1}^{8} \\operatorname{ELU}\\left(\\tilde{A} \\mathbf{\\alpha}_{1}^{k}\\mathbf{W}_1 H \\right)\\right) \\right) $$\n(Answers Q-8)\nQuestions  What attention mechanism is used in the experiments? Link to Answer Why LeakyReLU but not the standard ReLU ? Link to Answer Proof complexity of GAT layer is $O\\left(|V| F F^{\\prime}+|E| F^{\\prime}\\right)$. Link to Answer What is the memory complexity of GAT layer? Link to Answer Explain difference between transductive and inductive learning. Link to Answer Draw architecture of two layer GAT model for transductive learning. What is the Mathematical formulation? Link to Answer Draw architecture of three layer GAT model for inductive learning. What is the Mathematical formulation? Link to Answer Design a GAT model that embed the nodes of the cora network in a two-dimensional space. Draw the architecture and give the mathematic form. Link to Answer Design a GAT model for graph classification. Draw the architecture and give the mathematic form. Link to Answer  References  Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio, Graph Attention Networks , ICLR 2018 Graph Attention Networks overview by Peter Veličković ML Paper explained - AISC by Karim Khayrat: Graph Attention Networks Explained A Tutorial on Attention in Deep Learning, by Alex Smola and Aston Zhang, ICML 2019 https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6  "
},
{
	"uri": "https://harshakokel.com/posts/gcn/",
	"title": "Graph Convolutional Networks",
	"tags": ["#GNN", "#coursework"],
	"description": "  My notes on Thomas N Kipf and Max Welling ICLR 2017. Written as part of the Complex Networks course by Prof. Feng Chen.",
	"content": "Introduction Kipf et al. 2017 introduces Graph Convolutional Networks (GCN) which uses features of each node and leverages edges of the graph to derive class similarity between nodes in semi-supervised setting.\nTraditionally semi-supervised learning in a graph-structured data heavily relied on the assumption that the edges in the graph represent class similarities (i.e. nodes with similar classes have edge between them). For example, in an image segmentation task, an image can be thought as a grid (Graph with node for every pixel and edges between neighboring pixels as shown in figure below). Such representation of an image represents the assumption that the neighboring pixels belong to the same class (hence an edge between them).\nsrc: https://medium.com/@BorisAKnyazev/...\n With this assumption, semi-supervised learning task is then posed as an optimization problem with following loss function:\n\\[ \\mathcal{L}=\\mathcal{L}_{0}+\\lambda \\mathcal{L}_{\\mathrm{reg}}, \\quad \\text { with } \\quad \\mathcal{L}_{\\mathrm{reg}}=\\sum_{i, j} A_{i j}\\left|f\\left(X_{i}\\right)-f\\left(X_{j}\\right)\\right|^{2} \\]\nHere $\\mathcal{L}_{0}$ is some form of cross-entropy loss for the supervised examples and the $\\mathcal{L}_{\\mathrm{reg}}$ is a regularizing function which tries to reduce difference in labels of connected nodes. $\\mathcal{L}_{\\mathrm{reg}}$ is laplacian quadratic form.\n\\[ \\begin{aligned} f\\left(X\\right)^{\\top} \\Delta f\\left(X\\right) \u0026amp;= f\\left(X\\right)^{\\top}(D-A) f\\left(X\\right) \\\\ \u0026amp;=f\\left(X\\right)^{\\top} D f\\left(X\\right)-f\\left(X\\right)^{\\top} A f\\left(X\\right) \\\\ \u0026amp;=\\sum_{i=1}^{n} D_{ii} f\\left(X_{i}\\right)^{2}-\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} f\\left(X_{i}\\right)f\\left(X_{j}\\right) \\\\ \u0026amp;=\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} f\\left(X_{i}\\right)^{2}-\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} f\\left(X_{i}\\right) f\\left(X_{j}\\right) \\\\ \u0026amp;=\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}\\left(f\\left(X_{i}\\right)^{2}-f\\left(X_{i}\\right) f\\left(X_{j}\\right)\\right) \\\\ \u0026amp;=\\sum_{{i, j} \\in E}\\left(f\\left(X_{i}\\right)-f\\left(X_{j}\\right)\\right)^{2} \\\\ \u0026amp;= \\sum_{i \\leq j} A_{i j}\\left|f\\left(X_{i}\\right)-f\\left(X_{j}\\right)\\right|^{2} \\end{aligned} \\]\nGraph Convolutional networks GCN proposes a way to do graph convolutions by using the following layer wise propagation rule:\n\\[ \\begin{aligned} H^{(l+1)} \u0026amp; =\\sigma\\left( \\hat{A} H^{(l)} W^{(l)}\\right), \\\\ \\quad \\text { with } \\quad\\hat{A} \u0026amp; = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}, \\\\ \\tilde{A} \u0026amp; = (A + I_{N}), \\\\ \\end{aligned} \\]\n\\[ \\tilde{D_{ij}}=\\left\\{\\begin{array}{ll}{\\sum_{k=1}\\tilde{A}_{ik},} \u0026amp; {\\text { if } i = j} \\\\ { 0,} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\nThe paper proposes a two layer Graph Convolutional Network which amounts to following model:\n\\[ Z=f(X, A)=\\operatorname{softmax}\\left(\\hat{A} \\operatorname{ReLU}\\left(\\hat{A} X W^{(0)}\\right) W^{(1)}\\right) \\]\n ReLU activation function is used in the first layer because the gradient of ReLU is $0/1$, so with multiple iterations, the gradient values do not vanish (tends to zero), which is the case with other non-linear functions. Softmax is used in the top layer, because the final output expected are class probabilities.  GCN uses gradient descent to learn weight matrices \u0026ndash; $W^{(0)}$ and $W^{(1)}$ that minimizes the following cross-entropy error for all the supervised nodes ($\\mathcal{Y}_{L}$):\n\\[ \\mathcal{L}=-\\sum_{l \\in \\mathcal{Y}_{L}} \\sum_{f=1}^{F} Y_{l f} \\ln Z_{l f} \\]\nLoss function is then a combination of cross-entropy loss for the supervised labels and some regularization.\n\\[ Loss=\\mathcal{L} + \\mathcal{L}_{reg}, \\\\ \\begin{aligned} \\text{with } \\mathcal{L}_{reg} = \u0026amp; \\frac{\\lambda}{2} * \\sum|W|^{2} \\quad \\text{ for L2-regularization} \\\\ \u0026amp; \\frac{\\lambda}{2} * \\sum|W| \\quad \\text{ for L1-regularization} \\end{aligned} \\]\nIn the paper, authors observe that L2-regularization of weight matrix at the first layers alone is sufficient.\n Regularization is used to avoid overfitting by penalizing the weight matrices of hidden layers. L2-regularization in particular uses 2-norm of weight matrix for penalty. This pushes the weight matrix close to zero. (Answers Q-14)  Experiments\nConnection between CNN and GCN In CNN, input feature map (blue grid in below image) is convolved with discrete kernel ($W$) to produce output feature map (green grid). This can be seen as a message passing algorithm where the messages from the neighboring nodes ($h_i, i = 1 \\text{ to } 8$ in the example below) and the node itself ($h_0$) are multiplied with weight $W_i$ and the output feature map is obtained by summing up these messages.\nsrc: https://github.com/vdumoulin/conv_arithmetic\n   GCN similarly generates output feature map ($H^{(l+1)}$) by convolving the input feature map ($H^{(l)}$) with weight matrix ($W^{(l)}$).\n\\[ \\begin{aligned} H^{(l+1)} \u0026amp; = \\sigma\\left( \\hat{A} H^{(l)} W^{(l)}\\right), \\\\ \\text{ with } \\quad \\hat{A} \u0026amp; : n \\times n \\text{(\\# nodes)}, \\\\ H^{(l)} \u0026amp; : n \\times f^{i} \\text{(\\# input feature map)}, \\\\ W^{(l)} \u0026amp; : f^{i} \\times f^{o} \\text{(\\# output feature map)}, \\\\ \\end{aligned} \\]\nThis can be seen as a message passing algorithm where messages or input features ($H_{ik}$) of each node in the graph are multiplied by weights ($W_{jk}$) and summation is stored in $B$ and finally, the output feature map ($M$) is generated by summing the weighted messages of all the neighboring nodes along with the message from the node itself (Note: $\\tilde{A} = A + I_N$ takes care of message from node itself).\n\\[ \\begin{aligned} M \u0026amp; = \\hat{A} H^{(l)} W^{(l)} \\\\ \u0026amp; = \\hat{A} B, \\\\ \\text{ with } B_{ij} \u0026amp; = \\sum_{k=1}^{f^{i}} H_{ik}W_{jk} \\\\ M_{ij} \u0026amp; = \\sum_{k=1}^{n} \\hat{A}_{ik} B_{kj} \\end{aligned} \\]\n  In CNN the number of neighbors for each node are fixed ($8$ in above example), so the number of messages received by all the nodes in the output later are same. In GCN, since the structure of graph is dictated by the adjacency matrix, the number of messages received at each node ($M_{ij}$) is not the same. Hence, the need for normalizing $\\tilde{A}$ (i.e, $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$) arises in GCN but not in CNN.\n  Although theoretically there are no limitations on number of convolution layers, GCN paper proposes two layered network. Since every layer convolves the neighboring node, $k$ layers effectively convolves $k^{th}$ order neighbors. GCN convolve only upto $2^{nd}$ order neighbors. Their empirical evaluations suggest 2nd order neighbor is enough for most of the domains. (Answers Q-13)\n  In traditional approach, since we use $\\mathcal{L}_{reg}$, which is a function of adjacency matrix, the nodes with dense neighborhood will have high penalty and hence the model will overfit on the local neighborhoods of such nodes (consequently it will under-fit the nodes with sparse neighborhood). Normalization of adjacency matrix helps us alleviate this problem.\n  Extending GCN to graph classification and supervised learning task Answers Q-11\nGCN formulation can be leveraged for graph classification problem, where the task is to predict a single variable for the whole graph. Adjacency matrix of each graph ($Q, V, W$ in the figure below) is concatenated into a sparse block-diagonal matrix ($A$) as shown in the figure below. This adjacency matrix and the feature matrix ($X: n \\times c$, n = total number of nodes in all the graphs and c = # input features) can be used to train GCN and the output matrix Z can then be pooled to obtain class labels for each graph.\nsrc: https://github.com/tkipf/gcn\n For supervised learning task, two graphs can be created from training set (labeled nodes) and the testing set (unlabeled nodes). Adjacency matrix of two graphs can again be concatenated as block-diagonal matrix and GCN can be trained on it. The output matrix Z will have the class probabilities for test set as well.\nSpectral Graph Convolutions Convolution theorem states that convolution of two matrices is equivalent to point-wise multiplication in the fourier domain i.e. $\\mathcal{F}(x \\ast y) = \\mathcal{F}(x) \\odot \\mathcal{F}(y)$ when $\\mathcal{F}(\\ \\ )$ denotes fourier transform operator. In signal processing, the spectral transformation usually refers to transformation from time to frequency dimension (fourier domain), but in graph theory spectral transformation usually refers transformation to eigen-vector dimension ($U^{\\top}$). So, the convolution theorem in graphs represents following equation:\n\\[ \\begin{aligned} \\text{with } L \u0026amp; = U \\Lambda U^{\\top}, \\\\ \\hat{\\mathrm{x}} \u0026amp; = U^{\\top}\\mathrm{x}, \\\\ \\hat{\\mathrm{y}} \u0026amp; = U^{\\top}\\mathrm{y}, \\\\ U^{\\top} (\\mathrm{x} \\ast \\mathrm{y}) \u0026amp; = \\hat{\\mathrm{x}} \\odot \\hat{\\mathrm{y}} \\\\ \\mathrm{x} \\ast \\mathrm{y} = U ( \\hat{\\mathrm{x}} \u0026amp; \\odot \\hat{\\mathrm{y}}) = U ({ \\mathrm{diag}(\\hat{\\mathrm{y}})\\hat{x}} ) \\\\ \\mathrm{y} \\ast \\mathrm{x} = \\mathrm{x} \\ast \\mathrm{y} = \u0026amp; U \\ \\mathrm{diag}(\\hat{\\mathrm{y}})\\ U^{\\top} \\ \\mathrm{x} \\end{aligned} \\]\n  $\\text {with } L = U \\Lambda U^{\\top}$ is eigen-decomposition of graph laplacian matrix. Complexity of finding eigen-decomposition if $\\mathcal{O}(n^3)$ (Answers Q-2) Remember, matrix-vector multiplication ($A \\mathrm{b}$) is effectively transformation of the vector ($b$) to another dimension dictated by the matrix ($A$). Since, L is a square matrix, $U^T = U^{-1}$.   For spectral graph convolution, we directly estimate the convolution filter in the eigen-vector dimension as some function of eigen-vectors ($\\Lambda$). With filter $g_{\\theta}(\\Lambda) = \\mathrm{diag}(\\theta(\\Lambda))$ spectral convolution on graph is defined as follows:\n\\[ g_{\\theta}(\\Lambda) \\ \\star \\ \\mathrm{x} = U g_{\\theta}(\\Lambda) U^{\\top} \\mathrm{x} \\]\n Note the difference between convolution symbol ($\\ast$) and the spectral convolution symbol used in paper ($\\star$) signifies that the $g_{\\theta}$ is already in eigen-vector dimension.\n $g_{\\theta}$ can be approximated by truncated expansion in terms of Chebyshev polynomials $T_{k}(x)$ as:\n\\[ \\begin{aligned} g_{\\theta^{\\prime}}(\\Lambda) \\approx \\sum_{k=0}^{K} \\theta_{k}^{\\prime} T_{k}(\\tilde{\\Lambda}) \\\\ \\text{ with } \\quad \\tilde{\\Lambda}=\\frac{2}{\\lambda_{\\max }} \\Lambda-I_{N}, \\\\ T_{k}(x) = 2 x T_{k-1}(x)-T_{k-2}(x), \\\\ T_{0}(x)=1 \\text { and } \\quad T_{1}(x) =x \\\\ \\end{aligned} \\]\nChebyshev polynomial $T_{k}(\\tilde{\\Lambda})$ is a matrix with dimensions same as $\\tilde{\\Lambda}$ (i.e. $n \\times n$, where n = # nodes ). Elements of matrix $T_{k}(\\tilde{\\Lambda})$ are obtained by applying Chebyshev polynomial definition element wise. (Answers Q-1)\nSo,\n\\[ \\begin{aligned} T_0(\\tilde{\\Lambda}) \u0026amp; = \\left[\\begin{array}{cccc}{T_{0}\\left(\\frac{2}{\\lambda_{\\max }}\\left(\\lambda_{1}-1\\right)\\right)} \u0026amp; {} \u0026amp; {} \u0026amp; {} \\ {} \u0026amp; {T_{0}\\left(\\frac{2}{\\lambda_{\\max }}\\left(\\lambda_{2}-1\\right)\\right)} \u0026amp; {} \u0026amp; {} \\ {} \u0026amp; {} \u0026amp; {\\ddots} \u0026amp; {} \\ {} \u0026amp; {} \u0026amp; {} \u0026amp; {T_{0}\\left(\\frac{2}{T_{\\max }}\\left(\\lambda_{N}\\right)\\right)}\\end{array}\\right] \\\\ \u0026amp; = \\left[ \\begin{array}{cccc}{1} \u0026amp; {} \u0026amp; {} \u0026amp; {} \\\\ {} \u0026amp; {1} \u0026amp; {} \u0026amp; {} \\\\ {} \u0026amp; {} \u0026amp; {\\ddots} \u0026amp; {} \\\\ {} \u0026amp; {} \u0026amp; {} \u0026amp; {1}\\end{array} \\right] \\\\ \u0026amp; = I_N \\\\ \\\\ \u0026amp; \\therefore\\boxed{ \\quad T_{0}(\\tilde{\\Lambda})= I_N \\text{ and } T_{1}(\\tilde{\\Lambda}) =\\tilde{\\Lambda} } \\end{aligned} \\]\nObserve that $\\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top}$ and $U T_{k}(\\tilde{\\Lambda}) U^{\\top} = T_{k}(\\tilde{L})$.\nProofs (Answers Q-4): \\[ \\begin{aligned} (U \\Lambda U^{\\top})^{2} \u0026amp; = ( U \\Lambda U^{\\top} )( U \\Lambda U^{\\top} ) \\\\ \u0026amp; = U \\Lambda ( U^{\\top} U ) \\Lambda U^{\\top} \\\\ \u0026amp; = U \\Lambda I \\Lambda U^{\\top} \\\\ \u0026amp; = U \\Lambda ^{2} U^{\\top} \\\\ \\end{aligned} \\] \\[ \\text{ Similarly, } \\boxed{ \\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top} } \\]\nand\n\\[ \\begin{aligned} {U T_{0}(\\tilde{\\Lambda}) U^{\\top}=U I_{N} U^{\\top}=I_{N}=T_{0}(\\tilde{L})}, \\\\ {U T_{1}(\\tilde{\\Lambda}) U^{\\top}=U \\tilde{\\Lambda} U=\\tilde{L}=T_{1}(\\tilde{L})}, \\\\ \\end{aligned} \\] \\[ \\begin{aligned} U T_{2}(\\tilde{\\Lambda}) U^{\\top} \u0026amp; = U \\Big(2 \\tilde{\\Lambda} T_{1}(\\tilde{\\Lambda})- T_{0}(\\tilde{\\Lambda}) \\Big) U^{\\top} \\\\ \u0026amp; = 2 U \\tilde{\\Lambda} T_{1}(\\tilde{\\Lambda}) U^{\\top}- U T_{0}(\\tilde{\\Lambda}) U^{\\top} \\\\ \u0026amp; = 2 U \\tilde{\\Lambda} U^{\\top} U T_{1}(\\tilde{\\Lambda}) U^{\\top}- U T_{0}(\\tilde{\\Lambda}) U^{\\top} \\\\ \u0026amp; = 2 \\tilde{L} T_{1}(\\tilde{L}) - T_{0}(\\tilde{L})\\\\ \u0026amp; = T_2(\\tilde{L}) \\\\ \\end{aligned} \\] \\[ \\text{ Similarly, } \\boxed { U T_{k}(\\tilde{\\Lambda}) U^{\\top} = T_{k}(\\tilde{L}) } \\\\ \\]\nUsing the above two results, we can approximate the convolution without needing eigen-decomposition, directly using graph laplacian,\n\\[ \\begin{aligned} g_{\\theta^{\\prime}}(\\Lambda) \\star x \u0026amp; \\approx U \\sum_{k=0}^{K} \\theta_{k}^{\\prime} T_{k}(\\tilde{\\Lambda}) U^{\\top} \\mathrm{x} \\\\ \u0026amp; = \\sum_{k=0}^{K} \\theta_{k}^{\\prime} U T_{k}(\\tilde{\\Lambda}) U^{\\top} \\mathrm{x} \\\\ \u0026amp; = \\sum_{k=0}^{K} \\theta_{k}^{\\prime} T_{k}(\\tilde{L}) \\mathrm{x} \\end{aligned} \\]\nSo, we effectively reduced the complexity of convolution from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(|\\mathcal{E}|)$, i.e. from cube of # nodes ($n$) to linear in # edges ($\\mathcal{E}$). Since $T_i(\\tilde{L})$ has $2\\mathcal{E}$ non-zero elements, multiplication $\\underbrace{\\theta_k^{\\prime}}_{scalar} \\underbrace{T_i(\\tilde{L})}_{n \\times n} \\underbrace{\\mathrm{x}}_{n \\times 1}$ is a sparse matrix multiplication which can be done in $\\mathcal{O}(|\\mathcal{E}|)$. (Answers Q-5):\nConnection between GCN and spectral convolutions Answers Q-3\nGCN can be seen as approximation of spectral convolution with $K = 1$, $\\lambda_{max}=2$ and $\\theta = \\theta_0^{\\prime} = - \\theta_1^{\\prime}$ (Answers Q-8)\n\\[ \\begin{aligned} g_{\\theta^{\\prime}}(\\Lambda) \\star x \u0026amp; \\approx \\sum_{k=0}^{1} \\theta_{k}^{\\prime} T_{k}(\\tilde{L}) x \\\\ \u0026amp; = \\theta_{0}^{\\prime} T_{0}(\\tilde{L}) x+\\theta_{1}^{\\prime} T_{1}(\\tilde{L}) x\\\\ \u0026amp; = \\theta_{0}^{\\prime} I_{N} x+\\theta_{1}^{\\prime}\\left(\\frac{2}{\\lambda_{\\max }} L-I_{N}\\right) \\\\ \u0026amp; =\\theta_{0}^{\\prime} x+\\theta_{1}^{\\prime}\\left(L-I_{N}\\right) \\quad \\boxed{\\because \\lambda_{max}=2} \\\\ \u0026amp; =\\theta_{0}^{\\prime} x-\\theta_{1}^{\\prime} D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\quad \\boxed{\\because L=I_{N}-D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}} \\\\ \u0026amp; = \\theta \\left(I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) x \\quad \\boxed{\\because \\theta = \\theta_0^{\\prime} = - \\theta_1^{\\prime}} \\\\ \u0026amp; = \\theta \\left(\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}\\right) x \\quad \\boxed{ \\because \\underbrace{I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\rightarrow \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}}}_{\\text{renormalization trick }}, \\text { with } \\tilde{A}=A+I_{N} } \\\\ \u0026amp; = \\theta \\hat{A} x \\end{aligned} \\]\n If we use convolution formula $g_{\\theta} \\star x \\approx \\theta\\left(I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) x$ repeatedly over multiple layers, values keep increasing in every layer since we have $I_N$. Renormalization trick is used to avoid this. (Answers Q-9 and 10)\n  Since GCN uses $K =1$ and approximates $\\theta = \\theta_0^{\\prime} = - \\theta_1^{\\prime}$, which reduces the number of parameters we intuitively determine that GCN will not overfit the graphs. (Answers Q-6)\n  When we use $\\theta = \\theta_0^{\\prime} = - \\theta_1^{\\prime}$ we are assigning same weights to all the 1st and 2nd order neighbors. If we used different $\\theta$, we will assign different weights to different hop neighbors.\n  Renormalization Trick: With $I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, the eigen values are in range [0,2]. When the largest eigenvalue is less than $1$ the vanishing problem may occur even in case of two layers. With the renormalization trick, we can ensure that the eigen values are between [0,1] and the maximum eigenvalue is $1$. So, we avoid vanishing problem.\n So, \\[ \\begin{aligned} g_{\\theta^{\\prime}}(\\Lambda) \\star x \u0026amp; = diag(\\theta) T_{k}(\\tilde{L}) X \\\\ \u0026amp; = T_{k}(\\tilde{L}) \\ X \\ diag(\\theta) \\end{aligned} \\]\nSince this will still give us $n \\times f^{i}$ matrix, we add a weight matrix ($W^{\\prime}_{f^{i} \\times f^{o}}$) to linearly transform the result to $n \\times f^{o}$\nSo, \\[ \\begin{aligned} g_{\\theta^{\\prime}}(\\Lambda) \\star x \u0026amp; = T_{k}(\\tilde{L}) \\ X \\ diag(\\theta) \\ W^{\\prime} \\\\ \u0026amp; = \\hat{A} X W \\end{aligned} \\]\nWith $X \\in \\mathbb{R}^{n \\times f^{i}}$, we have $\\underbrace{Z}_{n \\times f^{o}} = \\underbrace{\\hat{A}}_{n \\times n} \\underbrace{X}_{n \\times f^{i}} \\underbrace{W}_{f^{i} \\times f^{o}}$ which is graph convolution. Complexity of matrix multiplication is $\\mathcal{O}(n f^{i}f^{o})$ since $\\hat{A}$ is a sparse matrix with $\\mathcal{E}$ non-zero elements \u0026ndash; Not sure how paper gets $\\mathcal{O}(|\\mathcal{E}| f^{i}f^{o})$ (Answers Q-12)\nLimitations   GCN provides equal importance to self node as well as the neighboring introduces. It also does not allow to different weights for different neighbors, which is allowed in CCN.\n  GCN being a graph convolution in the spectral domain, the Graph structure if fixed. Spatial domain convolutions on other hand allow for graph modifications.\n  Although GCN considers node features, it still heavily rely on the node locality.\n  Questions   What is the dimensionality of $T_{k}(\\tilde{\\Lambda})$, how to calculate $i^{th}$ row and $j^{th}$ column of matrix $T_{k}(\\tilde{\\Lambda})$? Link to Answer\n  What is the time complexity to compute the eigen-decomposition Link to Answer\n  What is the relation between $g \\star x$ and GCN? Link to Answer\n  Prove $\\left(U \\Lambda U^{\\top}\\right)^{k}=U \\Lambda^{k} U^{\\top}$. Link to Answer\n  Prove complexity of $\\sum_{k=0}^{K} \\theta_{k}^{\\prime} T_{k}(\\tilde{L}) x$ is $\\mathcal{O}(|\\mathcal{E}|)$. Link to Answer\n  Describe the intuition, why GCN can alleviate the problem of overfitting on local neighborhood structures for graphs with very wide node degree distributions? Link to Answer\n  Why is $\\lambda_{max} = 2$ a valid approximation?\n   Probably because eigen values of A $\\in$ [0,1] and $\\lambda_{max}$ is the eigen value of $A + I_N$, so the eigen values are in range [0,2]. We need this rescaloing because the Chebyshev polynomial needs input in range [-1,1]\n  Prove $g_{\\theta^{\\prime}} \\star x \\approx \\theta_{0}^{\\prime} x+\\theta_{1}^{\\prime}\\left(L-I_{N}\\right) x$. Link to Answer\n  Why does repeated application of $g_{\\theta} \\star x \\approx \\theta\\left(I_{N}+D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\right) {x}$ result in numerical instability? Link to Answer\n  Why does renormalization trick help? Link to Answer\n  Show in detail how to apply GCN for graph classification and supervised learning problem? Link to Answer\n  Derive time and memory complexity of $Z=\\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} X \\Theta$ Link to Answer\n  Why use 2 layers in GCN? Link to Answer\n  Explain L-2 regularization. Link to Answer\n  References  Thomas N Kipf and Max Welling, Semi-supervised classification with Graph Convolutional Networks, ICLR 2017 Notes by Prof. Feng Chen https://tkipf.github.io/graph-convolutional-networks/ Ferenc Huszár\u0026amp;rsquo;s GCN post https://dtsbourg.me/thoughts/posts/semi-supervised-classification-gcn https://medium.com/@BorisAKnyazev  "
},
{
	"uri": "https://harshakokel.com/posts/batch-rl/",
	"title": "Fitted Q and Batch Reinforcement Learning",
	"tags": ["#RL", "#offline", "#summary"],
	"description": "  Some pointers on Batch Reinforcement Learning and Fitted Q. These were gathered while working on an RL for healthcare project, as part of Advanced RL course by Prof. Sriraam Natarajan.",
	"content": "Terminologies Offline Planning Problem (MDP) We are given the full MDP model and the problem is solved using all the components of the MDP\nOnline Planning Problem (RL) We have limited knowledge of the MDP. We can discover it by interacting with the system\nModel-based RL Approaches to solving Online planning problem (RL) by first estimating (when missing) or accessing the full MDP Model i.e. transition and reward function and then finding policy $\\pi$ is called Model-based RL\nModel-free RL On the contrary, approaches to solving the Online RL Problem directly, i.e. solving for $\\pi$ directly with either value function $V$ or state-action function $Q$ is called Model-free RL.\n Batch RL The simulation environment is not present and complete set of transition samples $\\langle s, a, r, s^{\\prime} \\rangle$ is given and the challenge is to learn without exploring.\nBackground on Q Learning Bellman optimality equation for the action-value function ($Q$) is given as:\n$$Q^{\\pi}(s, a)=\\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\sum_{a^{\\prime}} \\pi\\left(s^{\\prime}, a^{\\prime}\\right) Q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] $$\nwhere $T\\left(s, a, s^{\\prime}\\right)$ is a transition probability of landing in state $s^{\\prime}$ on taking action $a$ in state $s$ and $R\\left(s, a, s^{\\prime}\\right)$ is a Reward at state $s^{\\prime}$ reached on taking action $a$ in state $s$.\nIn the dynamic programming setting, the $Q$ function for optimal policy is represented as:\n$$Q_{k+1}(s, a) \\leftarrow \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q_{k}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] $$\nQ-Learning is a model-free approach to learn the $Q$ function by exploring the environment, i.e. performing actions based on some policy. A table of Q function for each state action pair $Q(s,a)$ is maintained and the table in updated after every action using the running average formula:\n$$Q(s, a) \\leftarrow(1-\\alpha) Q(s, a)+(\\alpha)[ R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)] $$\nWith multiple episodes the Q values will eventually converge and the optimal policy might be retrieved from that.\nDrawbacks of Q Learning. There are several draw backs of the Q-Learning. These drawbacks might be minor in typical reinforcement learning setting where we have simulators. But these drawbacks are serious limitations in the Batch RL setting. In Batch RL, the simulation environment is not present and complete set of transition samples ($\\langle s, a, r, s^{\\prime} \\rangle$) is given and the challenge is to learn without exploring.\nExploration overhead\nAs we see it in the figure below, at some point in the top left cell $(1,3)$, agent explored the action of going north and because it landed in the same cell, it updated $Q(s,north)=0.11$ as per the $\\max_{a} Q(s,a)$ of that cell during that episode. After that episode, even though the $\\max_{a} Q(s,a)$ of that cell changes, the $Q(s,north)$ does not get updated till the agent explores going north.\n source: UC Berkeley CS188: Lecture of Pieter Abbeel  Stability issue\nQ-Learning has \u0026lsquo;asynchronous update\u0026rsquo; i.e. after each observation the value is updated locally only for the state at hand and all other states are left untouched. In the above figure, we know the value at the Red tile, but the Q value for the tile below it is not updated until we explore the action of going to red tile from that tile.\nSimilar idea of asynchronous update is also applicable in function approximations where $Q$ function is estimated by a function and at every time step the function is updated using:\n$$f(s, a) \\leftarrow(1-\\alpha) f(s, a)+\\alpha \\left( r +\\gamma \\max_{a^{\\prime} \\in A} f\\left(s^{\\prime}, a^{\\prime}\\right) \\right) $$\nInefficient approximation\nThe \u0026lsquo;asynchronous update\u0026rsquo; in function approximation is particularly harmful with global approximation functions. An attempt to improve $Q$ value of a single state after every time step might impair all other approximations. Specially when approximation function used is like Neural Network, where a single example can impact changes in all the weights. Gordon 1995 proves that using an impaired approximation in next iteration, the $f$ function may divergence from the optimal $Q$ function.\nThis is where fitted methods come in.\n Fitted Approaches Gordon 1995 provided a stable function approximation approach by separating dynamic programming step from function approximation step. Effectively, the above function update equation is now split to two steps.\n$$f^{\\prime}(s, a) \\leftarrow r +\\gamma \\max_{a^{\\prime} \\in A} f\\left(s^{\\prime}, a^{\\prime}\\right) , , , , \\forall s,a \\\\ f(s, a) \\leftarrow(1-\\alpha) f(s, a)+\\alpha f^{\\prime}(s, a) $$\nObservation: Splitting the function update from one to two steps is equivalent to changing the gram-schmidt orthonormalization to modified-gram schmidt orthonormalization.\nErnst 2005 proposed fitted Q iteration by borrowing the splitted approach from Gordon. The approach proposes iterative approximation of Q value by reformulating the Q Learning as a supervised regression problem. Algorithm proposed for fitted Q iteration is mentioned below.\nGiven: tuples {\u0026lt;s,a,r,s'\u0026gt;}, stopping condition  1. Q(s, a) = 0 2. while (!stopping condition): 3. Build a training set: {feature; regression value} = {\u0026lt;s,a\u0026gt; ; r + max_a Q(s,a)} 4. Learn a function approximating the regression values Q (s,a) This is in principle similar to equations mentioned above, with $f^{\\prime}$ as regression value and $\\alpha=1$.\nFurther extensions to the fitted Q approaches have learnt $f$ function as some linear combination of the previous function and regression values.\nReferences  Lange, S., Gabel, T., \u0026amp; Riedmiller, M. Batch Reinforcement Learning. 2012 Gordon, G. J. Stable Function Approximation in Dynamic Programming, ICML 1995. Ernst, D., Geurts, P., \u0026amp; Wehenkel, L. Tree-Based Batch Mode Reinforcement Learning, In JMLR 2005. http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf  "
},
{
	"uri": "https://harshakokel.com/tags/offline/",
	"title": "offline",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/hierarchical-rl/",
	"title": "Hierarchical Reinforcement Learning",
	"tags": ["#RL", "#hierarchy", "#coursework"],
	"description": "  An overview of Hierarchical RL. Written as part of Advanced RL course by Prof. Sriraam Natarajan.",
	"content": "Standard RL planning suffers from the curse of dimensionality when the action space is too large and/or state space is infeasible to enumerate. Humans simplify the problem of planning in such complex conditions by abstracting away details which are not relevant at a given time and decomposing actions into hierarchies. Several researchers have proposed to model the temporal-abstraction in RL by composing some form of hierarchy over actions space (Dietterich 1998, Sutton et al 1998, Parr and Russell 1998). By modeling actions as hierarchies, researchers extended the primitive action space by adding abstract actions. Options framework (Sutton et al 1998), refer the abstract actions as options, MAXQ (Dietterich 1998) refer to them as tasks and Hierarchical Abstract Machines (HAM) (Parr and Russell 1998) refers to them as choices.\nCommon theme among these papers is to extend the Markov Decision Process (MDP) to Semi-Markov Decision Process (SMDP), where actions can take multiple time steps. As compared to MDP, which only allow actions of a discrete time-steps, SMDP allows modeling temporally abstract actions of varying length over a continuous time. As represented in first two trajectories of figure below. By constraining/extending the action space of the MDP over primitive and abstract actions, hierarchical RL approaches superimpose MDPs and SMDPs as shown in last trajectory.\nSemi-Markov Decision Process\n HRL is appealing because the abstraction of actions facilitate accelerated-learning and generalization while exploiting the structure of the domain.\nFaster learning is possible because of the compact-representation. Original MDP is broken into sub-MDP with less states (abstracted states hide irrelevant details and hence reduce the number of states) and less actions. For example, in the Taxi Domain introduced in (Dietterich 1998), if the agent is learning to navigate to a location it does not matter if the passenger is being picked or dropped. Details about location of passenger are irrelevant and hence the state space is reduced.\nBetter generalization is possible because of the abstracted actions. In the taxi domain, because we define an abstract action called $Navigation$, agent learns a policy to navigate the taxi to a location. Once that policy is learned for navigation to pick up a passenger, the same policy can be leveraged when then agent is navigating to drop the passenger.\nTwo important promises of HRL are prior-knowledge and transfer-learning. A complex task in HRL is decomposed into hierarchy (usually by humans). Hence, it is easier for humans to provide some prior on actions from their domain knowledge. Different levels of hierarchy encompass different knowledge and hence ideally it would be easier to transfer that knowledge across different problems.\nOne minor limitation of HRL is that all the hierarchical methods converge to hierarchically optimal policy, which can be a sub-optimal policy. For example in the taxi domain, if the hierarchy decomposition states first navigate to the passenger location and then navigate to the fuel location, the HRL agent will find an optimal policy to do that in exactly that order. This policy might be sub-optimal given an initial state which is closer to the fuel location. This limitation is an artifact of restricting the action space while solving sub-MDPs. If full action space is available in all the MDPs, the exponential increase in computational overhead makes the learning infeasible.\nMax-Q framework has a clear hierarchical decomposition of tasks, while the options-framework do not have clear hierarchy. Options framework achieves temporal abstraction of actions, Max-Q framework additionally also achieves state abstractions. While there has been an attempt on discovering and transferring the Max-Q hierarchies (Mehta et al. 2008), learning Max-Q hierarchies directly from the trajectories is still an open problem. For large and complex problem it might be a challenge to provide the task hierarchy or options and their termination conditions.\nReferences  [Dietterich 1998] Dietterich, T. G. 1998. The maxq methodfor hierarchical reinforcement learning. In ICML. [Sutton, Precup, and Singh 1998] Sutton, R. S.; Precup, D.;and Singh, S. P. 1998. Intra-option learning about tempo-rally abstract actions. In ICML. [Parr and Russell 1998] Parr, R., and Russell, S. J. 1998. Reinforcement learning with hierarchies of machines. In NeurIPS [Mehta et al. 2008] Mehta, N.; Ray, S.; Tadepalli, P.; and Di-etterich, T. 2008. Automatic discovery and transfer of maxq hierarchies. In ICML. The Promise of Hierarchical Reinforcement Learning by Yannis Flet-Berlia in The Gradient Hierarchical Reinforcement Learning lecture by Doina Precup on YouTube  "
},
{
	"uri": "https://harshakokel.com/posts/active-advice-seeking/",
	"title": "Active Advice Seeking for Inverse Reinforcement Learning",
	"tags": ["#RL", "#advice", "#starling"],
	"description": "  My notes on Phillip Odom and Sriraam Natarajan, AAMAS 2016.",
	"content": "In Kunapali et al 2013, authors present a way of incorporating advice in Inverse Reinforcement Learning (IRL) by extending IRL formulation to include constraints based on the expert\u0026rsquo;s advice of preferred and avoided actions, state and reward. Odom et al 2017 expands on Kunapali et al (2013)\u0026rsquo;s formulation of preferred and avoided actions by seeking the advice in active learning setting. The paper proposes a active advice-seeking framework, where instead of seeking mere-label from the expert for selected example as done in active learning, they seek advice (set of preferred and avoided labels) over set of examples. Two clear advantages of active advice-seeking framework over the traditional active learning setting is that expert can provide multiple advice and that advice is generalized over set of states.\nThe workflow of the paper is shown in the figure above. First the states are clustered together, heuristic proposed is to cluster the states which has similar action distribution, and authors use the action distribution of the demonstrations as a proxy. Then, a policy is learnt only from the demonstrations. Next, query is generated by selecting the state cluster with the maximum uncertainty measure (equation below). This uncertainty measure is an artifact of the entropy of the distribution of actions in demonstration and the entropy of the policy\n\\[ U_{s}\\left(s_{i}\\right)=\\underbrace{w_{p} F\\left(s_{i}\\right)}_{\\text{demonstration}}+\\underbrace{\\left(1-w_{p}\\right) G\\left(s_{i}\\right)}_{\\text{policy}} \\]\nAdvice, set of preferred and avoid actions, is received on the query and a new policy is learnt based on the advice and the demonstrations. Query generation, advice seeking and policy learning steps are repeated again and again till the query budget surpasses.\nActive-advise seeking is compared against learning only from trajectories (standard IRL), advice over single state (active learning) and advice over random clusters. This paper show empirically (on 4 datasets) that the active-advice seeking framework proposed is superior to the other approaches, albeit sensitive to the quality of the cluster.\nCritique The paper introduces first of its kind framework to actively solicit advice from the human experts and is a very important contribution towards showing the need for Human-In-The-Loop approaches. Abstract MDPs and Relational RL have a natural way to reference a set of states by abstract state and a horn clause respectively. This paper shows that in case of propositional RL, this can be achieved by clustering states using K-Means. This could be a valuable direction as it provides best of both worlds (Propositional \u0026amp; Relational RL). The clustering heuristic does not have a theoretical justification but empirical evaluations show that it performs well for IRL. This is reasonable considering that we are learning a policy from the demonstration and hence the states with similar actions are grouped together. However, this might not be the case always. So, a more matured clustering technique might be useful.\nThe clustering is done only once, based on the demonstrations which might be sub-optimal. An iterative clustering might have been more challenging but one wonders if that would have helped achieve optimality especially in the drone-flying domain where there is lot of scope to improve.\nIn this framework, the advice is generalized over the clustered states, but in real-world I imagine an advice can be generalized beyond clusters and hence a way to generalize advice across states might be an interesting future direction.\nAlso, I would like to appreciate the complexity of the drone flying domain. Here, as the drone is supposed to visit the corners in a specific order, the preferred action at any location will be different based on the next corner being targeted. Paper lacks the details of how the state encoding was done, it would have been interesting to see some details. The expert advice will also have to be carefully tailored to achieve the corner sub-goal.\nReferences  Phillip Odom and Sriraam Natarajan, Active Advice Seeking for Inverse Reinforcement Learning, AAMAS 2016 Gautam Kunapuli, Phillip Odom, Jude W Shavlik, and Sriraam Natarajan, Guiding autonomous agents to better behaviors through human advice, ICDM 2013  "
},
{
	"uri": "https://harshakokel.com/tags/advice/",
	"title": "advice",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/minimal-sufficient-explanation/",
	"title": "Minimal Sufficient Explanation",
	"tags": ["#planning", "#XAI", "#RL"],
	"description": "  My notes on Omar Zia Khan, Pascal Poupart, and James P Black, ICAPS 2009",
	"content": "Automated planning problems have long been attempted using Markov Decision Processes (MDPs). MDPs are capable of handling the probabilistic and sequential nature of planning problem. They solve the problem by providing a policy which is a mapping from states to actions. However, to use this policy in the real-world, we first need users to trust the policy. The issue of trust can be ameliorated if the policy provides an explanation for its recommeded actions. This is the pretext of khan et al 2009.\nPolicy in MDP is usually obtained by optimizing Bellman\u0026rsquo;s equation which considers value of a state as expected discounted total reward. This paper uses an alternate formulation of the value function introduced by and extends it to the factored MDPs. In the alternate formulation, value function is expressed as a dot product of reward and the discounted occupancy frequencies of all the states. With this formulation, an optimal policy for a given state can explain the recommended action by simply showing that the recommended action has highest occupancy frequency of highly rewarded state(s). To this effect the paper propose three explanation templates:\n ActionName is the only action that is likely to take you to Var1 = Val1, Var2 = Val2, \u0026hellip; about $\\lambda$ times, which is higher (or lower) than any other action. ActionName is likely to take you to Var1 = Val1, Var2 = Val2, \u0026hellip; about $\\lambda$ times, which is high (or low) as any other action'' ActionName is likely to take you to Var1 = Val1, Var2 = Val2, \u0026hellip; about $\\lambda$ times.  Var1 = Val1, Var2 = Val2, \u0026hellip; in the template represents a single state (in case of MDPs) or a scenario\\footnote{In the case of factored MDPs, the reward function is also factored and the scenario used is defined as set of states which have similar rewards.} (in case of Factored MDPs). Multiple templates can be used for explaining a recommended action.\nPower of these templates can be realized if they provide minimal-sufficient explanations (MSE). An explanation is sufficient if it can prove that the action recommended is optimal (or no other action has better utility than the recommended action). Sufficient explanation is minimal if the number of templates used are minimum possible.\nWhen the policy ($\\pi^{\\ast}$) is optimal, the value of the recommended action for state $s_0$~$\\left(V^{\\pi^\\ast}\\right.$ or $\\left.Q^{\\pi^\\ast}(s_0, \\pi^\\ast(s_0))\\right)$ will be higher than value of all other actions $\\left(Q^{\\pi^\\ast}(s_0,a), , \\forall a \\neq \\pi^\\ast(s_0)\\right)$. Khan et al. 2009 define $V_{MSE}$ (eqn. below) which has two components: (a) expected utility from all the states(or scenarios) included in the explanation, and (b) worst case utility of all the states (or scenarios) not included in the explanation. By this definition $V_{MSE}$ cannot exceed the value of optimal action. If the explanation is minimal and sufficient, $V_{MSE}$ will be higher than the value of any other action.\n\\[ V_{MSE} = \\sum_{i \\leq k}\\mathbb{E}[\\operatorname{utility}(s_i)] + \\sum_{i \u0026gt; k} \\min(\\operatorname{utility}(s_i)) \\]\n\\[ V^{\\pi^{\\star}} \\geq V_{MSE} \u0026gt; Q^{\\pi^\\star}(s_0,a)\\ \\ \\forall a \\neq \\pi^\\star(s_0) \\]\nWith that definition of $V_{MSE}$, Khan et al. propose an algorithm to produce MSE at any state $s_0$ for a given optimal policy in factored MDP setting. They evaluate the MSE on two experimental domains of \u0026ldquo;course-advising\u0026rdquo; and \u0026ldquo;handwashing\u0026rdquo;. The experimental results show that the number of templates in MSE is high (or low) when the ratio of the expected utility for the optimal and the second best optimal policy is high (or low). User study with advisors in the course-advising domain show that the advisors found the explanations useful and perhaps beneficial for grade-conscious or poor performing students. User study with students show that more than 50% of the students found the explanations easy to understand, accurate and informative. However, most of them needed further information to trust the action recommended.\nCritique The paper addresses a very important topic of explaining the recommendations of a Markov Decision Process. The work is especially commendable as it was done way ahead of the Explainable AI Hype. They provide a very principled and straight forward way to explain the recommendations from the perspective of how the policy is constructed. However, it might not be intuitive enough to the users. As seen in the user-study, most of the users wanted to compare the explanation of the recommended action with their preferred actions or need more information about occupancy frequencies.\nIn complex domains, Bellman\u0026rsquo;s optimality equation is not solved exactly and hence, the value function of each state is only discovered approximately by the RL agent. In that scenario, as I understand, the optimality of the policy is only with respect to that approximate value function and hence the explanation of optimality is also only with respect to that approximate value function. To end user, this values might not mean anything and hence the sufficiency of the explanation might be questioned.\nThe explanations are only useful when it has less number of templates, the user-study in course-advising domain shown in the paper has less than 3 templates for the explanations. But in complex domain, as seen in handwashing experiments the number of templates will eventually increase and I conjecture that with increase in the number of templates the explanations will be less meaningful to the users.\nAlso, as raised by Dr. Natarajan in the class discussion the template only explains the action with respect to the states it visits but not with respect to the states it helps avoid. For domain where we want to avoid a certain states, we encode high negative rewards for that state. Even though the optimal policy learns to avoid that negative state, the MSE framework will only look at the occupancy frequency of highly rewarding states and try to explain positive reward states. As the occupancy frequency of the highly negative state is $0$ with the recommended action, it will not come up in the explanation. Although the templates proposed does mention \u0026ldquo;or lower\u0026rdquo; in brackets, it is not quite clear how the avoided state will be added in the explanation.\nFinally, in RL we usually assume the policy is mapping of states to a distribution over actions. Explaining a distribution might be complex than a single action. This paper concentrates on explaining the best action out of that distribution. However, there might be two equally good actions, or there might be more than one optimal policy. One has to extend this framework for that use cases before using it in a real system.\nReferences  Omar Zia Khan, Pascal Poupart, and James P Black, Minimal Sufficient Explanations for Factored Markov Decision Processes, ICAPS 2009 Pascal Poupart, Exploiting structure to efficiently solve large scale partially observable Markov decision processes, Citeseer 2005.  "
},
{
	"uri": "https://harshakokel.com/tags/xai/",
	"title": "XAI",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/advice-based-learning/",
	"title": "Advice based relational learning",
	"tags": ["#advice", "#coursework", "#survey"],
	"description": "  As part of an independent study with Prof. Sriraam Natarajan, I read advice-based methods for data in relational first-order logic. Here are my notes on it.",
	"content": "Artificial Intelligence (AI) is rapidly becoming one of the most popular tool for solving various problems of humankind. Ranging from trivial day-to-day activity of switching on/off lights, to severe life-changing decision of detecting tumors in scans, all the problems have been tackled with this tool and hence it is no longer acceptable to have a black-box algorithm making calls. AI Community has realized this and hence has put lot of significance on Explainable AI (XAI) in recent years. AI research has taken a shift from Human vs Robot to Human-Allied AI or Human-in-the-loop AI. However, this is not an unexplored territory. Advice-based or Knowledge-based methods have been around since the conception of AI.\nAdvice based method Lot of work has been done to incorporate knowledge in propositional domain and was surveyed by me as part of CS 7301 Recent Advances in Computing - Survey of Adv Research in CS in Fall of 2018. I summarize it here and then review the work done in relational domain.\nUsually advice-based methods encode human knowledge or domain information in various forms, a small list with examples is provided below.\n A set of if-then rules is represented as propositional horn clauses\nAdvice: if $B_1, B_2, \u0026hellip;, B_n$ then $A$\nHorn clause: $A \\leftarrow B_1, B_2, \u0026hellip;, B_n$ Qualitative influence between variables is rendered as Qualitative Probabilistic Network (QPN) (Wellman 1990)\nAdvice: \u0026ldquo;price of apple rises with an increase in demand\u0026rdquo;     Uncertainty between variables is formulated by conditional probability tables\nAdvice: \u0026ldquo;When it is cloudy, it mostly rains\u0026rdquo;\n   Preference statements under a ceteris paribus (all else being equal) is interpreted as CP-Net (Boutilier et al., 2004)\n   Advice: Fish soup ($S_f$) is strictly preferred over Vegetable soup ($S_v$), and preference between red ($W_r$) and white ($W_w$) wine is conditioned on the soup. Red wine is preferred if served with a vegetable soup, and white wine is preferred if served with a fish soup\nOnce advice is encoded, it is used while learning the model in one of the following ways:\n  Theory Refinement Advice is used as initial model with approximate domain knowledge and training examples are then used to refine this model.\n  Objective Refinement Advice is used to constrain or modify the objective function and then training samples are used to learn a model that maximizes this objective.\n  Observation Refinement Advice is used to refine, clean or augment part ( or all ) of the observed training samples and then this is used to learn the model.\n  Relational and SRL methods Most of the real-world data is complex, attempts are made to represent them using object (as variables) and relations (as predicates) using first-order logic programs called Inductive Logic Programming (ILP). Add uncertainty to such complex data and they become convoluted. Multiple formalisms have been proposed for such probabilistic-logic representations using statistical techniques, now popular known as field of Statistical Relational Learning (SRL) (Getoor 2007). With power of complex representation, these model lose out on other tasks. Specifically, inference and learning (which involves inference) in SRL is NP-hard or harder. For that reason, learning SRL models have been a topic of interest among researchers for a long time and it called for advice-based learning.\nAdvice-based relational methods First-order logic unlike propositional logic or flat feature vectors have two important linguistic features, viz. variables and quantifiers. These features augment capability of providing a general high-level advice for example we can say \u0026ldquo;Do not change lanes if there is no vehicle in front of you\u0026rdquo; which can be easily written as\n\\[ \\begin{aligned} lane(x, l)\\ \\wedge\\ \u0026amp;\\neg\\ \\exists\\ y,\\ (\\ lane(y, l)\\ \\wedge\\ ahead(y, x)\\ )\\ \\\\ \u0026amp;\\implies \\ \\neg\\ changelane(x) \\end{aligned} \\]\nOne straight forward way of advice-based relational model is to hand-code the predicates or clauses without need of any data. Pazzani and Kibler (1992) proposed one of the first successful algorithms for advice-based inductive learning called First Order Combined Learner (FOCL). FOCL is an extension of FOIL (Quinlan, 1990). FOCL uses both inductive as well as explanation-based component (a set of horn clauses provided by human expert) to propose an extension to a learned concept at every step. FOCL has been shown to generalize more accurately than the purely inductive FOIL algorithm. Bergadano and Giordana (1988) had also proposed ML-SMART, to integrate explanation-based learning (EBL) and inductive techniques to construct operational concept definitions using three components: theory guided component, arbitrary component and a drop component. ML-SMART is able to handle both overly-general and overly-specific theories. The goal is to find rules that cover the positive examples of the concept and exclude the negatives. GRENDEL (Cohen, 1992) is another FOIL based system that uses advice in the form of an antecedent description grammar (ADG) to explicitly represent the inductive hypothesis space. Mooney and Zelle (1994) reviews advice-based methods developed for ILP which integrate ILP with EBL.\nSRL models have probabilities (parameters) associated with the clauses (structure). Most raw form of advice in SRL can be direct probability values. But, there has been lot of work to incorporate richer form of advice. I briefly describe a few studies:\n\u0026ndash; Yang et al. 2014 proposed a cost-sensitive soft margin approach to learn from imbalanced domain. It builds up on the Relational Functional Gradient Boosting (RFGB) (Natarajan et al., 2012) approach of learning an SRL model by including a cost-augmented scoring function equation below that treats positive and negative examples differently in the objective function.\n$$ c\\left(\\hat{y_{i}}, y\\right)=\\alpha I\\left(\\hat{y_{i}}=1 \\wedge y=0\\right)+\\beta I\\left(\\hat{y_{i}}=0 \\wedge y=1\\right) $$\nwhere $\\hat{y_{i}}$ is the true label of $i^{th}$ instance and $y$ is the predicted label. $I(\\hat{y_{i}} = 1 \\wedge y = 0)$ is 1 for false negatives and $I(\\hat{y_{i}} = 0 \\wedge y = 1)$ is 1 for false positives. Hence, $c(\\hat{y_{i}}, y) = \\alpha$ when a positive example is misclassified, while $c(\\hat{y_{i}},y) = \\beta$ when a negative example is misclassified.\n$$ \\log J =\\sum_{i} \\psi\\left(y_{i} ; \\mathbf{X}_{i}\\right)- \\log \\sum_{y_{i}^{\\prime}} \\exp \\left\\{ \\psi\\left(y_{i}^{\\prime} ; \\mathbf{X}_{i}\\right)+c\\left(\\hat{y}_{i}, y_{i}^{\\prime}\\right) \\right\\} $$\nTaking derivation of this modified objective function gives a nice form of gradient\n$$\\Delta=I\\left(\\hat{y}_{i}=1\\right)-\\lambda P\\left(y_i=1 ; \\mathbf{x}_i\\right)$$\nwhere $\\lambda=\\frac{e^{c\\left(\\hat{y}_{i}, y=1\\right)}}{\\sum_{y^{\\prime}}\\left[P\\left(y^{\\prime} ; \\mathbf{X}_{i}\\right) e^{c\\left(\\hat{y}_{i}, y^{\\prime}\\right)}\\right]}$\nRelational regression trees are then learnt using this soft-gradient.\n\u0026ndash; Odom et al. 2015 uses preference advice encoded in Horn clause form as a relational advice constraint and use it as cost function while learning the model.\nA relational advice constraint (RAC), F is defined using a Horn clause $\\wedge_i f_i(x_i) \\implies label(x_e)$, where $\\wedge_i f_i(x_i)$ specifies the conjunction of conditions under which the advice applies on the example arguments $x_e$. Each \\textit{relational advice rules} (RAR) is defined using the triple $\\langle F, l+, l- \\rangle$ where $F$ is the RAC clause specifying the subset of examples, $l+$ is the preferred label and $l-$ is the avoided label. And, each relational advice set (RAS), $R$ is specified as a set of RAR. Cost function is defined as:\n$$ c\\left(x_{i}, \\psi\\right)=-\\lambda \\times \\psi\\left(x_{i}\\right) \\times\\left[n_{t}\\left(x_{i}\\right)-n_{f}\\left(x_{i}\\right)\\right] $$\nwhere, $n_t$ indicates the number of advice rules that prefer the example to be true, $n_f$ the number of rules that prefer it to be false, $\\lambda$ is scaling factor of the cost function and $\\psi(x_i)$ is the current value of the $\\psi$ function for the $x$.\nThe modified log-likelihood function is:\n$$ MLL(\\mathbf{x}, \\mathbf{y})=\\sum_{x_{i} \\in \\mathbf{x}} \\log \\frac{\\exp \\left(\\psi\\left(x_{i} ; y_{i}\\right)\\right)}{\\sum_{y^{\\prime}} \\exp \\left(\\psi\\left(x_{i} ; y^{\\prime}\\right)+c\\left(y_{i}, y^{\\prime}, \\psi\\right)\\right)} $$\nScaled gradient of this is\n$$ \\eta \\Delta\\left(x_{i}\\right) =\\alpha \\cdot \\left[ I\\left(y_{i}=1\\right)-P\\left(y_{i}=1 ; \\psi\\right) \\right] + (1 - \\alpha) \\cdot\\left[n_t\\left(x_i\\right)-n_f\\left(x_i\\right)\\right] $$\nSo, when the example label is preferred target in more advice models than the avoided target, $n_t(x_i)-n_f(x_i)$ is set to be positive. This will result in pushing the gradient of these examples in the positive direction. And, $\\alpha$ is the linear trade-off between data and advice.\nThis approach was successfully adapted in relation extraction and healthcare problems (Odom et al., 2015a; Soni et al., 2016; Natarajan et al., 2017).\n\u0026ndash; Later, Odom and Natarajan (2018) extended that constraint-based framework from Odom et al. (2015b) by redefining $l+$ and $l-$ in relational advice rules (RAR) triple $\\langle F, l+, l- \\rangle$ to weighted label. $\\beta_t$ weight for preferred label and $\\beta_f$ for the avoided label and modifying the cost function.\n$$ c_{LP}\\left(x_{i}, \\psi\\right)=-\\lambda \\times \\psi\\left(x_{i}\\right) \\times\\left[\\beta_{t} \\times n_{t}\\left(x_{i}\\right)-\\beta_{f} \\times n_{f}\\left(x_{i}\\right)\\right] $$\nThis enables them to include following types of advice:\n Preferential advice: This advice is handled similar to the previous paper, except now magnitude of each constraint is determined by the weights $\\beta s$ Cost-based advice: The soft margin approach in Yang wt al. 2014 is contained in the framework by controlling $\\beta s$. Specifically, $\\beta_t$ handles the sensitivity to false positives and $\\beta_f$ for false negatives. Qualitative constraints: In relational domain advice on qualitative constraints can be viewed as providing multiple pieces of advice over a given feature. As the number of objects satisfying this constraint increases more advice will apply. $n_t$ (or $n_f$ ) to scale based on that feature. Privileged information: This is the features that are available during training but not during testing. To handle this, they define a cost function that reduces the KL divergence between $P(y|x)$ and $P(y|x^{RP})$ where $x^{RP}$ is the set of all the features including privilege features.  $$ c_{R P}\\left(x_{i}, \\psi\\right) = -\\lambda \\times K L\\left(P_{D}\\left(y_{i} | \\mathbf{x}_{i}^{\\mathrm{RP}}\\right) | P\\left(y_{i} | \\mathbf{x}_{i}\\right) \\right) $$\nThe gradients obtained for the modified log-likelihood is as follows:\n$$ \\eta \\Delta_{L P}\\left(x_{t}\\right)= \\alpha \\cdot \\left[ I\\left(y_{t}=1\\right)-P\\left(y_{t}=1 ; \\psi\\right) \\right] + (1- \\alpha) \\cdot\\left[\\beta_{t} \\cdot n_{t}\\left(x_{t}\\right)-\\beta_{f} \\cdot n_{f}\\left(x_{t}\\right)\\right] $$\n$$ \\Delta_{R P}\\left(x_{i}\\right)=I\\left(y_{t}=1\\right) -P\\left(y_{t}=1 ; \\psi\\right) -\\alpha \\cdot\\left(P\\left(y_{t}=1 | \\mathrm{x}_{l}^{\\mathrm{CF}}\\right)-P_{D}\\left(y_{t}=1 | \\mathrm{x}_{t}^{\\mathrm{RP}}\\right)\\right) $$\n\u0026ndash; Odom and Natarajan (2016) also introduced active advice seeking a framework for PLMs where model can query the human expert in the most useful areas of the feature space takes full advantage of the data as well as the expert. Advice here is a set of Queries from the model and responses from the expert. Each query is a conjunction of literals which represents set of examples and expert response is a preferred or avoid labels for that query.\nKey idea here is how generate the query. Model calculates the score of each example from the entropy of prediction\n$$ H\\left(x_{i}\\right)=\\sum_{l \\in \\text {Labels}} P_{l}\\left(y_{i} | x_{i}\\right) \\log \\left(P_{l}\\left(y_{i} | x_{i}\\right)\\right) $$\nThese scores are used as regression values and a set of weighted first-order-logic clauses are learned that group examples. These clauses are posed as query to the expert.\nConclusion There are ways to incorporate the rich domain knowledge gathered by humans over course of time and hence we should not reinvent the wheel by making our model learn solely from the data. There can be situations where learning from data is enough but for complex and rich models it would not be sufficient and such approaches can be very useful in that case.\n"
},
{
	"uri": "https://harshakokel.com/tags/survey/",
	"title": "survey",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/posts/srl-for-myocardialinfarction/",
	"title": "SRL for Myocardial Infarction",
	"tags": ["#SRL", "#healthcare", "#starling"],
	"description": "  It is no secret that my motivation to do a Ph.D. is to uncover how AI can help improve healthcare. Weiss, Natarajan et al. (IAAI 2012) is one of the first papers I read to start my Ph.D. journey.",
	"content": "Myocardial Infarctions (generally known as heart attacks) causes one in three deaths in the United States and unsurprisingly have the most mysterious trajectory. It has been established that the prediction of future MIs is a challenging task and hence there have been extensive studies to identify and/or quantify the risk factors that contribute to MIs. Few common risk factors that have been identified are age, gender, blood pressure, low-density lipoprotein (LDL) cholesterol, diabetes, obesity, inactivity, alcohol and smoking. The canonical method of study in this field using: case-control studies, cohort studies, and randomized controlled trials concentrated on one risk factor at a time. So, the natural way forward is to analyze effects of multiple factors at a time and question is can we do it using machine learning? Also, Electronic Health Records (EHR) is used to over come the limitation of the data collected in previous studies.\nThe limitation of the previous data is that in these studies the risk factors are established at t0 and data is collected at the onset of the study, and then annual check-ups are conducted to assess patient health and determine occurrence of an MI event. The patients who did not possess risk factors at time t0 and developed them at later time were considered as not possessing that risk throughout the analysis. On the contrary, EHRs provides information of the development of risk factors as it tracks the health trajectories of its patients through time and hence provides an unique advantage for risk modelling. It can help us create a risk profile similar to Framingham Risk Score (FRS) without medical interventions (like additional laboratory tests). Also, as FRS works better for Caucasians than other populations it is biased. Hence, EHR risk profiles would be more reliable score than FRS.\nThis paper approaches the task of prediction and risk stratification of MI from EHRs using two Statistical Relational Learning (SRL) methods Relational Probability Trees (RPT) and Relational Functional Gradient Boosting (RFGB). RPTs upgrade decision trees to relational setting and RFGBs upgrade FGBs to relational setting. For RPT, paper uses Tilde relational regression learner (RRT) for positive examples to learn tree whose leaves have regression values which gives the probability of MI occurrence. Inner nodes in this tree represent conjunctions of literals (maximum two literals). FGBs fit the regression tree on training examples at each gradient step, for RFGBs propositional regression trees are replaced with relational regression trees.\nExperiments were performed on de-identified EHR data of 18,386 subjects. Total of 1,528 binary features were chosen a priori from relational tables for diagnoses, medications, labs, procedures, vitals, and demographics. This included major risk factors, common risk factors, drugs and patient relations. Also, features were discretized (e.g. for blood pressure, we created five binary features by mapping the real value to critically high, high, normal, low, and critically low). The training data was split in a way that 1:1 ratio is achieved for positive to negative examples. The paper compares RFGB, RPT, Boosted Decision Trees, Decision Trees, Naive Bayes, Tree Augmented Naive Bayes, Support Vector Machines (SVM) with linear kernel, SVMs with radial basis function kernel and Random Forests. The results show that accuracies of all the algorithms are comparable. However, in the medical domain as it is more important to avoid false negative than false positive, better precision @ high recall is much more significant measure than accuracy. Experiments show that RFGB performs way better than other approaches for precision at high recall.\nKey contribution  They approach the problem of predicting MIs in real patients and identifying ways in which machine learning can contribute to clinical studies. It establishes that the each relational learner can out perform its propositional variants even for the large scale domains like EHRs and provide interpretable results. It introduces task of MI prediction to SRL community.  Limitations  Features in this experiment were chosen a priori even for the algorithms that employ feature selection for computational reasons and to compare it with algorithms that do not employ feature selection. Features were discretized and not used in their natural form. Relational information such as hierarchies present in the EHR for diagnoses, drugs, and laboratory values are not considered in this experiments.  Critique The paper highlights a very important limitation of the data collected in clinical trials and signifies the importance of EHRs. The interpretability achieved by these models is very important, especially in the medical domain.\nThe paper claims that its key contribution is that they address the challenging problem of predicting MI in real patients and identify ways in which machine learning can augment current methodologies in clinical studies.Though this is the first paper to use SRL methods to predict MI in real patients, it is not the first paper to use machine learning to predict/detect MI. ”Kernel-based Support Vector Machine classifiers for early detection of myocardial infarction” by D. Conforti, and R. Guido pre-dates this paper. Though it is unclear to me whether or not prediction and early detection are synonymous, I would have liked to see some mention of a previous ML works as a related work section.\nReference  Jeremy C. Weiss, Sriraam Natarajan, Peggy L. Peissig, Catherine McCarty, and David Page, Statistical Relational Learning to Predict Primary Myocardial Infarction from Electronic Health Records, IAAI 2012  "
},
{
	"uri": "https://harshakokel.com/posts/bayesian-logic/",
	"title": "BLOG: Relational Modeling with Unknown Objects",
	"tags": ["#SRL", "#coursework"],
	"description": "  BLOG by Milch et al. provides a language which help us model random functions and probabilistic properties of unknown objects. Going beyong Herbrand Universe. This mainly reviews the key contributions and limitations of that paper. Written as part of the Statistical Relational Learning course by Prof.Sriraam Natarajan.",
	"content": "Most of the First-Order Logic Models we have seen till now like Bayesian Logic Programs or MLNs works with an assumption that we are given a fixed set of objects, a Herbrand Universe and we have a possible world, Herbrand Interpretation on which we build our BLP model or MLN Network. However, in many practical problems we do not have a fixed Herbrand Universe, or we do not know how many objects are there in the universe. This paper introduces BLOG (Bayesian LOGic) which can perform inference even when the objects are unknown. BLOG is a representation language which can generate objects for such an unknown universe and create a Bayesian network which can then be used for inference. BLOG model describes a stochastic process which creates a world by generating objects in an order. The existence of a generated object is governed by the unknown number of objects in the universe and by the random functions defined in the model. The key idea here is that any property of the newly created object depends only on the objects that are already created. Using this and the concept of context specific independence, BLOG generates a countably infinite Bayesian network in which each node has finite set of parents. The finite set of parents and prior knowledge about the distribution makes the inference possible.\nKey contribution According to me, the key contribution of this paper is the representation language. This language helps us model random functions and probabilistic properties of unknown objects. Consider the Balls in Urn example provided in the paper. We do not know the number of balls in the urn and we randomly draw balls and put it in. We have no way to identify if the same ball was drawn twice. We can only detect the color of the ball and that too has false detection probability of 0.2. With this information, we are supposed to find out number of balls in the urn. Most other models will give up at the first step itself. BLOG provides a simple language which can model such a stochastic scenario by using just 6 kinds of statements: Type Declarations, Random Function Declarations, Non-random Function Definitions, Guaranteed Object Statements, Dependency Statements, Generating Functions, and Number Statements. Once we have the model, we can represent any possible instantiation of the world.\nLimitations As BLOG creates a Random Variable for every function and object type, we potentially get a Bayesian Network with exponential number of Random Variables. Although context specific independence assumption helps us do the inference, it is computationally impossible to work with a such huge Bayesian Network. This makes the whole development less viable in practical scenarios as the purpose of the model was to provide inference over unknown or non-finite universe.\nRepresentation can have unnecessary variables. For example, if we have the evidence that balls drawn in Draw1 and Draw2 are equal, model creates a new constant for that. Also, we would still have all random variables for both Draw1 \u0026amp; Draw2 like ObsColor[Draw1], ObsColor[Draw2]. So, there will be 2n extra random variables where n is the number of functions for Draw object.\nTo make the Bayesian network inferable, BLOG makes the context specific independence assumption. But this assumption is not straight forward. For instance, in the aircraft example, a blip at time t is dependent on the state of all the aircraft at time t. To make it dependent only on its source aircraft, we would need evidence or prior knowledge about the source of the blip.\nAs the BLOG assumes that any newly created object depends only on the already existing object, it is assuming the universe to be acyclic and objects have topological order.\nThe model cannot learn any prior distributions. It assumes that it has all the prior distributions from the domain expert.\nAs each object is stored as a tuple, they can expand to considerable size. For example, in the aircraft model each blip is represented as (Blip, (Source, (Aircraft, 2)), (Time, 8), 1), if the object blip had k properties the tuple size would increase further.\nCritique Pros:\n The examples of the stochastic world taken are motivating. The paper provides a beautiful explanation on the difference between conditioning on the constant versus conditioning on the existence. These new constants resemble Skolem constants, but conditioning on assertions about the new constants is not the same as conditioning on an existential sentence. For example, suppose you go into a new wine shop, pick up a bottle at random, and observe that it costs USD 40. This scenario is correctly modeled by introducing a new constant Bottle1 with a Uniform CPD. Then observing that Bottle1 costs over USD 40 suggests that this is a fancy wine shop. On the other hand, the mere existence of a USD 40+ bottle does not suggest this, because almost every shop has some bottle at over USD 40.”  Cons:\n The paper represents the model, functions, worlds, etc,. in mathematical expressions but doesn’t provide strong mathematical proof/explanation for the unique joint distribution. There is inconsistency in the models. For instance, Aircraft example has a number function for creating the blip which assigns the source and time simultaneously. However, in the Ball example, true color of balls is not assigned when it is created using number function. Similarly, the number function of Publication assigns Author but the generating function for author is missing. Hence, it appears that there can be multiple models to represent the same scenario. However, there is no explanation if different representation would result in same joint distribution. Paper doesn’t put forward enough experimental evidence. The experiment conducted asserts that 10 balls were drawn and all appeared blue. The query was made about number of balls in the urn. Results showed that when the prior for the number of balls is uniform over 1, . . . , 8, the posterior puts more weight on small numbers of balls. And this experiment was validated by saying ”this makes sense because the more balls there are in the urn, the less likely it is that they are all blue”. Few more experiments would have helped.  References  Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L. Ong and Andrey Kolobov, BLOG: Relational Modeling with Unknown Objects, IJCAI 2005 Lise Getoor and Ben Tasker, Introduction to Statistical Relational Learning, (Adaptive Computation and Machine Learning), The MIT Press.  "
},
{
	"uri": "https://harshakokel.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": " ",
	"content": ""
},
{
	"uri": "https://harshakokel.com/cv/",
	"title": "cv",
	"tags": [],
	"description": "This is the CV or Resume or Curriculum Vitae of Harsha Kokel. ",
	"content": " Curriculum Vitae Harsha Kokel,\nhkokel@utdallas.edu\nThe University of Texas at Dallas,\nErik Jonsson School of Engineering and Computer Science,\nStARLinG Lab: 3.214, 800 W. Campbell Road, Dallas, TX 75080  Google Scholar | Semantic Scholar |  DBLP |  ORCID |  Research Gate Resume     Education   Doctor of Philosophy (Ph.D.) Computer Science \u0026nbsp; [ Fall 2018 - present ]  The University of Texas at Dallas, TX, USA  Advisor: Prof. Sriraam Natarajan    Master of Science (M.S.) Computer Science \u0026nbsp; [ Fall 2017 - Spring 2021 ]  The University of Texas at Dallas, TX, USA  Advisor: Prof. Sriraam Natarajan    Bachelor of Technology (B.Tech.) Information and Communication Technology  \u0026nbsp; [ May 2013 ] Dhirubhai Ambani Institute of ICT (DA-IICT), Gandhinagar, India  Thesis Advisor: Prasenjit Majumder Thesis Topic: Language identification for short text in transliterated space    Publications  [C7] Hybrid Deep RePReL: Integrating Relational Planning and Reinforcement Learning for Information Fusion,\nHarsha Kokel, Nikhilesh Prabhakar, Balaraman Ravindran, Eric Blasch, Prasad Tadepalli, Sriraam Natarajan, In IEEE 25th International Conference on Information Fusion (FUSION) 2022. paper | blog | cite @inproceedings{KokelPRBTN22,\n\u0026emsp;\u0026emsp; title={Hybrid Deep RePReL: Integrating Relational Planning and Reinforcement Learning for Information Fusion},\n\u0026emsp;\u0026emsp; author={Harsha Kokel and Nikhilesh Prabhakar and Balaraman Ravindran and Eric Blasch and Prasad Tadepalli and Sriraam Natarajan},\n\u0026emsp;\u0026emsp; journal={IEEE 25th International Conference on Information Fusion (FUSION)},\n\u0026emsp;\u0026emsp; year={2022},\n\u0026emsp;\u0026emsp; }\nCopy  [W6] Action Space Reduction for Planning Domains,\nHarsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi, Kavitha Srinivas, In Planning and RL (PRL) Workshop at ICAPS 2022. paper  [W5] Dynamic probabilistic logic models for effective task-specific abstractions in RL (Abstract),\nHarsha Kokel, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In 5th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2022. paper | code | poster  [C6] How to Reduce Action Space for Planning Domains? (Student Abstract),* oral presentation (20/111 accepted abstracts)\nHarsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi, Kavitha Srinivas, In AAAI 2022. paper | cite @article{KokelLKSS22,\n\u0026emsp;\u0026emsp; title={How to Reduce Action Space for Planning Domains? (Student Abstract)},\n\u0026emsp;\u0026emsp; author={Kokel, Harsha and Lee, Junkyu and Katz, Michael and Sohrabi, Shirin and Srinivas, Kavitha},\n\u0026emsp;\u0026emsp; journal={AAAI},\n\u0026emsp;\u0026emsp; year={2022},\n\u0026emsp;\u0026emsp; }\nCopy  [W4] Deep RePReL-Combining Planning and Deep RL for acting in relational domains,\nHarsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In Deep RL Workshop at NeurIPS 2021. paper | code | supplemental | slide | video | cite @article{KokelMNBTDRL,\n\u0026emsp;\u0026emsp; title={Deep RePReL-Combining Planning and Deep RL for acting in relational domains},\n\u0026emsp;\u0026emsp; author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},\n\u0026emsp;\u0026emsp; journal={Deep {RL} Workshop at {NeurIPS}},\n\u0026emsp;\u0026emsp; year={2021},\n\u0026emsp;\u0026emsp; }\nCopy  [W3] Dynamic probabilistic logic models for effective abstractions in RL,\nHarsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In Statistical Relational AI (StarAI) Workshop at IJCLR 2021. paper | poster | cite @article{KokelMNBTSTARAI,\n\u0026emsp;\u0026emsp; title={Dynamic probabilistic logic models for effective abstractions in RL},\n\u0026emsp;\u0026emsp; author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Ravindran, Balaraman and Tadepalli, Prasad},\n\u0026emsp;\u0026emsp; journal = {CoRR},\n\u0026emsp;\u0026emsp; volume = {abs/2110.08318},\n\u0026emsp;\u0026emsp; year={2021},\n\u0026emsp;\u0026emsp; }\nCopy  [C5] Human-guided Collaborative Problem Solving: A Natural Language based Framework,\nHarsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, Dan Roth, In ICAPS (demo track) 2021. paper | blog | video | cite @article{KokeletalDemo,\n\u0026emsp;\u0026emsp; title={Human-guided Collaborative Problem Solving: A Natural Language based Framework},\n\u0026emsp;\u0026emsp; author={Harsha Kokel, Mayukh Das, Rakibul Islam, Julia Bonn, Jon Cai, Soham Dan, Anjali Narayan-Chen, Prashant Jayannavar, Janardhan Rao Doppa, Julia Hockenmaier, Sriraam Natarajan, Martha Palmer, Dan Roth},\n\u0026emsp;\u0026emsp; journal={Thirty First International Conference on Automated Planning and Scheduling ({ICAPS}) Demo Track},\n\u0026emsp;\u0026emsp; year={2021},\n\u0026emsp;\u0026emsp; }\nCopy  [W2] RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction (Extended Abstract),* contributed talk (11/25 accepted paper)\nHarsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In Planning and RL (PRL) Workshop at ICAPS 2021. paper | poster | video  [C4] A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes,\nAthresh Karanam, Alexander L. Hayes, Harsha Kokel, David M. Haas, Predrag Radivojac, Sriraam Natarajan, In AIME 2021. paper | blog | DOI| video | cite @inproceedings{KaranamHKHRN21,\n\u0026emsp;\u0026emsp; author = {Athresh Karanam and Alexander L. Hayes and Harsha Kokel and David M. Haas and Predrag Radivojac and Sriraam Natarajan},\n\u0026emsp;\u0026emsp; title = {A Probabilistic Approach to Extract Qualitative Knowledge for Early Prediction of Gestational Diabetes},\n\u0026emsp;\u0026emsp; year = {2021},\n\u0026emsp;\u0026emsp; publisher = {Springer International Publishing},\n\u0026emsp;\u0026emsp; pages = {497--502},\n\u0026emsp;\u0026emsp; booktitle = {Artificial Intelligence in Medicine},\n\u0026emsp;\u0026emsp; isbn = {978-3-030-77211-6},\n\u0026emsp;\u0026emsp; DOI = {10.1007/978-3-030-77211-6_59},\n\u0026emsp;\u0026emsp; }\nCopy  [C3] RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction,\nHarsha Kokel, Arjun Manoharan, Sriraam Natarajan, Balaraman Ravindran, Prasad Tadepalli, In ICAPS 2021. paper | blog | code | supplemental | DOI| video | cite @article{KokelMNRT21,\n\u0026emsp;\u0026emsp; title={RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstraction},\n\u0026emsp;\u0026emsp; author={Kokel, Harsha and Manoharan, Arjun and Natarajan, Sriraam and Balaraman, Ravindran and Tadepalli, Prasad},\n\u0026emsp;\u0026emsp; journal={Proceedings of the International Conference on Automated Planning and Scheduling},\n\u0026emsp;\u0026emsp; number={1},\n\u0026emsp;\u0026emsp; volume={31},\n\u0026emsp;\u0026emsp; year={2021},\n\u0026emsp;\u0026emsp; month={May},\n\u0026emsp;\u0026emsp; pages={533--541},\n\u0026emsp;\u0026emsp; url={https://ojs.aaai.org/index.php/ICAPS/article/view/16001},\n\u0026emsp;\u0026emsp; }\nCopy  [W1] Graph Sparsification via Meta-Learning,* contributed talk (4/22 accepted paper)\nGuihong Wan, Harsha Kokel, In Deep Learning for Graphs (DLG) Workshop at AAAI 2021. paper | slide | video | cite @article{WanK21,\n\u0026emsp;\u0026emsp; title={Graph Sparsification via Meta-Learning},\n\u0026emsp;\u0026emsp; author={Wan, Guihong and Kokel, Harsha},\n\u0026emsp;\u0026emsp; journal={Deep Learning for Graphs (DLG) Workshop at AAAI},\n\u0026emsp;\u0026emsp; year={2021}\n\u0026emsp;\u0026emsp; }\nCopy  [C2] A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domain,\nHarsha Kokel, Phillip Odom, Shuo Yang, Sriraam Natarajan, In AAAI 2020. paper | blog | code | supplemental | poster | slide | DOI | cite @article{KokelOYN20,\n\u0026emsp;\u0026emsp; author = {Harsha Kokel and Phillip Odom and Shuo Yang and Sriraam Natarajan},\n\u0026emsp;\u0026emsp; title = {A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domains},\n\u0026emsp;\u0026emsp; journal = {Proceedings of the AAAI Conference on Artificial Intelligence}\n\u0026emsp;\u0026emsp; volume = {34},\n\u0026emsp;\u0026emsp; number = {04}, \u0026emsp;\u0026emsp; pages = {4460--4468},\n\u0026emsp;\u0026emsp; year = {2020},\n\u0026emsp;\u0026emsp; month = {Apr.},\n\u0026emsp;\u0026emsp; DOI = {10.1609/aaai.v34i04.5873},\n\u0026emsp;\u0026emsp; url = {https://aaai.org/ojs/index.php/AAAI/article/view/5873},\n\u0026emsp;\u0026emsp; }\nCopy  [C1] Morpheme Extraction Task,\nRashmi Sankepally, Komal Agarwal, Harsha Kokel, Prasenjit Majumder, In Forum for Information Retrieval Evaluation (FIRE) 2013. paper | DOI | cite @inproceedings{SankepallyKAM13,\n\u0026emsp;\u0026emsp; author = {Rashmi Sankepally and Harsha Kokel and Komal Agarwal and Prasenjit Majumder},\n\u0026emsp;\u0026emsp; title = {Morpheme Extraction Task at {FIRE} 2012-2013},\n\u0026emsp;\u0026emsp; booktitle = {Proceedings of the Forum on Information Retrieval Evaluation, {FIRE}},\n\u0026emsp;\u0026emsp; pages = {5:1--5:4},\n\u0026emsp;\u0026emsp; publisher = {{ACM}},\n\u0026emsp;\u0026emsp; year = {2013},\n\u0026emsp;\u0026emsp; url = {https://doi.org/10.1145/2701336.2701637},\n\u0026emsp;\u0026emsp; doi = {10.1145/2701336.2701637},\n\u0026emsp;\u0026emsp; }\nCopy    Experience Industry   Research Intern, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA \u0026nbsp; [ Summer 2021, Summer 2022]  Bridging the gap between Symbolic planning and Reinforcement Learning.\nManager: Dr. Shirin Sohrabi   ML Intern, Turvo Inc., CA, USA \u0026nbsp; [ Summer 2018 ]  Modeled a cost estimator that leverages the knowledge of the domain experts. Kokel et al. AAAI 2020 was motivated by the work at Turvo.   Senior Software Engineer, Amadeus Software Labs, Bangalore, India \u0026nbsp; [ 2016-17 ] Implemented efficient low fare search for Air Canada.  Associate Technology, Publicis Sapient, Bangalore, India \u0026nbsp; [ 2013-16 ] \nProvided content management solutions for enhanced digital presence.   Academia  Instructor, UT Dallas, TX, USA \u0026nbsp; [ Fall 2021 ] Designed and organized labs for an 8-Week course on Advanced Machine Learning for engineers from Vistra Corporation.   Research Assistant, Starling Lab, UT Dallas, TX, USA \u0026nbsp; [ Spring 2019 - present ]   Teaching Assistant, UT Dallas, TX, USA \u0026nbsp; [ Fall 2018 ] CS6343: Artificial Intelligence (graduate level)\nCS4365: Artificial Intelligence (undergrad level)   Research Assistant, IR Lab, DA-IICT, Gandhinagar, India \u0026nbsp; [ 2012-13 ] Worked on Sandhan, a multilingual search engine for 8 Indian languages. Including cross-lingual search.\nAdvisor: Prasenjit Majumder   Research Intern, Mobile Research Lab, NID, Gandhinagar, India \u0026nbsp; [ Summmer 2011 ] Developed a Point of Interest (POI) collector with Google maps for Android Smart Phones. Advisor: Jignesh Khakhar    Technical skills  Python, PyTorch, JAX, Java, C, Shell Scripting, MATLAB, R, Linux/Unix, Git, SQL, Prolog, PDDL, Jupyter.   Projects   Effective Abstraction in Relational RL Providing state abstraction for better task transfer in taskable reinforcement learning environments using D-FOCI statements. [ code ]     Relational Q Learning Implemented Q-learning for relational domains like blocksworld, where the states are represented using first-order logic predicates. [ code ]     Communicating with Computers This is a DARPA funded project to build intelligent minecraft agent that can communicate and collaborate with humans through chat to build structures. [ video ]     Knowledge Intensive Gradient Boosting Leveraging qualitative domain knowledge while learning tree based gradient boosting models to improve predictions in regions where data is noisy or absent. Kokel et al. AAAI 2020.\n    Learning Sparse Graph for GNN Used meta-learning techniques to optimize the graph structure for obtaining sparse graph. Wan and Kokel, DLG 2021.\n    Causal inference from Protein Expression Data Discovering causal molecular relationships from the evaluation of observational data using do-calculus. More details.\n    Ja-Walk-ER Developed an interface that allows users with basic understanding of ER Diagrams to provide search bias for Inductive Logic Programming based. As described in Hayes et al. 2017. [ code ]     Expression Detection A small project to detect wink and shush expression using OpenCV. [ code ]     RL for Healthcare Learning polices for management of children on ECMO using batch reinforcement learning techniques. More details.\n    SRL model for credit default Learnt and evaluated a statistical relational model for Kaggle Home credit default risk dataset and compared it with propositional models.\n    Optimization Implementation of various first-order and second-order gradient methods for optimization. Including Barzilai-Borwein Gradient Descent, Conjugate Gradient Descent, Limited Memory BFGS and Armijo Line-search. [ code ]     ML/AI basket My basket of Machine Learning and AI algorithms implemented over time. [ code ]   \t Talks  RePReL: Integrating Relational Planning and Reinforcement Learning for Effective Abstractions at Eleventh RBCDSAI Workshop on Recent progress in Data Science and AI, IIT Madras, India, Nov 17, 2021.\nslide | video  Human Allied Artificial Intelligence at Ph.D. mixer, UT Dallas, USA, Sep 10, 2021.\nslide  A Unified Framework for Knowledge Intensive Gradient Boosting: Leveraging Human Experts for Noisy Sparse Domain at AAIR Lab Spring Meeting, ASU, USA, Jan 22, 2021.\n   Academic Service  Assistant Electronic Publishing Editors for JAIR 2020 - present. PC Member for Workshop on Graphs and more Complex structures for Learning and Reasoning (GCLR) at AAAI 2022. Reviewed papers for I Can\u0026rsquo;t Believe It\u0026rsquo;s Not Better (ICBINB) Workshop at NeurIPS 2021. Reviewer for Data Mining and Knowledge Discovery Journal. Volunteered at ICAPS 2021. Volunteered at AAAI 2021. Reviewer for Big Data Journal. Reviewed papers for AAAI 2021. Student volunteer at ICDE 2020. Reviewed papers for CODS-COMAD, 2020 and SDM, 2020. Helped organize meeting of Forum for Information Retrieval Evaluation (FIRE), 2018, 2013. Conducted a lab on Information Retrieval with Terrier, an open source search engine, in MSR \u0026amp; IRSI Pre-FIRE workshop 2013 at Microsoft Research India.   Awards and Recognitions  Our team won Hackathon at Amadeus in November 2016 Certified Lead Developer for Adobe Experience Manager (AEM) 6.0 and 5.6  Star Employee of Sapient Bangalore, awarded in 2015 for the work on Loblaws project in the United Content Workz vertical Nominated and Recognised at Sapient for exhibiting the Core Value of Client-Focused Delivery Ranked among top 15% performer of the 2013 batch at Sapient and rewarded accordingly Student Representative in Disciplinary action Committee (2012-2013) Deputy convener of Cultural Committee (2012-2013) Elected as Deputy Convener of the Cultural Committee in 2012-13 at DAIICT  Elected as the first student representative in the Disciplinary Action Committee at DAIICT for academic year 2012-13   My Github Contribution   "
},
{
	"uri": "https://harshakokel.com/misc/",
	"title": "misc",
	"tags": [],
	"description": "Random Musings. ",
	"content": "Here are some quotes that speak to me, stay with me, and motivate me. Time and again.\nWith a little coffee and sunlight, the troubles will get smaller and the world will keep standing. I promise. \u0026mdash; Richard Webber\u0026rsquo;s Character, in Grey\u0026rsquo;s Anatomy\nThe future enters into us, in order to transform itself in us, long before it happens. \u0026mdash; Rainer Maria Rilke\nFrom [Grothendieck], I have also learned not to take glory in the difficulty of a proof: difficulty means we have not understood. The idea is to be able to paint a landscape in which the proof is obvious. \u0026mdash; Pierre Deligne\n\u0026ldquo;If you don\u0026rsquo;t set your own agenda, somebody else will.\u0026rdquo; If I didn\u0026rsquo;t fill my schedule with things I felt were important, others would fill my schedule with things they felt were important. \u0026mdash; Melinda Gates, in The Moment of Lift\nHer Time.\nShe has been feeling it for a while\u0026mdash;that sense of awakening. There is a gentle rage simmering inside her, and it is getting stronger by the day. She will hold it close to her\u0026mdash;she will nurture it and let it grow. She won\u0026rsquo;t let anyone take it away from her. It is her rocket fuel and finally, she is going places. She can feel it down to her very core\u0026mdash;this is her time. She will not only climb mountains\u0026mdash;she will move them too.\n\u0026mdash; Lang Leav, in The Universe of Us\nOpportunities have to be equal before you can know if abilities are equal. And opportunities for women have never been equal. \u0026mdash; Melinda Gates, in The Moment of Lift\n  .gr_custom_container_1595191475 { border: 1px solid gray; border-radius:10px; padding: 10px 5px 10px 5px; background-color: #FFFFFF; color: #000000; } .gr_custom_header_1595191475 { border-bottom: 1px solid gray; width: 100%; margin-bottom: 5px; text-align: center; font-size: 150% } .gr_custom_each_container_1595191475 { width: 100%; clear: both; margin-bottom: 10px; overflow: auto; padding-bottom: 4px; border-bottom: 1px solid #aaa; } .gr_custom_book_container_1595191475 { overflow: hidden; height: 60px; float: left; margin-right: 4px; width: 39px; } .gr_custom_author_1595191475 { font-size: 10px; } .gr_custom_tags_1595191475 { font-size: 10px; color: gray; } .gr_custom_rating_1595191475 { float: right; }  Books I read recently and recommend      A Madman Dreams of Turing Machines  by Janna Levin  This is a page turner. It is the best science-history fiction I have ever read. Author does a great job narrating parallel lives of Alan Turing and Kurt Gödel. The narration is fantastic and consuming. I absolutely loved how she puts her...       The Ethical Algorithm: The Science of Socially Aware Algorithm Design  by Michael Kearns  For so long machine learning has been taught independently of ethical and privacy matters. Word privacy is rarely used outside of cyber security courses. I have taken machine learning courses where I studied about the collaborative filte...       Invisible Women: Data Bias in a World Designed for Men  by Caroline Criado Perez  In the era when Machine Learning and AI are thriving, data about anything and everything is recorded and sold, where privacy and ethical use of data is a top concern, who would imagine that we lack data to tackle some of the most critica...       Share book reviews and ratings with Harsha, and even join a book club on Goodreads.       Some resources that I find useful for work, life, and Ph.D.\n Job is what you make it, by Amar Bose How to read a paper? S. Keshav and Jason Eisner Powerpoint etiquettes, by Phillip Odom and Sriraam Natarajan Littman\u0026rsquo;s style guide Yobibytes\u0026rsquo;s productive grad school collection My Cheatsheets How to Read a book The Extent and Consequences of P-Hacking in Science Why Most Published Research Findings Are False Productive Project Meetings How to write great research papers CRA Grad Cohort for Women resources\na. A Research Mindset\nb. Building Self-Confidence Danfeng (Daphne) Yao, Depth and Persistence: What Researchers Need to Know About Impostor Syndrome, Communications of the ACM, June 2021, Vol. 64 No. 6, Pages 39-42.  "
},
{
	"uri": "https://harshakokel.com/projects/",
	"title": "projects",
	"tags": [],
	"description": "These are selected projects of Harsha Kokel. ",
	"content": "Selected Projects   Effective Abstraction in Relational RL Providing state abstraction for better task transfer in taskable reinforcement learning environments using D-FOCI statements. [ code ]     Relational Q Learning Implemented Q-learning for relational domains like blocksworld, where the states are represented using first-order logic predicates. [ code ]     Communicating with Computers This is a DARPA funded project to build intelligent minecraft agent that can communicate and collaborate with humans through chat to build structures. [ video ]     Knowledge Intensive Gradient Boosting Leveraging qualitative domain knowledge while learning tree based gradient boosting models to improve predictions in regions where data is noisy or absent. Kokel et al. AAAI 2020.\n    Learning Sparse Graph for GNN Used meta-learning techniques to optimize the graph structure for obtaining sparse graph. Wan and Kokel, DLG 2021.\n    Causal inference from Protein Expression Data Discovering causal molecular relationships from the evaluation of observational data using do-calculus. More details.\n    Ja-Walk-ER Developed an interface that allows users with basic understanding of ER Diagrams to provide search bias for Inductive Logic Programming based. As described in Hayes et al. 2017. [ code ]     Expression Detection A small project to detect wink and shush expression using OpenCV. [ code ]     RL for Healthcare Learning polices for management of children on ECMO using batch reinforcement learning techniques. More details.\n    SRL model for credit default Learnt and evaluated a statistical relational model for Kaggle Home credit default risk dataset and compared it with propositional models.\n    Optimization Implementation of various first-order and second-order gradient methods for optimization. Including Barzilai-Borwein Gradient Descent, Conjugate Gradient Descent, Limited Memory BFGS and Armijo Line-search. [ code ]     ML/AI basket My basket of Machine Learning and AI algorithms implemented over time. [ code ]   \t   My leisure projects \n"
}]