<!DOCTYPE HTML>

<html>
  <head>
    <title>Hindsight Experience Replay</title>
    
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="referrer" content="no-referrer">
    
    
    <link rel="stylesheet" href="/css/main.css" />
    
    
    
    <link rel="stylesheet" href="/css/academicons.min.css"/>
    <link rel="stylesheet" href="/css/ocs-ui.min.css">
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hindsight Experience Replay"/>
<meta name="twitter:description" content="Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017 Remember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.
In standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states."/>

    <meta property="og:title" content="Hindsight Experience Replay" />
<meta property="og:description" content="Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017 Remember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.
In standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://harshakokel.com/post/her/" />
<meta property="article:published_time" content="2020-03-20T16:40:08+02:00" />
<meta property="article:modified_time" content="2020-03-20T16:40:08+02:00" /><meta property="og:site_name" content="Home" />

    <meta itemprop="name" content="Hindsight Experience Replay">
<meta itemprop="description" content="Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017 Remember the sampling approaches used for approximate inference in Bayesian Networks, how the rejection sampling is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in importance sampling. This paper proposes something similar.
In standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states.">
<meta itemprop="datePublished" content="2020-03-20T16:40:08+02:00" />
<meta itemprop="dateModified" content="2020-03-20T16:40:08+02:00" />
<meta itemprop="wordCount" content="303">



<meta itemprop="keywords" content="RL," />

    
  </head>
  <body>

    
    <div id="wrapper">

      
      <div id="main">
	<div class="inner">

	  
	  
<header id="header">
	<h3><a href="#">Harsha Kokel</a></h3>
	<ul class="icons">
		<li id="linkedin">
			<a target="_blank" href="http://linkedin.com/in/harshakokel" title="linkedin" class="icon brands fa-linkedin">
				<span class="label" >Linked in</span>
			</a>
		</li>
		<li id="github">
			<a target="_blank" href="https://github.com/harshakokel" title="github" class="icon brands fa-github">
				<span class="label" >Github</span>
			</a>
		</li>
		<li id="email">
			<a  href="mailto:hkokel@utdallas.edu" title="mail" class="icon solid fa-envelope" >
				<span class="label" >Mail</span>
			</a>
		</li>
		<li id="twitter">
			<a target="_blank" href="https://twitter.com/harsha_kokel" title="twitter" class="icon brands fa-twitter" >
				<span class="label" title="Twitter">Twitter</span>
			</a>
		</li>
	</ul>
</header>
    <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax '})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

	  
<div class="page__section">
  <nav class="breadcrumb breadcrumb_type5" aria-label="Breadcrumb">
<ol  class="breadcrumb__list r-list">
  



<li class="breadcrumb__group" >
  <a href="https://harshakokel.com/" class="breadcrumb__point r-link">Home</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>


<li class="breadcrumb__group"  class="active">
  <a href="https://harshakokel.com/post/" class="breadcrumb__point r-link">Posts</a>
  <span class="breadcrumb__divider" aria-hidden="true">&raquo;</span>
</li>

  <li class="breadcrumb__group">
    Mar 20, 2020
  </li> 
  <li class="tags">  Tagged with:
      <a href="https://harshakokel.com/tags/rl/" class="button">rl</a>
  </li>
</ol> 



</nav>
</div>

  <header class="main">
    <h2>Hindsight Experience Replay</h2>
  </header>


  
  

  <p><strong></strong></p>

  <p><h4 id="marcin-andrychowicz-filip-wolski-alex-ray-jonas-schneider-rachel-fong-peter-welinder-bob-mcgrew-josh-tobin-pieter-abbeel-wojciech-zaremba-neurips-2017">Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba, NeurIPS 2017</h4>
<br>
<p>Remember the sampling approaches used for approximate inference in Bayesian Networks, how the <strong>rejection sampling</strong> is super expensive since it wastes lot of samples and we try to capitalize on those samples by providing weights in <strong>importance sampling</strong>. This paper proposes something similar.</p>
<p>In standard RL setting, with sparse reward there can be a long time before the Q-values propagate from the goal state to individual states and even when they do because of sparsity they might not be adequate to differentiate between different states. Popular solution for this problem is to use reward shaping functions but even they have some unforeseen consequences.</p>
<p>This paper highlights that even though the current trajectory $T_i = &lt;s_0, a_0, s_1, a_1, &hellip; s_{t_i}, a_{t_i}&gt;$ did not reach the achieved goal state $g_i$, it reached the state $s_{t_i}$ and hence if the goal state was $s_{t_i}$ this would have been a useful trajectory. With the advent of goal-conditioned policy learning, policies $\pi$ are no longer learnt for a single goal, rather goal state $g$ is taken as input along with state and action. i.e instead of $\pi:S \times A \rightarrow [0,1]$, goal-conditioned policies are $\pi:S \times A \times S_G \rightarrow [0,1]$. So, we can engineer different goal for trajectories which do not reach their pre-determined goal and add it to the *replay buffer* with engineered goal state as well as the pre-determined goal. This increases the buffer size, providing more transition samples to learn.</p>
<h3 id="critique">Critique</h3>
<p>Although the idea of the using the existing sampled trajectory by engineering the goal seems useful, it is not clear if there exists a principled approach to do this and how is it better than engineering reward-shaping functions?</p>
<h3 id="references">References</h3>
<ol>
<li><a href="https://openai.com/blog/ingredients-for-robotics-research/">https://openai.com/blog/ingredients-for-robotics-research/</a></li>
</ol>
</p>



<script src="https://utteranc.es/client.js"
        repo="harshakokel/harshakokel.github.io"
        issue-number=12
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<div class="postpagination">
  
    <a class="postpagination__item" href="https://harshakokel.com/post/logic-program-policies/">
        <span class="postpagination__label">Previous Post</span>
        <span class="postpagination__title">Logical Program Policies</span>
    </a>
  

  
    <a class="postpagination__item" href="https://harshakokel.com/post/maml/">
      <span class="postpagination__label">Next Post</span>
      <span class="postpagination__title" >Model-Agnostic Meta-Learning</span>
    </a>
  
</div>
<br>



	</div>
      </div>

      
<div id="sidebar">
  <div class="inner">

    

    
<nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    
    
    
    <li><a href="/">Homepage</a></li>
    
    
    
    <li><a href="/cv/">cv</a></li>
    
    
    
    <li><a href="/projects/">projects</a></li>
    
    
    
    <li><a href="/posts/">BLOG POSTS</a></li>
    
    
  </ul>
</nav>


    
<footer id="footer">
  <p class="copyright">Powered by <a href="https://gohugo.io">Hugo</a> and design by <a href="https://html5up.net">HTML5 UP</a>.</p>
</footer>


  </div>
</div>


    </div>
    
    
    
    <script src="/js/jquery.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    
    <script src="/js/main.js"></script>
    
      

  </body>
</html>
